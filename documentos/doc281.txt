Online Appendix to: 
Data Stream Clustering: A Survey 
JONATHAN A. SILVA, University of Sao Paulo ˜ 
ELAINE R. FARIA, University of Sao Paulo and Federal University of Uberl ˜ andia ˆ RODRIGO C. BARROS, EDUARDO R. HRUSCHKA, and 
ANDRE C. P. L. F. DE CARVALHO ´ , University of Sao Paulo ˜ 
JOAO GAMA ˜ , University of Porto 
A. COMPUTATIONAL COMPLEXITY ANALYSIS 
In this appendix, we analyze the computational complexity of the data stream clus tering algorithms regarding their processing time, that is, our focus will be on time complexity. 
We start by analyzing the time complexity of the pioneering BIRCH algorithm [Zhang et al. 1996, 1997]. It employs a B+Tree to store the summarized statistics. The cost for (re)inserting an object in a B+Tree is O(n × B × H), where B is the number of entries in an inner node and H is the maximum height of the tree. Recall that n is the data dimensionality. 
In the ClusTree algorithm, the process of inserting a new object in an R-tree takes O(log(q)), where q is the number of CFs. The merge process requires O(M2), where M is the number of entries in a leaf node. The clustering result can be obtained by applying any clustering algorithm, such as k-means or DBSCAN, over the CFs stored in the leaf nodes. 
As previously seen, Scalable k-means [Bradley et al. 1998] adopts the divide and-conquer strategy for processing chunks of the stream. Each chunk holds m n dimensional objects that are clustered by k-means into k clusters during the primary compression step, taking O(m × n × k × v) time, where v is the number of k-means iterations. There are papers that provide theoretical upper bounds on its running time[Vattani 2009; Arthur and Vassilvitskii 2006], which can be exponential even for low-dimensional data. However, we note that, in practice, usually the number of itera tions is fixed a priori or a convergence criterion, such as those based on the difference between centroids in two consecutive iterations, is adopted. Either way, it is reason able to assume that the number of iterations is usually much less than the number of objects. 
In order to identify data objects that will be discarded, the Scalable k-means algo rithm uses a version of the Mahalanobis distance [Maesschalck et al. 2000]—known as normalized Euclidean distance, which allows simplifying the covariance matrix inversion—is employed, taking O(m × n) time. Finding the discard radius for all k clusters takes O(m × log m) if sorting is used. The total time complexity for the first method of primary compression is O(m(n + log m)), whereas the second method of pri mary compression (cluster means perturbation) takes O(m× k × n). In the secondary compression step, all objects that did not meet the requirements for being clustered in the first step (the worst case is m objects) are grouped into k2 > k clusters, thus taking O(m× n × k2 × v). The k2 clusters are checked again to verify if their covariances are bounded by the threshold β. The clusters that do not meet this criterion are merged with the k clusters obtained in the first compression step by using a hierarchical agglomera tive clustering, whose time complexity is O(k22 ×n). The algorithm total time complexity 
 c 2013 ACM 0360-0300/2013/10-ART13 $15.00 
DOI: http://dx.doi.org/10.1145/2522968.2522981 
ACM Computing Surveys, Vol. 46, No. 1, Article 13, Publication date: October 2013.
App–2 J. A. Silva et al. 
is therefore O(m×n×k×v)+O(m(n+log m))+O(m×k×n)+O(m×n×k2×v)+O(k22 ×n). Single-pass k-means [Farnstrom et al. 2000], which is a simplified version of the Scal able k-means algorithm with no compression steps, takes O(m× n× k × v). 
For the online phase of CluStream [Aggarwal et al. 2003], the cost of creating mi croclusters is O(q × Nf irst × n × v), where q is the number of microclusters and Nf irst is the number of objects used to create the initial microclusters. Next, for each new data object that arrives, updating microclusters requires three steps: (i) find the closest microcluster, which takes O(q×n) time; (ii) possibly discarding the oldest microcluster, also taking O(q) time; and (iii) possibly merging the closest microclusters, which takes O(q2 × n). The offline step runs the traditional k-means algorithm over the microclus ters, which takes O(q × n× k × v) time. 
Similary to CluStream, the SWClustering algorithm has time complexity of O(q × n) to insert a new object into the nearest EHCF, where q is the number of EHCFs. In addition, it takes O(q2 ×n) to merge the two nearest EHCFs, O(q) to update the EHCF containing expired objects, and O(q×n×k×v) to cluster the EHCFs using the k-means algorithm. Unlike CluStream, SWClustering does not require the creation of an initial summary structure. 
DenStream [Cao et al. 2006] is a density-based algorithm that is also executed in two phases. In the online phase, the cost for creating microclusters through DBSCAN [Ester et al. 1996] is O(N2f irst).6 For each new object of the stream, one of the q p-microclusters is updated according to the following three steps: (i) finding the closest p-microcluster, which takes O(q × n); (ii) possibly finding the closest o-microcluster, which also takes O(q × n); and (iii) possibly verifying if an o-microcluster is a potential p-microcluster, which once again takes O(q × n). Hence, the cost for updating microclusters for each new object is O(q × n). The periodical analysis of outdated p-microclusters also takes O(q × n). 
StreamKM++ [Ackermann et al. 2012] maintains a summary of the stream using the merge-and-reduce technique [Har-Peled and Mazumdar 2004; Agarwal et al. 2004, Bentley and Saxe 1980]. Basically, it maintains log Nm buckets in main memory, where each bucket i stores only m objects. Each bucket represents 2i × m objects from the stream. In Ackermann et al. [2012], the merge-and-reduce technique implements a coreset tree that organizes the data in a binary tree (reduce step). The time complexity for a single operation of reduce (from 2mto mobjects) is O(m2×n) (or O(m×n×log m) to a balanced coreset tree). In order to obtain a coreset tree with m objects, representing the union of all buckets, the merge-and-reduce of all buckets is executed in O(m2 × n× log Nm). After having processed the whole input stream, the k-means++ algorithm finds k clusters on the m points (obtained from the merge-and-reduce of all buckets) with time complexity O(m× k × n × v). 
Stream [Guha et al. 2000] runs the facility location algorithm [Charikar and Guha 1999; Meyerson 2001] over chunks with B objects to find 2k clusters, which takes O(B×n×k) to execute. Similarly, Stream LSearch [O’Callaghan et al 2002] also assumes that a chunk of B objects will be clustered into 2k clusters, though this time the clustering algorithm employed is LSearch. First, LSearch creates an initial solution with k  clusters, where the value of k  depends on the properties of the dataset but is usually small [OCallaghan et al. 2002], which takes O(B×n×k ). Next, a binary search is performed to partition the B objects into k clusters, which takes O(B× n× k× log k). Hence, the overall time complexity of Stream LSearch is O(B×n×k  + B×n×k×log k). 
DGClust [Rodrigues et al. 2008; Gama et al. 2011] operates both in local and central sites. Considering the time complexity of a local site, after the bins for the two layers of 
6Accelerating index structures, such as KD-trees [Bentley 1975], can be used to reduce the time complexity. ACM Computing Surveys, Vol. 46, No. 1, Article 13, Publication date: October 2013.
Data Stream Clustering: A Survey App–3 
discretization have been created, the cost of inserting a new object is O(log pi), where pi is the number of bins of the first layer, that is, it is the cost of searching the proper bin for inserting the object. Since each local site operates in a parallel fashion, the time complexity for the local sites is O(log p), where p = maxi(pi) and i ∈ 1, 2,..., n. Recall that n is the number of dimensions of the stream and thus the number of local sites for DGClust. The central site keeps the top-m most frequent global states. Searching a state in the list of states takes O(m). Clustering the top-m most frequent global states takes O(m× n × k). 
D-Stream [Chen and Tu 2007] is a grid-based stream clustering algorithm. In the worst-case scenario, at each iteration of D-Stream there are pn grid cells in memory, where p is the number of partitions in each dimension. However, as already noted in Chen and Ti [2007] and Amini et al. [2011], although in theory the number of possible grid cells grows exponentially with n, empty or infrequent grid cells can be discarded, under the assumption that the data space is sparse. 
ACM Computing Surveys, Vol. 46, No. 1, Article 13, Publication date: October 2013.
