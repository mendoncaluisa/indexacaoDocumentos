Marcone Jamilson Freitas Souza b ,,Maria Amélia Lopes Silvaa , Sérgio Ricardo de Souzaa,ÿ Ana
Lúcia C. Bazzanc
a Federal Center of Technological Education of Minas Gerais (CEFET-MG), Belo Horizonte ZIP Code: 30510-000, MG, Brazil b
resumoinformação do artigo
Esta afirmação se justifica em parte por sua versatilidade e sua grande adaptabilidade
para resolver problemas, mas, por outro lado, principalmente pela possibilidade de se
obter, em tempo computacional limitado, soluções de boa qualidade para grandes e
complexos problemas de otimização. Problemas com estas características em muitos e
importantes casos não têm sua solução conhecida, em tempo computacional limitado,
se apenas técnicas de programação matemática forem aplicadas. Esta afirmação é
particularmente verdadeira para a classe de problemas NP-Difíceis, que envolve
problemas de grande importância teórica e prática.
relevância física, como o Problema de Roteamento de Veículos ou o Problema de
Programação de Máquinas Paralelas.
Blum e Roli (2003), Blum, Puchinger, Raidl e Roli (2011), Talbi (2009) e Gendreau e
Potvin (2010) apresentam formulações gerais e conceituais sobre metaheurísticas, bem
como revisões importantes sobre seu uso na resolução de problemas de otimização.
Nos últimos anos, a combinação de duas ou mais metaheurísticas para resolver
problemas de otimização vem crescendo (Blum et al., 2011; Cotta, Talbi, & Alba, 2005).
O principal objetivo da hibridização de metaheurísticas é aplicar em conjunto as
melhores características de cada metaheurística para resolver um problema, permitindo,
além de obter a melhor qualidade de solução em um menor tempo, aumentar a
capacidade de lidar com problemas mais complexos. Cotta et al. (2005) e Blum et al.
(2011) afirmam que as hibridizações são responsáveis por muitos dos melhores
resultados encontrados na literatura para várias classes de problemas de otimização, o
que justifica o crescente interesse por essa abordagem.
1. Introdução
As metaheurísticas se consolidaram como uma das principais metodologias para
resolução de problemas de otimização de diversas classes.
página inicial do periódico: www.elsevier.com/locate/eswa
Listas de conteúdo disponíveis no ScienceDirect
O aumento da utilização de metaheurísticas, seja por especialistas ou não em
métodos de resolução de problemas de otimização, tem orientado
Uma estrutura multiagente baseada em aprendizagem por reforço aplicada para
resolver problemas de roteamento e agendamento
Sistemas Especialistas Com Aplicações
Revisado em 24 de abril de 2019
Aprendizagem por reforço
Metaheurísticas
Recebido em 6 de fevereiro de 2018
Histórico do artigo:
Estrutura multiagente para otimização
Palavras-chave:
https://doi.org/10.1016/j.eswa.2019.04.056 0957-4174/©
2019 Elsevier Ltd. Todos os direitos reservados.
E-mail addresses: mamelia@ufv.br (M.A. Lopes Silva), sergio@dppg.cefetmg.br (S.R. de Souza), marcone@iceb.ufop.br
(M.J. Freitas Souza), bazzan@inf.ufrgs.br (A.L.C. Bazzan).
Department of Computer Science, Federal University of Ouro Preto (UFOP), Ouro Preto ZIP Code: 35400-000, MG, Brazil c Institute of Informatics, Federal University
of Rio Grande of the Sul (UFRGS), Porto Alegre ZIP Code: 91501-970, RS, Brazil
Problema de agendamento de máquinas paralelas não
relacionadas
ÿ Autor correspondente.
Sistemas Especialistas Com Aplicações 131 (2019) 148–171
Disponível online em 24 de abril de 2019
Problema de roteamento de veículos com janela de tempo
Aceito em 24 de abril de 2019
Sistemas multiagentes
Os resultados também reforçam a questão da escalabilidade do framework, uma vez que, com a adição de novos agentes, há uma
melhoria das soluções obtidas.
© 2019 Elsevier Ltd. Todos os direitos reservados.
Os agentes compartilham informações e colaboram entre si por meio do ambiente. O objetivo é permitir que o agente modifique suas
ações com base nas experiências adquiridas na interação com os outros agentes e o ambiente usando os conceitos de Aprendizado
por Reforço. Para melhor introdução e validação do framework AMAM, este artigo utiliza a instanciação do Problema de Roteamento
de Veículos com Janelas de Tempo (VRPTW) e do Problema de Escalonamento de Máquinas Paralelas Não Relacionadas com
Tempos de Configuração Dependentes de Sequência (UPMSP-ST), ou seja, dois problemas clássicos de otimização combinatória. O
objetivo principal dos experimentos é avaliar o desempenho dos agentes adaptativos propostos. Os experimentos confirmam que a
capacidade de aprendizagem atribuída ao agente influencia diretamente na qualidade das soluções, tanto do ponto de vista individual
quanto do ponto de vista do trabalho em equipe. Dessa forma, o framework aqui apresentado é um passo à frente em relação aos
demais frameworks da literatura no que se refere à adaptação aos aspectos particulares dos problemas. Adicionalmente, confirma-se
a cooperação entre agentes e sua capacidade de influenciar a qualidade das soluções dos agentes envolvidos na busca da solução.
Este artigo apresenta um framework multiagente para otimização usando metaheurísticas, chamado AMAM. Nesta proposta, cada
agente atua independentemente no espaço de busca de um problema de otimização combinatória.
Machine Translated by Google
149MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171
Coelho et al. (2011). Sua principal característica é uma interface para
Como outras estruturas disponíveis na literatura, o AMAM
soluções fornecidas por outros agentes para melhorar suas próprias soluções.
(MAS). MAS são usados aqui como uma ligação entre diferentes meta-
Para avaliar o desempenho da aprendizagem, são realizados dois experimentos
computacionais: (i) novos testes utilizando a proposta de aprendizagem apresentada
em Silva et al. (2015) e (ii) testes realizados
do desenvolvimento desta estrutura.
sistemas. A abordagem baseada em agentes se distingue por seu poder
e, portanto, permitindo a determinação de melhores soluções com
em Silva (2007). Este enquadramento permite a fácil hibridização de
novas técnicas para resolver esta classe de problemas é fundamental
(Watkins & Dayan, 1992). Portanto, aqui o conceito de Reinforcement Learning é
usado para definir a ordem de aplicação das estruturas de vizinhança da busca local.
Como as funções de vizinhança
Cahon, Melab e Talbi, 2004; Coelho et al., 2011; Durillo & Ne-bro, 2011; Fink & Voß,
2002; Gaspero & Schaerf, 2003; Taluk-dar, Baerentzen, Gove, & Souza, 1998), a
maioria deles com características e propostas semelhantes. Uma revisão bibliográfica
e
Para melhor introdução e validação do framework AMAM,
de uma metaheurística particular e sua adequação ao tratamento
problemas de otimização multiobjetivo e uma ampla gama de questões
problemas de otimização usando metaheurísticas, incluindo importantes
ocorre por meio de um pool de soluções. A comunicação é governada
em Parejo, Ruiz-Cortés, Lozano e Fernandez (2012), é uma ferramenta de software
que fornece implementações de um conjunto de metaheurísticas,
permitindo uma comparação eficaz entre os resultados encontrados. Um
adequado para ser usado aqui para demonstrar as potencialidades do
via metaheurísticas de vários problemas de otimização combinatória
Silva et al. (2015). O objetivo é permitir que o agente modifique sua
pesquisadores no desenvolvimento de frameworks. Esses especialistas são
Problema de roteamento com janela de tempo – VRPTW (Toth & Vigo, 2002)
ser usado permite que a estrutura se adapte melhor às características do problema.
A estratégia de aprendizagem é definida aqui para cada
estrutura flexível, na qual cada metaheurística é definida como um agente autônomo
que interage com seu ambiente cooperativamente.
de França Filho (2018). OptFrame is a framework proposed in
a abordagem cooperativa paralela gerenciada por Sistemas Multiagentes
heurísticas para resolver problemas de otimização. Cada agente é responsável por
executar sua própria tarefa e, ao mesmo tempo, por usar o
Souza, Souza, & de Oliveira, 2015) have presented different stages
desempenho destes no quadro.metaheurísticas de população e trajetória, técnicas evolutivas multiobjetivo, bem
como implementações paralelas e distribuídas. Da mesma forma, a demanda por
software cada vez mais adaptativo e inteligente tem levado à incorporação de novas
tecnologias no tratamento de vários problemas, como, por exemplo, a tecnologia
baseada em agentes autônomos por meio de modelos multiagentes.
a solução, sem exigir esforços para refazer a aplicação,
AMAM, ou seja, “ Arquitetura Multiagente para Metaheurística ”, em Por-tuguese, or
Multi-agent Architecture for Metaheuristics , proposto
processo. Ao mesmo tempo, no caso dos pesquisadores, a busca por
termos do ponto de vista do trabalho em equipe.
& Barto, 1998), mais especificamente, através do algoritmo Q-LearningVárias estruturas para metaheurísticas podem ser encontradas na literatura
(Alba, Luque, Garcia-Nieto, Ordonez, & Leguizamon, 2007;
VRPTW e UPMSP-ST são problemas NP-difíceis e, consequentemente,
metaheurísticas baseadas em trajetória. jMetal é um framework orientado a objetos
baseado na linguagem Java (Durillo & Nebro, 2011). Ele inclui um número significativo
de métodos clássicos e modernos para
comparação de diferentes métodos; (iii) facilidade no desenvolvimento
framework como uma ferramenta de software consolidada para resolver problemas combinatórios
objetivo pré-definido. A interação entre os vários agentes
recursos para a solução de problemas de um domínio específico, tornando o
desenvolvimento de novas aplicações neste domínio muito mais simples. Um
Metaheuristic Optimization Frameworks (MOF), como é chamado
os experimentos são realizados sob as mesmas condições computacionais e com a
mesma estrutura de cooperação entre os agentes,
(i) Melhorar as habilidades autoadaptativas do agente, usando os conceitos de
Aprendizagem por Reforço, especificamente o algoritmo Q-Learning, permitindo
que o agente se adapte melhor aos parâmetros específicos do problema a ser
abordado pelo framework;
de natureza distribuída e expressar através das entidades do sistema a complexidade
das relações envolvidas. Como consequência, a abordagem multiagente tem sido
aplicada na solução
capacidades dos agentes de estrutura, uma questão discutida em
aprendizagem na seleção do bairro mais adequado para
essas ferramentas de software.
problemas de otimização combinatória. O primeiro é o Veículo
encontrado em Parejo et al. (2012) e Silva, de Souza, Souza e
através de uma estrutura multiagente. Desta forma, é uma abordagem genérica e
2011; Melab, Luong, Boufaras, & Talbi, 2013). Esses módulos tratam
aqui tem a força da hibridização das metaheurísticas através
Ribeiro, 2009; Silva, de Souza, de Oliveira, & Souza, 2014; Silva, de
As estruturas oferecem flexibilidade na incorporação de novos métodos para
coordenação explícita entre eles. Ao mesmo tempo, esta estrutura introduz, até onde
sabemos, o primeiro uso de aprendizagem por reforço para estruturas especializadas
em resolver problemas de otimização combinatória. Esta capacidade adaptativa
incorporada permite que os agentes se ajustem a problemas específicos, fornecendo
o melhor
O artigo atual apresenta uma estrutura multiagente chamada
resolução de problemas de otimização.
como oferecer recursos que melhoram o desempenho da solução
a aprendizagem, incorporada no agente, tem influência direta no desempenho do
framework no que diz respeito à qualidade dos resultados obtidos, tanto do ponto de
vista individual como do ponto de vista
os outros agentes e com o ambiente. Esta experiência é obtida usando os conceitos
de Aprendizagem por Reforço (RL) (Sutton
elementos comuns de metaheurísticas baseadas na população e de
(Allahverdi, 2015; Allahverdi, Ng, Cheng e Kovalyov, 2008). Ambos
apresenta características comuns como: (i) as metaheurísticas são pré-implementadas
para testar e reutilizar; (ii) suportam a avaliação e
Nessa abordagem, os agentes interagem e trabalham juntos para atingir um objetivo.
A principal motivação deste artigo é apresentar a AMAM
com a estrutura de aprendizagem proposta neste artigo. Essas duas
Este artigo também propõe a melhoria da autoadaptação
baixos custos de desenvolvimento. Frameworks fornecem uma estrutura de genéricos
para modelar problemas como problemas de otimização combinatória
Em resumo, as principais contribuições desta proposta são:
metaheurísticas para resolução de problemas de otimização combinatória,
são parâmetros específicos do problema, o uso de reforço
desafio colocado, mas também é direcionado para o desenvolvimento de
um estudo comparativo com os principais frameworks disponíveis pode ser
este artigo utiliza a instanciação de dois clássicos e conhecidos
problema. Além dessas características, o framework apresentado
instâncias. ParadisEO é uma estrutura global composta por 4 módulos conectados
(Cahon et al., 2004; Liefooghe, Jourdan, & Talbi,
pelas regras de acesso ao pool, tanto para escrita quanto para leitura, visando
garantir a diversidade no compartilhamento de informações de busca. Vários
trabalhos anteriores (Fernandes, de Souza, Silva, Borges, &
Estrutura AMAM.
características como a autonomia dos agentes, sem qualquer tipo de
(Silva et al., 2018).
através de códigos reutilizáveis, facilitando o desenvolvimento de aplicações para
ações baseadas nas experiências adquiridas na interação com
buscando ferramentas que possam facilitar a resolução desses problemas, bem como
um dos principais objetivos deste artigo é avaliar se a forma de
agente individualmente.
e o segundo é o Problema de Escalonamento de Máquinas Paralelas Não
Relacionadas com Tempos de Configuração Dependentes de Sequência – UPMSP-ST
Machine Translated by Google
MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171150
elementos que compõem as metaheurísticas, como intensificação, diversificação,
memória e adaptação ou autoadaptação. Segundo os autores, “para obter uma
metaheurística a partir do modelo organizacional AMF, é necessário refinar os
diferentes papéis e
que usam Q-learning, e (ii) seu algoritmo comparado com os resultados já
conhecidos na literatura. Eles concluem que seu algoritmo é competitivo e que tem
melhor desempenho do que o VNS
Segundo os autores, os resultados mostram que a capacidade de aprender
o espaço é explorado usando o agente mais eficaz no momento.
aprendizagem e descreve o agente adaptativo proposto. A seção 6 relata
experimentos realizados nas instâncias VRPTW e no
neste caso, a recompensa é atribuída aos agentes de otimização cada
geralmente utilizado na metaheurística VNS, a construção da solução inicial
utiliza o algoritmo Q-learning, realizado a partir do
uma tabela de valores Ant-Q (AQ (r, s), onde (r, s) é um par de cidades
Problema do caixeiro viajante simétrico (TSP), usando as instâncias disponíveis
no repositório TSPLIB (http://ftp.zib.de/pub/
influencia diretamente o desempenho cooperativo dos agentes.
Queiroz dos Santos et al. (2014) show an implementation
regra de atualização de feromônio, poderia ser fortemente simplificada sem afetar
o desempenho, Ant-Q foi abandonado em favor do mais simples e
Meignan, Créput e Koukam (2008), Barbucha, Czarnowski,
Sistema (LBMAS) para resolução de problemas de otimização combinatória.
aplicação ao Problema de Roteamento de Veículos são apresentados para ilustrar
o uso do modelo AMF. CBM é uma metaheurística baseada em
A Seção 2 apresenta uma revisão sobre trabalhos sobre o uso da aprendizagem
em conjunto com metaheurísticas dentro de frameworks. Barbucha et al. (2010) apresentaram o ambiente A-Team baseado em JADE
(JABAT), uma ferramenta para construção de A-Teams (Talukdar & Souza, 1990).
os agentes dos algoritmos Ant-Q cooperam para aprender valores AQ,
de pesquisa.
problemas de otimização combinatória, segundo os autores
(ii) Seleção da Pesquisa Local a ser aplicada: para uma eficiente
os conceitos de roleta, em que os valores de avaliação das meta-heurísticas são
obtidos com base nos níveis de melhoria das
soluções são solicitadas na memória comum. Assim, as soluções necessárias
(ii) Melhorar a cooperação entre os agentes, através da inclusão de
Lotfi e Acan (2015), Martin et al. (2016), Mesmo, Lim, et al
& Hansen, 1997). A seleção da heurística de busca local para
metaheurísticas combinadas com a construção de soluções iniciais
Meignan et al. (2008) propõem o Agent Metaheuristic Framework (AMF). O
AMF é baseado em um modelo organizacional que descreve uma metaheurística
em termos de papéis. Os papéis representam os principais
comum aos agentes. Desta forma, diferentes regiões da pesquisa
quando encontra soluções melhores do que aquelas já encontradas anteriormente.
desta estrutura, denominada ATeam baseada em JADE Cooperativa (Cooper-ative
JABAT), introduzindo um mecanismo de Aprendizagem por Reforço.
A Seção 4 descreve a estrutura AMAM e seus principais componentes. A Seção 5
mostra os conceitos básicos de reforço
(i) Construção da solução inicial: em vez da construção
O algoritmo AS é a proposta inicial da metaheurística Ant Colony Optimiza-tion
(Dorigo & Stützle, 2019). No algoritmo Ant-Q,
desempenho dos agentes. Os agentes cooperam compartilhando seus
O algoritmo apresentado pelos autores foi aplicado ao
(iii) Demonstrar como o aprimoramento das habilidades individuais dos agentes
métodos melhores e mais eficientes para resolver problemas de otimização, como
mostrado, por exemplo, em Gambardella e Dorigo (1995),
considerando os pesos associados a cada um deles.
bom desempenho, devido a alguns aspectos do Ant-Q, em particular o
Gambardella e Dorigo (1995) apresentam o algoritmo Ant-Q, a primeira
aplicação do algoritmo Q-learning para resolver
metaheurística chamada Metaheurística Baseada em Coalizão (CBM) e sua
Lotfi e Acan (2015) apresentaram o Learning-Based Multi-Agent
O restante deste artigo está organizado da seguinte forma.
agentes envolvidos aumenta.
(r, s), sendo esta tabela atualizada em tempo de execução. O objetivo é que
seção apresenta algumas conclusões e discute direções futuras
a próxima iteração é escolhida. Esta seleção é realizada usando
espaço);
a recompensa também pode ser concedida se o agente não conseguir melhorar
uma solução. O peso de cada agente nesse contexto é considerado quando
Metaheurística de Busca de Vizinhança Variável (VNS) (Mladenovic´
através de um mecanismo de aprendizagem, para selecionar o operador mais
apropriado, com base no contexto. A recompensa é dada ao agente apenas
Queiroz dos Santos, de Melo, Neto, and Aloise (2014),
Gambardella (1997).
apresentaram: (i) seu algoritmo comparado com uma versão do VNS
é composto por vários agentes que têm a capacidade de individualmente
uma população de soluções comuns e em um arquivo de dois estágios, também
usando um conjunto de agentes de otimização, cada um implementando um
algoritmo de solução. Barbucha et al. (2010) também propuseram uma extensão
Algoritmo do Sistema (AS) e propriedades de Q-learning. Por sua vez, o
Problema com tempos de configuração dependentes de sequência (UPMSP-ST).
para ser usado com base no algoritmo Q-learning.
Dorigo et al. (1999) e Dorigo e Stützle (2019), além de terem
inicialmente têm a mesma probabilidade de serem escolhidos e essa probabilidade
muda (cresce ou diminui) de acordo com o indivíduo
estrutura, buscando maior diversidade de soluções;
O uso do Aprendizado por Reforço em conjunto com meta-heurísticas tem sido
um assunto de grande interesse na busca por
mas somente depois que todos os agentes executarem essas solicitações e, além disso,
determinar a estrutura multiagente do sistema de otimização”. A
discutido a seguir.
versão com Q-learning.
melhora a qualidade das soluções, principalmente quando o número de
Queiroz dos Santos et al. (2014) utilizam o algoritmo de Reinforcement Learning Q-
Learning de duas formas:
Instâncias UPMSP-ST usando a estrutura AMAM. Finalmente, a última
tempo é encontrada uma solução melhor do que a anterior. Um negativo
do TSP) é definido, associado aos valores Q do algoritmo de Q-learning. Ele indica
o quão útil será mover-se na borda
conhecimento do ambiente (busca de soluções para o problema)
A cada iteração da busca, a metaheurística a ser utilizada em
Pacotes/mp-testdata/tsp/tsplib/tsplib.html). Duas comparações são
que propõe uma hibridização da busca reativa com a
tratar o problema de otimização, mas cooperar para coordenar e melhorar a busca”.
Uma estratégia adaptativa é usada pelo agente CBM,
igualmente bom ACS (Ant Colony System), introduzido em Dorigo e
Je¸ drzejowicz, Ratajczak-Ropel e Wierzbowska (2010),
Este sistema permite a colaboração entre agentes metaheurísticos em
a metáfora das coligações. Segundo os autores, “a coligação
arquiteturas para resolver diferentes problemas de otimização. Este framework
produz soluções para problemas de otimização combinatória
conhecimento. Os experimentos realizados usam instâncias do Problema do
Caixeiro Viajante (TSP). Conforme proposto em Dorigo, Caro e Gambardella
(1999), Ant-Q é um algoritmo que tenta mesclar Ant
A Seção 3 analisa brevemente o Problema de Roteamento de Veículos com
Janelas de Tempo (VRPTW) e o Agendamento de Máquinas Paralelas Não Relacionadas
explorar a escolha de qual pesquisa local é mais adequada para um ponto de
pesquisa específico, o algoritmo seleciona a próxima pesquisa local
buscando assim boas soluções para o TSP. No entanto, como indicado em
função objetiva de cada uma das metaheurísticas. Todas as metaheurísticas
as soluções não são enviadas imediatamente após a solicitação de cada agente,
critérios para inserção de novas soluções na cooperativa
2. Revisão bibliográfica
ser utilizado em um determinado ponto da busca é feito em uma aprendizagem
autoadaptativa por meio do Reinforcement Learning. Nesse sentido,
Saleh (2016) e Silva et al. (2015). Alguns desses artigos são
Machine Translated by Google
3.1. Caso 1: VRPTW
uma recompensa negativa é atribuída (ÿ1);
combinação de metaheurísticas/heurísticas de pesquisa local e adapta
Reinforcement Learning-based Memetic Particle Swarm Optimiza-tion (RLMPSO).
Motivado pela dificuldade de integrar o PSO
Para fazer isso, cada agente metaheurístico divide a solução atual em objetos de
borda e os envia ao agente iniciador, que
Fig. 1(a)). Uma solução para o VRPTW é um conjunto de rotas (ver Fig. 1(b)),
VRPTW é uma das técnicas de otimização combinatória mais estudadas
título;
O LBMAS é competitivo em relação aos outros algoritmos já existentes
elementos a serem inseridos nas soluções. As heurísticas construtivas RandNEH e
RandCWS, propostas em Juan, Faulin, Grasman,
A UPMSP-ST também tem forte relevância econômica em vários tipos
mostram que o RLMPSO excede um número significativo de variantes do PSO
relatadas na literatura. Além disso, o aprendizado por reforço
avaliados usando quatro problemas de referência unimodais e multimodais,
de clientes espalhados geograficamente. No caso aqui considerado,
com um agente isolado. Além disso, os resultados também mostram que em
os agentes envolvidos na busca por uma solução. Uma iteração do
diferentemente disto, há uma frota de veículos, um conjunto de clientes,
geograficamente dispersos, a serem satisfeitos, com suas respectivas demandas e
os horizontes de tempo para o atendimento. O completo
as partículas do enxame. Na definição de RLMPSO, Reforço
3. Estudo de caso
um veículo. Os arcos mostram a conexão entre clientes e
mecanismo de aprendizagem permite que cada agente mantenha um curto prazo
experiências individuais por meio de uma memória externa de dois estágios
visão. Vale destacar (Noel & Pandian, 2014; Radac & Pre-cup, 2018; Radac, Precup,
& Roman, 2018), que tratam de aprendizagem por reforço envolvendo redes neurais
artificiais para modelos contínuos; (Kazemitabar, Taghizadeh, & Beigy, 2018), que
discutem sobre aprendizagem por reforço hierárquico em sistemas grandes e
complexos; e (Salgado & Clempner, 2018), que estudam a interação de agentes por
meio de emoção e estímulos.
Programação e (ii) Roteamento de Veículos Capacitados. Os testes foram realizados
melhor valor for encontrado, a recompensa é positiva (1) e, caso contrário,
2008). A Seção 3.2 apresenta esse problema, nos termos que interessam a este
artigo atual.
& Rimassa, 2007). Nesta proposta, cada agente desempenha uma função diferente
as soluções são identificadas, de acordo com a frequência com que ocorrem na
conversa, e depois são compartilhadas entre os agentes.
Samma et al. (2016) apresentam um novo algoritmo baseado em Parti-cle Swarm
Optimization (PSO) (Kennedy & Eberhart, 1995), denominado
(UPMSP-ST).
(i) Obter a melhor operação a ser realizada para cada par-
Problema de escalonamento multiprocessador. Os resultados mostraram que o
problemas de marcação, bem como dois problemas de projeto do mundo real”. Os
resultados obtidos nos experimentos realizados com esses problemas
As arestas identificadas pela aprendizagem são utilizadas para reordenar as listas de
encontrado em Toth e Vigo (2002) e Toth e Vigo (2014).
suas estruturas de resolução de problemas de otimização.
Neste problema, um conjunto K = {k : k = 1, 2, . . ., |K|} de veículos está localizado
em um único depósito e deve atender a um conjunto C = {i : i = 1, 2, . . ., N}
Segundo os autores, “a eficácia do RLMPSO tem sido
agentes. Os resultados mostram que, com um nível de confiança de 95%, cenários
com oito ou mais agentes apresentam melhor desempenho do que o cenário
protocolo. Este protocolo de cooperação permite a comunicação entre
Problema (TSP) (Applegate, Bixby, Chvátal, & Cook, 2007), em que,
Aprendizagem por Reforço no controle das operações aplicadas a
que determinam a sequência em que devem ser atendidos
agentes e farão parte das próximas soluções. Os implementados
dois problemas de otimização diferentes: (i) Permutation Flow-shop
Martin et al. (2016) apresentam uma estrutura distribuída baseada em agentes
chamada Multi-Agent Cooperative Search (MACS). Esta estrutura é implementada
usando a plataforma JADE (Bellifemine, Poggi,
várias publicações abordam esta questão a partir de diferentes pontos de vista
do UPMSP-ST podem ser encontrados em Rabadi, Moraga e Al-Salem (2006) e
Vallada e Ruiz (2011), e boas pesquisas sobre esse problema são (Allahverdi, 2015;
Allahverdi et al.,
função de aptidão obtida. A recompensa imediata será calculada de acordo com
a avaliação da função de aptidão; se uma
tem a mesma capacidade Q. Cada cliente i tem uma determinada demanda
entre agentes ocorre da seguinte forma: bons padrões que melhoram
seu desempenho e mostrar seu potencial. Os problemas considerados aqui para fins
de estudo de caso são o Problema de Roteamento de Veículos com Janelas de
Tempo (VRPTW) e o Problema de Programação de Máquinas Paralelas Não
Relacionadas com Tempos de Configuração Dependentes de Sequência
é duplicado.
pode ser descrito como:
excelentes revisões da pesquisa envolvendo este problema podem ser
valor de avaliação; e a segunda etapa mantém as soluções de acordo com sua
distribuição espacial, com base em uma medida de dissimilaridade definida. Na
proposta de Lotfi e Acan (2015), o sistema multiagente é avaliado experimentalmente
usando instâncias do
apresentar o controle do Q-learning sob as operações possíveis:
cada conversa para influenciar como novas soluções são construídas.
3.1.1. Definições básicas do VRPTW
acima nesta revisão de literatura usa aprendizagem por reforço entre
(iii) Atualize a tabela Q para cada partícula atual.
continuamente ao longo do processo de busca usando uma cooperação
Agente único; (ii) 4 agentes; (iii) 8 agentes; (iv) 12 agentes; e (v) 16
método e heurística de busca local, os autores propõem a utilização de
reúne todos os objetos de borda e os pontua de acordo com sua frequência. Esses
elementos são compartilhados com os outros
problemas. É uma generalização do clássico Caixeiro Viajante
em que cada rota é representada por uma lista ordenada de clientes
(ii) Executar a operação selecionada e avaliar o valor da
Rabe e Figueira (2015) são utilizados para a construção de novas soluções. Martin
et al. (2016) mostram a avaliação do MACS através
proposto para o problema.
de indústrias. Assim como o VRPTW, o UPMSP-ST pertence à classe de problemas
NP-Hard, o que justifica o uso de métodos metaheurísticos na solução desses
problemas. Uma descrição detalhada
envolvendo agentes tornou-se um tema relevante nos dias de hoje, e uma grande
seis problemas de referência compostos, cinco problemas de referência deslocados e rotacionados
a frota de veículos é homogênea, ou seja, todos os veículos são iguais e
protocolo de comunicação é chamado de conversação. Comunicação
Esta seção aborda a instanciação do AMAM para dois problemas de grande
importância na Otimização Combinatória. O propósito desta instanciação é analisar
esta estrutura, avaliar
em alguns casos o desempenho é melhor quando o número de agentes
tem um valor associado cij, que representa o custo de viagem entre o cliente i e o
cliente j. A solução x mostrada na Fig. 1(b)
a descrição deste problema está na Seção 3.1, sob os termos de interesse deste
artigo atual. Para mais detalhes sobre VRPTW,
memória de bons objetos de borda, que são usados no início de
arquivo: o primeiro estágio armazena soluções promissoras com base em suas
O aprendizado é implementado usando o algoritmo Q-learning. Cada partícula está
sujeita a cinco operações: exploração, convergência, salto alto, salto baixo e ajuste
fino. Três etapas, descritas abaixo,
usando 5 cenários para cada um dos dois problemas mencionados: (i)
É importante ressaltar que nenhuma das propostas discutidas
qi e deve ser servido dentro de uma janela de tempo especificada [ai, bi] (ver
MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171 151
x = [0, 2, 1, 12, 0, 3, 4, 5, 6, 0, 10, 7, 8, 9, 11, 0]
Machine Translated by Google
a vizinhança de x1 (Milano & Roli, 2004).
modelo de aprendizagem que será descrito na Seção 5. Essas estruturas são
apresentadas a seguir:
de acordo com:
distância entre os clientes;
onde:
terminam no depósito. No nosso caso, o custo de uma solução x é calculado
necessárias porque constituem o conjunto de estados definidos na
onde o índice 0 indica o depósito e as três rotas
Uma vizinhança é uma função N (x) que descreve uma solução
• cij : custo entre clientes (i, j), que pode ser relacionado ao
veículos (ou rotas, em consequência). Em caso de empate no número
• K(x) : número de veículos na solução x;
• E : conjunto de arcos pertencentes à solução x;
3.1.2. Bairros VRPTW
espaço do problema. Cada solução deste subconjunto é chamada de vizinho.
A função N (x) é definida como um operador que recebe uma solução x1 e a
transforma em outra solução x2, pertencente a
e rota3 = [0, 10, 7, 8, 9, 11, 0]. Assim, a solução x também é descrita como x =
[rota1,rota2,rota3].
Nesta função, a primeira prioridade é minimizar o número de
(i) Swap Intra-Route: função de vizinhança que realiza o movimento de troca
de um cliente com outro cliente da mesma
desta solução são route1 = [0, 2, 1, 12, 0], route2 = [0, 3, 4, 5, 6, 0]
subconjunto associado à solução x pertencente à solução
de veículos, a distância total percorrida deve ser minimizada.
• ÿ : um fator de penalidade arbitrário grande e não negativo.
Para explorar o espaço de soluções, oito funções de vizinhança diferentes
são usadas na instanciação AMAM para resolver
a rota está associada a um único veículo. As rotas devem começar e
VRPTW. O conhecimento dessas estruturas de vizinhança torna-se
O objetivo do VRPTW é determinar um conjunto de rotas em
para minimizar o custo total envolvido com esta operação. Cada
cij
(i,j)ÿE
(1)f(x) = ÿK(x) +
Fig. 1. Uma solução VRPTW.
MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171152
Fig. 2. Uma aplicação da função de vizinhança Intra-Route Swap em 2 rotas de uma solução.
Fig. 3. Uma aplicação da função de vizinhança Inter-Route Swap em uma solução.
Machine Translated by Google
2 6
Fig. 5. Uma aplicação da função de vizinhança Inter-Route Shift em uma solução.
71
m1 67 3 m2 43
40 29 26 46 49 7
Fig. 4. Uma aplicação da função de vizinhança Intra-Route Shift na rota 2 de uma solução.
Tempo de processamento.
4
Tabela 1
3
29 85 11 36 25 12
85
153
5
MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171
máquina i ÿ M. As Tabelas 1 e 2 apresentam um exemplo desta situação,
Função de deslocamento intra-rota, dois clientes consecutivos são removidos
de suas posições e reinseridos em outra posição
rota e posição para inserção de cada cliente removido são
rota. A Fig. 2 ilustra essa função de vizinhança. Neste exemplo, os clientes 4 e
6 da rota 2 são trocados;
A função de vizinhança Inter-Route Shift é mostrada na Fig. 5, em
com 8 empregos e 2 máquinas, proposto por Vallada e Ruiz (2011).
Uma solução para o UPMSP-ST é uma lista de máquinas, onde cada máquina
é representada por uma lista ordenada de trabalhos, que define o
na rota 3;
os clientes da menor rota são reinseridos em outras rotas.
função. Nesta figura, o cliente 6 é removido da rota 2
3.2.1. Definições básicas do UPMSP-ST
Solução UPMSP-ST na qual cada tarefa atribuída à sua máquina tem
(vii) Elimina Rota Menor: função de vizinhança que busca
1) é excluído e seus clientes são inseridos em outras rotas
consequentemente transferido para a rota 2 no local anteriormente
a troca de clientes na mesma rota, bem como a
ser alocado a um conjunto M = {m : m = 1, . . ., |M|} de máquinas, e
e m2. Como o problema é dependente da sequência, a matriz é
são removidos e inseridos em outras rotas da solução.
(ver Figura 7);
(iii) Deslocamento Intra-Rota: função de vizinhança que realiza a mudança de
realocação de um cliente para outra posição na mesma rota.
(viii) Elimina a rota aleatória: A função de vizinhança Elimina a rota aleatória opera de
forma semelhante à função Elimina a rota menor.
é definida como a rota que possui o menor número de clientes.
Função de troca intra-rota, dois clientes consecutivos são trocados por outros
dois clientes consecutivos da mesma rota.
versão do UPMSP-ST considerada aqui, também definimos o tempo de
processamento pij, que representa o tempo necessário para processar o trabalho
(vi) Dois deslocamentos intra-rota: função de vizinhança que consiste em
(ii) Inter-Route Swap: função de vizinhança que realiza o movimento de troca de um
cliente de uma rota com um cliente de outra rota. A Fig. 3 mostra a vizinhança
Inter-Route Swap
aqueles que resultam no melhor valor da função objetivo, respeitando todas as
restrições. Uma nova solução é gerada quando todas
que o cliente 12 é retirado da rota 1 e então inserido
(iv) Inter-Route Shift: função de vizinhança que realiza a realocação de um cliente de
uma rota para outra.
o tempo necessário para configurar o trabalho k ÿ N após o trabalho j ÿ N no
função de vizinhança de deslocamento intra-rota. No entanto, no Dois
A Fig. 6 mostra um exemplo da função de vizinhança Eliminates Smaller Route.
Neste exemplo, a menor rota (rota
Neste problema, um conjunto N = {n : n = 1, 2, . . ., |N|} de empregos deve
e inserido na rota 3 no lugar do cliente 7, que é
(v) Duas trocas intra-rotas: função de vizinhança que consiste em
3.2. Caso 2: UPMSP-ST
com dados referentes a instâncias de teste para experimentos de calibração
A Tabela 1 exibe os tempos de processamento de cada um dos 8 trabalhos nas
máquinas m1 e m2 . A Tabela 2 mostra os tempos de configuração para as máquinas m1
(rotas 2 e 3);
elimine a menor rota da solução. A menor rota
ocupado pelo cliente 6;
função de vizinhança de troca intra-rota. No entanto, no Two
cada trabalho j ÿ N deve ser alocado a uma única máquina i ÿ M. No
usado para indicar o tempo de configuração do trabalho i após a execução do trabalho j.
da mesma rota;
sequência em que serão realizados. A Fig. 8 ilustra uma
rota;
rota. A Fig. 4 mostra a aplicação da função Intra-Route Shift, onde o cliente 6
da rota 2 é retirado de sua posição e inserido entre os clientes 4 e 5;
Função de rota, mas a rota a ser excluída é escolhida aleatoriamente
j ÿ N em uma máquina i ÿ M; e o tempo de configuração Sijk, que representa
a realocação de clientes na mesma rota, bem como a
Para tal, os clientes da menor rota da solução
Machine Translated by Google
(ii) Inserção Múltipla nas Mesmas Máquinas: função de vizinhança que realiza
o movimento de realocação de uma tarefa para outra
VRPTW, essas estruturas de vizinhança compõem o conjunto de estados
para outro. A Fig. 9 mostra a aplicação desta função de vizinhança na
qual o trabalho 7 é removido da máquina m1 e inserido na máquina m2.
Como pode ser visto,
o makespan. Na verdade, antes da mudança, o makespan era 167
seu tempo de processamento representado na linha do tempo e nos intervalos
Quatro funções de vizinhança foram usadas para explorar a busca
pela máquina m1, no valor de 167; após aplicação
(Fig. 10(b)).
exemplo são baseados na instância das Tabelas 1 e 2.
pelo tempo de conclusão da máquina m1 e é dado pelo valor
A Fig. 9(b)) é novamente obtida pela máquina m1, com um valor de 134.
realiza o movimento de troca de um trabalho de uma máquina com um trabalho
3.2.2. Bairros UPMSP-ST
na Fig. 11, em que o trabalho 1 da máquina m1 é trocado
posição na mesma máquina. A Fig. 10 mostra a aplicação de
lema é apresentado na Seção 5. Essas estruturas são apresentadas
espaço do UPMSP-ST na instanciação do framework. Como em
(i) Inserção Múltipla em Máquinas Diferentes: função de vizinhança que
realiza a realocação de um trabalho de uma máquina
a última posição da máquina m1 e inserida na segunda posição desta
mesma máquina. Esta modificação reduz
o makespan da solução inicial da Fig. 9(a) é obtido
(Fig. 10(a)) e, após a mudança, o makespan foi reduzido para 164
entre os trabalhos representam o tempo de configuração. Os valores usados neste
definido na aprendizagem do agente. O modelo de aprendizagem usado neste prob-
(iii) Troca entre máquinas diferentes: função de vizinhança que
da função de vizinhança, o novo makespan (solução de
O objetivo do UPMSP-ST é alocar todos os n trabalhos em m máquinas
para minimizar o tempo máximo de conclusão do agendamento, valor
conhecido como makespan. Na Fig. 8, o makespan é definido
esta função de vizinhança na qual o trabalho 8 é removido de
abaixo:
com o trabalho 6 da máquina m2. Neste caso, a aplicação
de outra máquina. Essas funções de vizinhança são mostradas
167.
6
9
5
3
0
4
5
7
3 4 5
2
4 9
7 9
12
8
2
3
5
Tabela 2
6
8 5 6
8 3 6
7 2 2 8 6 8
2 0 3 8
2
6 3 4
3 4
Fig. 7. Uma aplicação da função de vizinhança elimina a rota aleatória em uma solução.
3 8
2
6
1
4 1
7
8 8
7
1
4 3
5
Fig. 8. Uma solução UPMSP-ST.
MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171
8
8 8 6
6 6 6
0 6 5
0 2 2
4
5
6 3
Fig. 6. Uma aplicação da função de vizinhança elimina rotas menores em uma solução.
2
7
0
9
6
2
7 2 0
1
5
7 0
8
7 8 m2
2 4 9 8 9 2
8 6 4
6 5 0 1 7
6 7
2 9
154
0 5 7
0 5 2
6 9 4
8 4 2
7
6 7
8 0
1
1
3 4
4 0 9
7
Tempo de configuração.
6
0 2 3 1 4 0 5 3 1
3 0 5 4 2 5 0 6 5
6 6 3 9
7
m1
Machine Translated by Google
Fig. 9. Exemplo: Inserção múltipla em vizinhança de máquinas diferentes.
155MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171
Fig. 10. Exemplo: Inserção múltipla na mesma vizinhança de máquinas.
Fig. 11. Exemplo: Troca entre diferentes vizinhanças de máquinas.
4. Estrutura de otimização metaheurística multiagente
As ações disponíveis para cada agente definem a visão que ele terá do ambiente.
Portanto, sua representação do ambiente é parcial. O objetivo é aplicar, ao mesmo
tempo, os pontos fortes de cada metaheurística por meio do trabalho cooperativo dos
agentes. A escalabilidade da arquitetura AMAM é garantida pela facilidade de
adicionar novos agentes, com impacto mínimo no restante da arquitetura. Esses
agentes interagem com o ambiente e com outros agentes cooperativamente, trocando
e compartilhando informações sobre sua condição e sobre o ambiente.
O agente adaptativo aqui proposto é incluído como parte integrante do framework
AMAM. Ele surge de formulações iniciais apresentadas em Silva (2007), Fernandes
et al. (2009) e Silva et al.
(2014, 2015). Neste framework, cada agente encapsula uma heurística/metaheurística
e tem a função de buscar a solução para um dado problema de Otimização
Combinatória.
• Percepção do ambiente: capacidade dos agentes de acessar informações sobre
o problema que lhe são necessárias; • Posicionamento: capacidade
dos agentes de definir suas posições no ambiente, seja pela construção de uma
nova solução ou pela escolha de soluções já disponíveis; • Movimentação:
capacidade do agente de se mover, de uma solução
para outra no ambiente. A movimentação aqui compreende todos os tipos de
modificações de solução (estruturas de vizinhança, operadores) que permitem
ao agente se mover de uma solução para outra; • Cooperação: capacidade do
agente de compartilhar e fornecer soluções
para os outros agentes do sistema.
as capacidades de percepção e ação do agente são definidas neste ambiente como:A aplicação do movimento leva a uma redução significativa do makespan da
solução, uma vez que, na solução inicial mostrada na Fig. 11(a), o valor do
makespan é 167, e, na solução resultante (Fig. 11(b)), o valor obtido é 131. (iv)
Troca entre Mesmas Máquinas: função de vizinhança que
realiza o movimento de troca de um job com outro job da mesma máquina. Essas
funções de vizinhança são mostradas na Fig. 12, na qual o job 1 da máquina m1
é trocado com o job 8 desta mesma máquina. A solução inicial é mostrada na
Fig. 12(a), com makespan 167. A solução obtida pela aplicação deste movimento
é mostrada na Fig. 12(b), com makespan resultante avaliado em 163.
Durante o processo de busca da solução, os agentes no framework devem
passar pelo ambiente do sistema multiagente. Neste caso, o ambiente multiagente
é definido pelo espaço de busca do problema endereçado. Conforme mostrado na
Fig. 13, o
O paradigma da Programação Orientada a Objetos é utilizado para facilitar o
desenvolvimento do framework, permitindo reduzir o esforço
Machine Translated by Google
a possibilidade de execução paralela, na qual cada agente é executado em um
na estrutura é atender à necessidade de aumentar a autonomia dos
melhora o resultado final e reduz o tempo necessário para resolver o problema
comum a ambas as soluções. A Fig. 14 apresenta dois exemplos de cálculo de
distância entre soluções VRPTW. Para o primeiro exemplo,
das soluções, sendo um parâmetro do problema. Por exemplo,
Seção 5.
utilizados na implementação de métodos e na adaptação de
muito próximo do outro. Portanto, seu valor depende diretamente do
A função (2) estima a densidade da solução na vizinhança da solução i por meio
da distância entre as soluções contidas no pool. O valor ÿij mede o quanto
Fernandes et al. (2009) e Silva et al. (2014). Uma estratégia de memória adaptativa
chamada Pool of Solutions é usada para compartilhar informações. As soluções
disponíveis são armazenadas neste Pool of Solutions,
através da troca de informações no espaço de busca do
elementos:
(iii) Agentes Metaheurísticos: responsáveis por orientar a busca pela
a inserção de novas soluções é regulada por uma função de avaliação, como em
Silva et al. (2015). Esta função de avaliação é baseada em
é definida pela soma das distâncias de uma solução i a todos os
O modelo conceitual é apresentado em Fernandes et al. (2009), e permite a criação
de uma instância do ambiente e múltiplos agentes para a busca da solução. Padrões
de projeto são utilizados
problema. Portanto, ele fornece todas as informações necessárias
conceitos de cooperação e paralelismo. Além disso, a AMAM oferece
no espaço de soluções para as áreas mais promissoras e, assim,
relação à proposta original. O principal objetivo desta mudança
VRPTW descrito na Seção 3.1, a distância entre duas soluções é calculada em
relação ao número de arcos que não são
O fator pr é o raio do pool e controla o grau de dispersão
são atribuídos a agentes de estrutura, usando os princípios definidos
aprendizagem, é apresentada na presente proposta e detalhada em
o número de veículos, e assim por diante;
Agente Coordenador; e (vi) Agente Analisador de Soluções. A estrutura cooperativa
da arquitetura AMAM é revisada e melhorada em
A cooperação entre agentes ocorre, na versão atual,
é o número mínimo de arcos para que uma solução seja considerada
A nova estrutura do quadro AMAM é composta por três principais
(ii) Pool de Soluções: responsável por manter as soluções compartilhadas para todos
os agentes;
O tamanho máximo do conjunto de soluções é predefinido e
O modelo conceitual do framework AMAM foi originalmente proposto em Silva
(2007). O desenvolvimento a partir do
capacidade de metaheurísticas através de uma abordagem multiagente, utilizando
onde P é o número de soluções no conjunto e ÿij é a distância entre as soluções i e
j. A função de avaliação g(ÿi)
(i) Ambiente: definido principalmente pelo espaço de busca do objeto abordado
o problema que está sendo tratado. Como exemplo, considere o caso de
ambiente e compartilhado pelos agentes no final de cada iteração. O objetivo desta
estrutura cooperativa é orientar os agentes
Os agentes do Solution Analyzer foram removidos da arquitetura, em
Em Silva et al. (2015), as habilidades autoadaptativas baseadas na aprendizagem
novo agente adaptativo, incorporando a abordagem de reforço por
para coordenar arquivos de solução. Quando uma solução precisa ser inserida no
pool e não há espaço disponível neste pool, as soluções existentes são avaliadas
de acordo com a função:
de clientes a serem atendidos, a distância entre os clientes,
tópico separado.
o Sistema Multiagente proposto: (i) Ambiente; (ii) Agente Construtor; (iii) Agente de
Busca Local; (iv) Agente Metaheurístico; (v)
o agente, impedindo que outros agentes interfiram em suas atividades.
problema.
para VRPTW, usado como um dos estudos de caso no artigo atual,
a distância entre a solução i, mostrada na Fig. 14(a), e a
estes para um problema específico. Portanto, uma estrutura genérica que permite a
definição das características do problema é usada.
A força da estrutura proposta é a hibridização
tamanho da instância a ser resolvida do problema abordado.
as soluções i e j são semelhantes e dependem fundamentalmente de
problema. As soluções disponíveis são armazenadas em um pool de soluções em
localizado no ambiente do Sistema Multiagente. Na última proposta, apresentada
em Silva et al. (2014, 2015), o Coordenador e
solução.
as técnicas de niching (Li, Epitropakis, Deb, & Engelbrecht, 2017)
em Aprendizagem de Autômatos (Narendra & Thathachar, 1974). Finalmente, um
outras soluções de pool, onde ÿ(ÿij) é definido como:
para garantir que a arquitetura AMAM seja flexível e extensível. Neste modelo
conceitual inicial, seis elementos principais compõem
para resolver o problema, ou seja, no caso do VRPTW, o número
se ÿij > pr
se ÿij ÿ pr (3)
,
(2)
0,
1 ÿ
g(ÿi) =
RP
ÿ(ÿij)
ÿ(ÿij) =
j=1
P
ÿij
Fig. 12. Exemplo: Troca entre vizinhanças de Mesmas Máquinas.
MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171156
Fig. 13. Interação do agente.
Machine Translated by Google
valor da função objetivo do que a pior solução do pool. Esses critérios para a
inserção de uma nova solução são propostos no atual
A versão mais recente do framework AMAM, lançada em
escolha é atribuída. Inicialmente, todos os pares de sequências têm a mesma
O objetivo principal é levar o agente a selecionar uma sequência de ações até o
estado objetivo, que maximize o reforço acumulado ao longo do tempo. Assim, uma
política de controle/decisão é gerada,
O principal objetivo desta função de avaliação é manter
são descritos nas subseções seguintes. Inicialmente, na Seção 5.1,
reunir experiências para melhorar seu desempenho.
arcos não comuns entre essas soluções. Para o segundo exemplo, a distância
entre a solução i, mostrada na Fig. 14(c), e
soluções iguais. Ao mesmo tempo, a melhor solução existente no
são introduzidos. Em seguida, na Seção 5.3, os detalhes de implementação de
experiência passada, e um sistema de aprendizagem é caracterizado por sua
capacidade de melhorar seu comportamento com o tempo, em algum sentido
tendendo a um objetivo final”. Na aprendizagem por reforço, o comportamento é
melhorado a partir de recompensas obtidas durante as interações de
AMAM-Multiagente-Arquitetura-para-Metaheurísticas, sob a
m2) é atualizado por um fator de reforço w se um movimento da estrutura de
vizinhança m2 aplicado após outro movimento da estrutura de vizinhança m1
melhora a solução atual. O conceito
são 6 arcos não comuns entre essas soluções.
atualizado a cada inserção, evitando assim que esta melhor solução
5.1. Aprendizagem por reforço
a percepção (i) do estado do indivíduo no ambiente; (ii) das ações realizadas neste
ambiente; (iii) da5. Agentes adaptativos
(Narendra & Thathachar, 1974).
artigo e é uma contribuição original do artigo atual.
No presente artigo, a escolha da ordem de aplicação do
Algoritmo Q-Learning. Os detalhes relativos a esta implementação
as interações são reforçadas de acordo com os efeitos que causam
inserção de uma nova solução no pool, se esta nova solução satisfizer dois critérios:
(i) ainda não estiver no pool; (ii) tiver melhor
que o ambiente retorna em resposta à ação realizada.
Em Silva et al. (2015), a ordem dos bairros em áreas locais
a diversidade do pool, evitando manter elementos muito semelhantes ou até mesmo
conceitos relacionados ao Aprendizado por Reforço e o algoritmo utilizado
a solução j, mostrada na Fig. 14(b), é igual a ÿij = 12, ou seja, há 12
não há pares de entrada/saída e, portanto, o agente precisa
Segundo Narendra e Thathachar (1974), “a aprendizagem é definida como
qualquer mudança relativamente permanente no comportamento resultante de
tomada de decisão subsequente.
Dezembro de 2017, está disponível em https://github.com/mamelials/
a solução h, mostrada na Fig. 14(d), é igual a ÿih = 6, ou seja, há
pool é sempre armazenado em um atributo específico do ambiente e
esta proposta são apresentadas.
o agente com o ambiente. A aprendizagem ocorre por meio
Licença GNU LGPLv3.
operador de seleção “roda” de Algoritmos Genéticos. Para cada par possível de
estruturas de vizinhança (m1, m2), uma probabilidade de
valor de probabilidade. A probabilidade de escolha da sequência (m1,
Como consequência, após avaliar o valor g(·) de cada solução no pool, a pior
solução avaliada é excluída para o
encontrado é eliminado.
O Aprendizado por Reforço consiste em aprender o que fazer em um ambiente
dinâmico a partir de interações baseadas em tentativa e erro. Essas
mudanças de estado resultantes dessas ações; e (iv) da recompensa
Esta seção introduz as capacidades adaptativas dos agentes de framework.
Ela estende as características de aprendizagem inicialmente abordadas em Silva
et al. (2015) para o mesmo framework.
que foi usado é muito semelhante ao definido em Learning Automata
Um sistema de Aprendizagem por Reforço (RL) inclui três aspectos básicos: (i)
percepção; (ii) ação; e (iii) objetivo. Neste sistema, como
mostrado na Fig. 15, o agente percebe (parcialmente) o estado do ambiente e, com
base no conhecimento obtido por meio dessa percepção, seleciona uma ação a ser
executada. A ação tomada afeta
o ambiente. No modelo definido pela aprendizagem por reforço
as estruturas de vizinhança da busca local são melhoradas usando o
Por meio da aprendizagem, o agente utiliza o valor de reforço no
a busca é escolhida aplicando um operador semelhante à “roleta
o ambiente, alterando o estado em que o agente se encontra. Cada
agente dentro de um sistema RL tem um estado objetivo que deve ser alcançado.
caracterizado pelo mapeamento de estados e ações, representando
157MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171
Fig. 14. Exemplos de cálculo de distância entre soluções.
Machine Translated by Google
TR = rt+1 + ÿ rt+2 + ÿ 2rt+3 + . . . + rT = ÿ krt+k+1
RT = rt+1 + rt+2 + rt+3 + . . . + rT
k=0
k=0
ÿ
ÿ krt+k+1 | st =s, em =a
ÿ
ÿ
ÿ krt+k+1 | st = s
k=0
um
a taxa de aprendizagem e o fator de desconto são parâmetros que dependem
sobre a política e é definido por:
Processo de Decisão de Markov (MDP) (Bellman, 1957; Bertsekas, 1987;
realizado na solução atual para encontrar seu melhor vizinho. Este método emprega
uma ordenação de vizinhança determinística, sendo este esquema de ordenação de
vizinhança um parâmetro a ser
ambiente. Assim, a convergência para a política ótima ÿ expressa
O algoritmo Q-Learning, introduzido por Watkins e
repita para cada etapa do episódio
(vii) Função Valor: valor obtido com o mapeamento do estado
maximizar o valor da função Q(s, a), definida como:
O sistema RL busca maximizar a recompensa esperada em função da sequência
de valores recebidos até um instante de tempo
é definido aqui como uma sequência de estados que variam de um estado inicial
Pesquisa (ALS-QLearning) baseada na heurística Variable Neighborhood De-scent
– VND (Mladenovic´ & Hansen, 1997) e na
(iv) Política de Controle/Decisão: define o comportamento do agente para
Algoritmo 1 Algoritmo Q-Learning.
em relação ao comportamento do agente. O objetivo é maximizar esse feedback
recebido do ambiente. Para atingir
até que s seja terminal
onde 0 ÿ ÿ ÿ 1. Assim, se ÿ = 0, os reforços imediatos são
e é denotado por V (s). A função valor-estado depende
o agente, através do algoritmo Q-learning, atualiza sua função
função, a função valor-ação depende da política
mento;
ÿ é a taxa de aprendizagem; ÿ é o fator de desconto; s é o estado atual; a é a ação
tomada; e s é o estado resultante.
Observe o próximo estado s e a recompensa r;
apresenta a estrutura VND padrão. Como visto, para cada vizinhança N (k)
selecionada pelo VND padrão, uma busca local é
As funções valor-estado e valor-ação podem ser modeladas pela
Os principais elementos que compõem a formulação do Rein-
função define a probabilidade de que uma ação a seja escolhida em um estado
s . Essas probabilidades mudam à medida que o agente acumula experiências
como consequência das interações com o
repetir
Inicializar s;
problema abordado.
(b) Função Valor-Ação: função que considera o par estado-ação e é denotada
por Q(s, a). Como na função valor-estado
modelo (Kaelbling, Littman, & Moore, 1996). Neste modelo, o
Processo (MDP). O objetivo é, em cada etapa de um episódio,
utilidade esperada de tomar uma ação a em um dado estado s. Um episódio
Esta seção mostra o agente adaptativo baseado em RL. A capacidade adaptativa
é atribuída ao agente por meio de um Adaptive Local
e deve ser pelo menos parcialmente observável;
(v) Reforço/Recompensa: mostra o feedback do ambiente
s ÿ s ;
(a) Função Valor-Estado: função que considera apenas o estado
livre de modelos e para dispensar conhecimento de uma política. Desta forma,
(i) Conjunto de Estados: conjunto de todos os estados possíveis que descrevem o ambiente
fator ÿ é aplicado à expressão de reforço. Como con-sequência:
Tome uma atitude ;
VND é uma heurística de refinamento que explora o espaço de so-luções pela
troca sistemática de vizinhanças. Algoritmo 2
o comportamento que o sistema RL segue até atingir o objetivo.
Inicializar Q(s, a) arbitrariamente;
s em ações a e é expressa pela função (s, a). Isto
(vi) Função de Reforço (Função de Recompensa): as funções de reforço nem sempre
são simples de definir e variam de acordo com a
tomada de decisão feita no momento presente. Existem vários modelos que
definem como o agente deve acumular as recompensas recebidas. O mais
utilizado é o modelo descontado de horizonte finito
seleção ótima de políticas de ações para qualquer decisão de Markov finita
Q(s , a ) ÿ Q(s, a) ;
conforme mostrado no pseudocódigo apresentado no Algoritmo 1, onde
5.3. Agente adaptativo baseado em RL
diretamente no problema tratado. A função Q estima o
Puterman, 1994). Uma boa revisão da literatura sobre Aprendizagem por Reforço é
conduzida por Kaelbling et al. (1996).
(iii) Ambiente: o ambiente no problema RL é dinâmico
o processo de aprendizagem no problema RL;
Dayan (1992), destaca-se por ser amplamente utilizado, por ser
Escolha um de s usando uma política derivada de Q (por exemplo, -
Considerando as recompensas recebidas a longo prazo, um desconto
ou do par ação-estado, das recompensas atuais e futuras.
Problemas de Aprendizagem de Força são:
ambicioso);
T, na forma:
para o estado final.
conceitos de Aprendizagem por Reforço.
5.2. Algoritmo de aprendizagem Q
1: procedimento QLearning(r, a, , c )
atingir a meta em qualquer momento. Uma política de controle mapeia estados
até atingir o número de episódios
para cada episódio
maximizado; se ÿ = 1, a mesma importância é dada aos ganhos imediatos e
futuros;
valor, seguindo qualquer política. Este algoritmo permite encontrar um
e é definido por:
esse objetivo, o agente deve considerar o comportamento futuro no
(ii) Conjunto de Ações: conjunto de todas as ações disponíveis;
Q(s, a) ÿ Q(s, a) + ÿ r + ÿ máx.
3:
11:
Fig. 15. Interação agente-ambiente na aprendizagem por reforço (Sutton &
2:
13: procedimento final
4:
8:
5:
7:
Barto, 1998).
10:
9:
6:
MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171
12:
158
Q(s , a ) ÿ Q(s, a)
(4)
(5)
V (s) = E{Rt | st = s} = E
Q(s, a)=E{Rt | st =s, em =a}=E
Q(s, a) = Q(s, a) + ÿ r + ÿ máx. (8)
(7)
(6)
um
Machine Translated by Google
recompensa acumulada. Cada bairro a ser utilizado pela busca
• Conjunto de Ações A(s): uma ação é definida como a mudança de uma
as 8 funções de vizinhança listadas na Seção 3.1.2:
ou seja, o número de funções de vizinhança do tratado
• Recompensa R: baseada no valor da função de aptidão do
solução x obtida com a aplicação da função de vizinhança atual N (x). A
maneira como esta transformação
1: procedimento chooseAnAction(estado, tipo_função)
Como as funções de vizinhança são parâmetros específicos do
O Processo de Decisão de Markov (MDP) para esta proposta é definido
(b) Para o estudo de caso UPMSP-ST: o conjunto de estados é formado por
representado por um gráfico completo, no qual cada ação é representada por
um arco conectando dois estados (nós do gráfico).
ocorre é apresentado abaixo.
Algoritmo 3 Função de escolha da próxima ação.
ordem de crescimento dessas estruturas. (ou seja, a ordem de aplicação é
predefinida), mas essa ordem nem sempre produz a melhor solução
se type_função = 1 então
estados (funções de vizinhança) e ações possíveis são mostrados
• Conjunto de Estados S: os estados são as funções de vizinhança, disponíveis
k ÿ 1;
outro
(Subramanian, Drummond, Bentes, Ochi, & Farias, 2010). Se a solução encontrada
for melhor que a solução atual, a primeira função de vizinhança é usada novamente;
caso contrário, a próxima função de vizinhança é usada novamente.
k ÿ 1;
problema sendo resolvido, o propósito de usar o aprendizado por reforço
kmax é o número de diferentes
k ÿ k + 1;
próximo_estado ÿ randomAction();
x ÿ melhorVizinho(x, N (k));
O VND retorna um ótimo local em relação a todas as vizinhanças exploradas.
do VRPTW são usados para facilitar a visualização. A tabela Q ,
os problemas de teste usados aqui, temos:
Algoritmo 2 Descida de Vizinhança Variável (VND).
próximo_estado ÿ 0;
e, a partir daí, premiar as melhores sequências e maximizar a
dimensões dadas por M × M, em que M é o número de estados,
1: procedimento VND(x, kmax)
estruturas de vizinhança
próximo_estado ÿ -greedy(estado);
porque a melhor ordem pode depender muito da instância
x ÿ x ;
as 4 funções de vizinhança listadas na Seção 3.2.2:
determinado. Em geral, este esquema é baseado na complexidade
O método é considerado, neste artigo, como um estado de aprendizagem.
estado para outro (“ir para”). Desta forma, o conjunto de ações pode ser
se type_função = 2 então
enquanto k ÿ kmax faça
outro
a função é usada até que não haja mais bairros disponíveis.
na Fig. 16(a). Neste exemplo, apenas quatro funções de vizinhança
Um exemplo de um gráfico que representa a relação entre
problema.
do seguinte modo:
fim se
fim se
Nesta proposta, a sequência em que as vizinhanças são aplicadas é definida
por meio de aprendizado por reforço, com base no algoritmo Q-Learning. O objetivo
principal é avaliar o ganho obtido com a aplicação de uma sequência de duas
vizinhanças,
referindo-se a este exemplo, é mostrado na Fig. 16(b). A tabela Q tem
(a) Para o estudo de caso VRPTW: o conjunto de estados é formado por
para que o problema seja tratado pelo framework. No caso de
se f(x ) < f(x) então
12: procedimento final
2:
7:
7:
5:
10: procedimento final
Fig. 16. Um gráfico representando a relação entre estados (funções de vizinhança) e ações possíveis.
6:
11: fim enquanto
8:
2:
6:
3:
4:
8:
10:
3:
9:
4:
9: fim se
159
5:
MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171
Trocar entre diferentes máquinas,
Inserção múltipla nas mesmas máquinas,
(9)
(10)
Elimina Rota Aleatória}
Supmspÿst = Inserção Múltipla em Diferentes Máquinas,
Elimina Rotas Menores,
Duas trocas intra-rota, duas mudanças intra-rota,
Svrptw = {Troca intra-rota, troca entre rotas,
Deslocamento intra-rota, Deslocamento inter-rota,
Troca entre as mesmas máquinas
Machine Translated by Google
a nova solução encontrada não é melhor que a atual, o valor a ser
problemas, uma vizinhança precisa ser aplicada a uma solução, de
estado ÿ próximo_estado;
para cada episódio
da pesquisa local atual.
meta
O algoritmo de busca local adaptável proposto neste artigo é
sem_melhoria + +;
se não
next_state ÿ chooseAnAction(0, 2); definido
pela função aleatória
1: procedimento adaptiveLocalSearchQLearnig(x0)
.
outro
uma heurística de busca local Descent/Ascent, apenas uma vez. Em consequência,
next_state = escolherUmaAção(estado, 1);
inicializar(Q(estado, ação));
Como o objetivo é definir o ganho obtido pela aplicação de uma
uma probabilidade ou uma ação que retorna a maior recompensa com
ser adicionado é zero.
Algoritmo 5 - função gananciosa.
x ÿ melhorVizinho(próximo_estado, x);
A recompensa usada é baseada na função de aptidão da solução
para que o processo de aprendizagem seja realizado de forma imparcial.
fim se
x ÿ x;
next_state = chooseAnAction(0, 2);
melhorado, a função gananciosa não deve ser usada
apresentado no Algoritmo 4. Neste algoritmo, primeiramente, a tabela Q é
se (o estado foi visitado) então
melhorado ÿ verdadeiro;
>
melhorado ÿ verdadeiro;
fim se
A segunda parte do episódio é executada até que o objetivo seja alcançado
os estados visitados (bairros) são registrados. Quando todos os estados
e seu cálculo depende do tipo de problema a ser tratado.
Então, para cada episódio relacionado ao algoritmo Q-Learning,
fim enquanto
ação de retorno;
ção. Para isso, na primeira parte, o estado inicial é selecionado aleatoriamente. A
função chooseAnAction(state, type-function) , apresentada
estados_visitados_contagem + +;
1: função -greedy(estado_atual, , tamanho_q)
se (x é melhor que x) então
e
sem_melhoria ÿ 0;
x = melhorVizinho(próximo_estado, x);
no Algoritmo 3, é usado para determinar tanto o estado inicial (onde
foram visitados, a busca local é finalizada. O mesmo acontece
4: sem_melhoria ÿ 0;
sequência de dois bairros, a recompensa é calculada a partir da
uma probabilidade 1 ÿ O próximo estado também é aplicado à solução
fim se
2: p ÿ aleatório();
x ÿ x;
Considerando o tratamento dado à matriz Q no Q-Learning
a aprendizagem está associada ao processo de busca da solução
ÿ ÿ taxa_de_decaimento;
x ÿ x0;
type-function = 2 conforme mostrado na linha 6 do Algoritmo 3) e para determinar
os próximos estados com funções específicas (onde type-function = 1
se
se p ÿ então
1:
x ÿ x0;
recompensa ÿ recompensa + x.getFitnessLearning();
se (x é melhor que xÿ) então
soma dos valores obtidos por esses bairros, conforme mostrado na
através da melhor estratégia de melhoria (bestNeighbor(next-state, x)
meta do estado:
alcançado (linhas 17–40 do Algoritmo 4). O objetivo é encontrar um
recompensa ÿ x.getFitnessLearning();
algoritmo, o valor de aptidão recompensa(x) da solução x é avaliado
fim se
Esta seção apresenta os experimentos computacionais realizados
2: estado inicial
((sem_melhoria
max_iterações_sem_melhoria)
(contagem_de_estados_visitados = q_size)) então
ação ÿ aleatório(q_size);
outro
com base na função objetivo f(x) do problema. Então, em
para avaliar e testar o framework AMAM. Este framework é implementado em
linguagem Java com JDK 1.8, usando o IDE
calculateQValue(estado, próximo_estado, recompensa);
repetir
recompensa ÿ 0;
conforme mostrado na linha 3 do Algoritmo 3). Desta forma, esta função permite
6. Experimentos computacionais
solução melhor que a atual e, se alcançada, encerra o episódio atual. Os outros
estados a serem visitados no episódio são definidos usando a função -greedy
(Watkins, 1989), apresentada no
após um certo número de iterações sem melhoria. Para o
até (não(melhorado))
linhas 14 e 29 do Algoritmo 4. Em caso de melhoria do
– linha 24 do Algoritmo 4). Quando a solução melhora, a recompensa é atribuída,
seu valor na matriz Q é calculado, o valor
estados_visitados_contagem + +;
os problemas de maximização, recompensa(x) = f(x); na minimização
Elipse neon 2. Os experimentos foram executados usando um computador
retornar x;
chegou ao estado
estados_visitados_contagem ÿ 0;
novas funções de seleção para serem facilmente inseridas. O estado inicial é
melhorou ÿ falso;
inicializado. Esta tabela é responsável por armazenar os valores do
outro
Algoritmo 5. A função -greedy seleciona uma ação aleatória com
avaliação do agente adaptativo aqui introduzido, o seguinte
solução com a aplicação do bairro, o valor a ser agregado
de diminui com a taxa de decaimento, e o episódio termina.
melhorando a solução
para a recompensa é a função de aptidão da solução atual; se o
Considerando as características específicas da otimização
então aplicado à solução através da melhor estratégia de melhoria (bestNeighbor(next-
state, x) – linha 11 do Algoritmo 4).
pares estado-ação no curso do processo de aprendizagem. Nesta proposta, cada
elemento da tabela Q é inicializado com um valor zero,
fim se
ação ÿ maxAction(estado_atual);
enquanto (não atingiu a meta do estado) faça
problemas, recompensa(x) = f(x0) ÿ f(x), onde x0 é a solução inicial
com processador Intel i7 - 4500U, 1,8 GHz, 16 GB DDR3 RAM e
função gananciosa epsilon
é permitir que a estrutura se adapte melhor às características próprias deste
problema.
outro
se (nenhuma_melhoria = 0) então
Algoritmo 4 Busca Local Adaptativa baseada em Q-learning.
seção apresenta os experimentos realizados.
10:
3:
17:
9: fim da função
6:
4:
32:
34:
8:
11:
37:
24:
27:
39:
6:
41:
25:
29:
45: fim do procedimento
36:
13:
8:
43:
3:
MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171
31:
38:
19:
15:
21:
33:
5:
40:
9:
2:
5:
20:
42:
7:
12:
35:
23:
26:
44:
14:
18:
28:
22:
16:
7: fim se
30:
160
Machine Translated by Google
(v) Desempenho da equipe: a influência da aprendizagem individual no
O teste paramétrico utilizado foi a análise de variância ANOVA. Este teste
a cada iteração a função de perturbação é alterada se não houver
(ii) Hipótese alternativa (H1): a média das soluções obtidas usando SC1 é menor
que a média das soluções obtidas por SC2. Se a hipótese nula for rejeitada,
então há
15, 20 e 25 máquinas. Quanto ao VRPTW, o principal objetivo deste
mostrados aqui, os valores dos parâmetros RL (ver Algoritmo 1)
agente proposto em Silva et al. (2015) foram executados novamente.
de agentes coopera na busca da solução. Este item
x ÿ perturbação(x, perturbação_nível);
Proposta ALS-LA e quatro cenários relativos ao ALS-QLearning
nível_perturbação ÿ 1;
Para permitir a sua avaliação, o quadro proposto foi
Solomon (1987) . Essas instâncias são formadas por três
por meio de soluções obtidas pelas duas propostas (ALS-LA e
(i) um único agente ILS;
levantam-se as seguintes hipóteses para comparar as soluções médias obtidas em
cada uma delas:
Para esta avaliação, os testes foram feitos com o sistema adaptativo
fim se
com UPMSP-ST foram propostas por Vallada e Ruiz (2011) e
A função implementa a busca local adaptativa específica de cada proposta. A
proposta descrita em Silva et al. (2015) é chamada aqui
(ii) Desempenho da proposta ALS-QLearning: análise específica do
Os experimentos foram conduzidos com o objetivo de analisar o desempenho
do agente adaptativo aqui proposto. O objetivo principal
obtido pela proposta ALS-LA, comparando o desempenho
metaheurística (Lourenço, Martin, & Stützle, 2003), uma conhecida
1: procedimento eles
x ÿ x;
A composição do sistema multiagente, utilizado nos experimentos para cada
uma das propostas, envolveu agentes ILS idênticos e
sistema operacional Windows 7 Home Premium. É importante
(iv) oito agentes ILS idênticos;
e conjuntos de problemas R2. Nos conjuntos de problemas C1 e C2, as posições
geográficas são agrupadas. Além disso, uma mistura de estruturas aleatórias e
agrupadas é usada nos problemas RC1 e RC2
teste de hipótese paramétrico, com nível de confiança de 95%. O
e ALS-QLearning); e,
mudanças na solução atual, são implementadas em níveis, ou seja, em
as soluções obtidas pelos cenários analisados;
o Problema de Escalonamento de Máquina Paralela Não Relacionada com Tempos
de Configuração Dependentes de Sequência (UPMSP-ST). Uma descrição desses
problemas é apresentada na Seção 4.
(Adaptive Local Search - QLearning). Para os testes computacionais
os problemas utilizados envolvem combinações de 50 e 100 empregos, com 10,
aqui considerados e, por consequência, os testes com o adaptativo
outros cenários (2 agentes, 4 agentes e 8 agentes), nos quais um conjunto
enquanto (não atingiu a condição de parada) faça
dos oito cenários avaliados, sendo quatro cenários sobre o
estrutura exposta.
à qualidade dos resultados obtidos, tanto do ponto de vista individual como do
ponto de vista do trabalho em equipa.
As 56 instâncias do VRPTW com 100 clientes propostas por
(iii) Comparação entre as duas propostas: comparação das
(i) Desempenho da proposta ALS-LA: análise específica dos resultados
propostas (ALS-LA e ALS-QLearning).
ser formulado da seguinte forma. Considerando dois cenários SC1 e SC2,
nível_perturbação + +;
O conjunto de instâncias utilizadas para testes computacionais associados
(linha 9), se uma solução melhor for encontrada. O adaptiveLocalSearch(s )
e a escalabilidade da estrutura usando a proposta ALS-LA;
UPMSP-ST.
Algoritmo 6 Pesquisa Local Iterada (Lourencho et al., 2003).
em Silva et al. (2015), uma variação da Busca Local Iterada (ILS)
este algoritmo, a perturbação da solução, realizada a partir
se (critérios de aceitação(x, x)) então
taxa de decaimento dada por 0,999.
As posições geográficas dos clientes são geradas aleatoriamente no R1
com janela de tempo (VRPTW). O segundo problema instanciado é
(iii) quatro agentes ILS idênticos;
(iv) Desempenho da aprendizagem individual: comparação do desempenho de um
único agente usando cada uma das propostas (ALS-LA
obtidos em cada um desses cenários foram comparados por meio de uma
agente proposto em Silva et al. (2015). Os testes foram realizados usando as
mesmas condições de teste para as duas estruturas de agentes adaptativos
cenários SC1 e SC2 são iguais. Se a hipótese nula não for rejeitada, então não
há diferença estatística significativa entre
descrito no artigo atual é aqui denominado ALS-QLearning
a busca de forma isolada, com a realização do
AMAM, 24 instâncias deste conjunto de dados foram escolhidas. Estes testes
Uma vez que os algoritmos utilizados são de natureza estocástica, cada um
fins do item anterior;
x ÿ localSearch(xo);
notebook, com um único processador, mostrando a eficácia do
influência direta no desempenho da estrutura em relação
cada uma das duas propostas é usada para avaliar o framework:
processo cooperativo, através da utilização de dois ou mais agentes cooperando
para resolver o problema, também avaliado para cada um dos
caminhos:
outro
verifica se há diferenças entre as médias das populações de soluções avaliadas.
Dessa forma, as hipóteses devem ser
melhoria na solução (linha 11), e retorna ao seu primeiro nível
experimentar.
o experimento não visa superar os melhores resultados da literatura para
usados nos experimentos são ÿ = 0,9, ÿ = 0,1 e = 0,05, com
nos permitirá avaliar a eficácia da cooperação
Os agentes utilizados neste experimento implementam, conforme realizado
evidência estatística (com nível de confiança de 95%) de que as soluções
obtidas pelo SC1 são melhores que as obtidas pelo SC2.
proposta, foram executadas 30 vezes para cada instância. Os resultados
x ÿ adaptiveLocalSearch(x );
diferentes conjuntos de clientes (C-Cluster; R-Random; e RC-Random-Cluster) de
acordo com a distribuição geográfica considerada.
instanciado para dois problemas clássicos de Otimização Combinatória. O primeiro
problema instanciado é o Problema de Roteamento de Veículos
ALS-QLearning);
(ii) dois agentes ILS idênticos;
(i) Hipótese nula (H0): a média das soluções obtidas em
agente apresentado na proposta atual e com o adaptativo
ALS-LA (Adaptive Local Search - Learning Automata); a proposta
do cenário em que há um único agente, que realiza
estão disponíveis em http://soa.iti.es/problem-instances. Para avaliar
metaheurística de trajetória. Este método é mostrado no Algoritmo 6. Em
resultados obtidos pela proposta ALS-QLearning, com os mesmos
xo ÿ pf ihSoluçãoInicial();
o processo de cooperação descrito na Seção 4. Quatro cenários para
destacar que os testes aqui apresentados foram executados em um ambiente comum
é avaliar se a forma de aprendizagem, incorporada no agente, tem um
conjuntos. Vale ressaltar que competir com os melhores resultados da literatura
para essas instâncias de VRPTW está fora do escopo deste Neste contexto, as propostas apresentadas foram analisadas em cinco
2:
4:
3:
8:
12:13: fim enquanto
14: procedimento final
161MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171
6:
7:
10:
5:
11:
9:
Machine Translated by Google
à medida que o número de agentes também cresce.
para o VRPTW, os resultados obtidos em alguns cenários da proposta ALS-
QLearning também empataram com os melhores resultados da literatura.
têm um número diferente de rotas. Se o número de rotas for diferente, não é possível
comparar o custo dessas soluções. Em
agentes utilizados para resolver os dois problemas é aumentada, a avaliação
problemas instanciados. As seções 6.3.1 e 6.3.2 mostram a análise de
da proposta ALS-QLearning foi melhor, para o VRPTW e
deles (21,42%), a proposta ALS-QLearning obteve a melhor
mais agentes eram melhores do que o cenário com 1 único agente.
para a instância R111, para VRPTW, e instância I_50_15_S_1-99_1,
há evidências estatísticas de que, em 89,28% dos casos, o
O custo de uma solução para VRPTW é calculado de acordo com
há evidências estatísticas de que, em 90,90% deles, os cenários
as instâncias que obtiveram diferentes números de rotas, mas não
resultados obtidos nas 30 execuções das instâncias R201 e R203 do VRPTW,
respectivamente, a melhoria na qualidade de
6.3. Aprendizagem ALS-LA × ALS-Q
empatado com os melhores resultados, ou seja, não houve diferença estatística
diferença estatística para comparação e, como consequência, eles
nas Tabelas 3 e 4, pelas Tabelas 5 e 6, conclui-se que aumenta o número de vezes
que cada cenário é melhor em
e instâncias UPMSP-ST, respectivamente, com base nos resultados do
em alguns casos, as soluções geradas nos cenários avaliados podem
Conforme descrito em relação aos resultados da proposta ALS-LA,
instâncias. As tabelas 3 e 4 também mostram que quando o número de
avaliados neste experimento. Esta comparação abordou os dois
As tabelas 5 e 6 mostram o número de vezes que cada cenário
Em relação ao VRPTW, das 56 ocorrências analisadas, em 12
em 95,83% das 24 instâncias analisadas, os cenários com 2 ou
Para as instâncias VRPTW, os resultados obtidos mostraram que
agentes no processo de solução. Esta figura exibe a solução devido
6.3.1. VRPTW
As instâncias que obtiveram o mesmo número de rotas foram comparadas em
relação à distância percorrida. O mesmo acontece para
6.1. Proposta ALS-LA
foram contabilizados os cenários que apresentaram os melhores resultados.
As Figuras 17 e 18 usam gráficos de boxplot para ilustrar, por meio da
com um único agente). Portanto, nesses casos, não houve
É importante destacar que, em alguns casos, os cenários
escalabilidade das propostas. De forma semelhante aos resultados apresentados
os quatro cenários da proposta ALS-LA para os dois problemas instanciados. As
tabelas 3 e 4 mostram o número de vezes que cada cenário da proposta ALS-LA foi
melhor, para as instâncias VRPTW
rotas da solução, ou seja, o número de veículos que são utilizados. Em
com 1 único agente. Por outro lado, para as instâncias UPMSP-ST, os resultados
obtidos mostraram que há evidências estatísticas de que o uso de 2 ou mais agentes
foi melhor em 100,00% dos casos.
na medida em que o número de agentes cresce.
execuções da instância UPMSP-ST I_50_10_S_1-9_1.
agente. Em relação à UPMSP-ST, há evidências estatísticas de que,
das soluções encontradas para as propostas ALS-LA e ALS-QLearning,
problemas instanciados.
envolvidos na solução influenciam o desempenho do sistema multiagente.
6.2. Proposta de aprendizagem ALS-Q
As figuras 20 e 21 mostram exemplos do efeito da adição de
Nestes casos, a comparação é feita através do número de rotas.
Nestes casos, tal como na análise da proposta ALS-LA, todos os
a função dos dois problemas diminui.
solução da literatura em todos os cenários (incluindo o cenário
esses resultados.
instâncias UPMSP-ST, respectivamente, com base nos resultados do teste
paramétrico utilizado. Como na Seção 6.1, o objetivo é avaliar a
A análise iniciar-se-á abordando os resultados obtidos em
para problemas UPMSP-ST, respectivamente, e revela sua melhoria
cenários com 2 ou mais agentes foram melhores que o cenário
Eq. (1). Nesta equação, a prioridade é minimizar o número de
a solução com a adição de mais agentes ao processo de solução. O mesmo
resultado pode ser observado no gráfico boxplot apresentado na Fig. 19, usado para
ilustrar os resultados obtidos para o processo de 30
com 2 ou mais agentes eram melhores do que o cenário com 1 único
Nesta seção, focamos nos resultados obtidos pela proposta ALS-QLearning,
apresentada no presente artigo, para os dois
Esta seção apresenta a comparação entre valores médios
foram excluídos desta análise. Para os 44 casos restantes,
teste paramétrico utilizado. O objetivo aqui é avaliar a escalabilidade das propostas,
ou seja, se o aumento do número de agentes
entre as médias das soluções. Nestes casos, foram contabilizados todos os cenários
que corresponderam aos melhores resultados.
162
8
8
56
Total
0
Tabela 3
R1 8
teste paramétrico.
7
4
9
Tabela 4
0
0
0
Total de instâncias por conjunto Cenários
812
38
teste paramétrico.
3
1 4
2
MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171
5
4
C2
1
RC1
Total de instâncias por conjunto Cenários
R2
Número de vezes que cada cenário foi melhor com ALS-LA VRPTW – valores obtidos a partir do
0
4
3
2
0
11
9
Total
Conjunto de instâncias
0
0
0
2
8
4
Número de vezes que cada cenário foi melhor com ALS-LA UPMSP-ST – valores obtidos a partir do
50 empregos
100 empregos
16
8
24
1
1 agente 2 agentes 4 agentes 8 agentes
11
2
6 26
Conjunto de instâncias
1
0 1
16
8
RC2
5
C1
1 agente 2 agentes 4 agentes 8 agentes
24
2
Machine Translated by Google
Fig. 17. Comparação dos cenários ALS-LA em relação à distância percorrida - instância R201.
163MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171
Fig. 18. Comparação dos cenários ALS-LA em relação à distância percorrida - instância R203.
Fig. 19. Comparação dos cenários ALS-LA em relação à instância makespan - UPMSP-ST - I_50_10_S_1-9_1.
Machine Translated by Google
R1
38
2
16
Total de instâncias por conjunto Cenários
RC2
Fig. 21. Comparação dos cenários ALS-QLearning em relação à instância makespan - I_50_15_S_1-99_1.
0
13
3
2
12
11
8
1
do teste paramétrico.
14
2
Tabela 5
0
44
C2
1
6
Total
Conjunto de instâncias
1 agente 2 agentes 4 agentes 8 agentes
8
Fig. 20. Comparação dos cenários ALS-QLearning em relação à distância percorrida - instância R111.
0
2
RC1
24
Número de vezes que cada cenário foi melhor com ALS-QLearning UPMSP-ST – valores obtidos
MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171
1
5
4 6
8
C1
2
1
50 empregos
100 empregos
21
22
teste paramétrico.
Total de instâncias por conjunto Cenários
9
1
R2
1
Tabela 6
1
2
4 3
8
164
Total
1 agente 2 agentes 4 agentes 8 agentes
0
0
0
0
0
1
3
4
8
11
Número de vezes que cada cenário foi melhor com ALS-QLearning – valores obtidos a partir do
Conjunto de instâncias
0
1
Machine Translated by Google
mostra o número de vezes que há evidências estatísticas de que o ALS-QLearning
foi melhor do que o ALS-LA, em relação à distância percorrida
outras soluções em relação à distância percorrida.
e esta situação é avaliada na Tabela 8 na sequência. Como pode ser
tanto para a proposta ALS-SA como para a proposta ALS-QLearning,
equivalente ao melhor resultado conhecido da literatura, obtido por
O desempenho da aprendizagem individual e da aprendizagem em equipe dos
agentes no ambiente multiagente apresentado foram
(obtido pela ALS-LA) para 12 rotas (obtidas pela ALS-QLearning).
das instâncias analisadas. Os resultados para grupos de instâncias R1
a proposta ALS-QLearning obteve melhores resultados na maioria dos
aprendizagem, o número de vezes que um único agente usa o ALS-
em todas as instâncias desses grupos.
melhores resultados na maioria das instâncias e cenários para VRPTW.
A Fig. 23 mostra um bom exemplo onde os valores das soluções ALS-QLearning
são significativamente melhores do que as soluções ALS-LA.
de 1609,4, obtido pelo ALS-LA, para o valor de 1249,4, que é
em apenas 26,78% dos casos.
Tabela 7, ou seja, os que obtiveram o mesmo número de rotas
ALS-QLearning.
ALS-QLearning e ALS-LA obtiveram o mesmo número de rotas
removido e, portanto, a análise estatística foi feita com o
foram analisados na Tabela 8 para a distância total percorrida. Esta Tabela
também avaliados. A Tabela 9 apresenta esta análise. Em relação aos indivíduos
A Fig. 22 ilustra a situação em que a proposta ALS-QLearning alcança melhores
resultados do que a proposta ALS-LA. Nesta figura, os resultados da instância R106
mostram uma redução de 15 rotas
visto, a proposta ALS-QLearning obteve melhores resultados na maioria
e por cenário. Uma observação direta da Tabela 7 conclui que
Em relação à análise estatística e considerando o número de rotas de soluções,
a proposta ALS-QLearning obteve a
e RC1 devem ser destacados porque a proposta ALS-QLearning foi capaz de
melhorar os resultados encontrados na proposta ALS-LA
instâncias analisadas.
apresentam diferença estatística nestes valores. Neste caso, as soluções com
número de rotas que não influenciam no resultado foram
Neste contexto, avaliou-se o número de vezes que o ALS-QLearning foi melhor
por cenário do que o ALS-LA, considerando o número de rotas das soluções
analisadas. A Tabela 7 mostra os valores referentes a 31 instâncias. Em relação às
25 instâncias restantes,
Neste caso específico, há uma redução na distância percorrida
Esta proposta encontrou o melhor número de rotas conhecidas na literatura em
60,71% das instâncias avaliadas, enquanto a proposta ALS-LA obteve o melhor
número de rotas conhecidas na literatura
Os 25 casos não abordados na situação tratada no
0
8 agentes
0
6
2
22
2
0
25
RC1
5
Total de instâncias por conjunto com diferentes cenários de número de rotas
1
6
0
9
0
Tabela 8
RC2
RC1
9
8
C1
8
4
5
R1
Tabela 7
Total de instâncias por conjunto
9
6
4 agentes
4
0
2
8
3
31
5
R2
22
Total de instâncias por conjunto
3 25
8
7
0
9
0
4
0
R2
C2
4
1
24
165
0
10
2 agentes
Conjunto de instâncias
4
0
9
0
Fig. 22. Comparação entre as propostas ALS-LA e ALS-QLearning em relação ao número de percursos – instância RC106.
Total de instâncias 56
5
1
R1
Total de instâncias 56
5
4
Conjunto de instâncias
25
12
9
1 agente 2 agentes 4 agentes 8 agentes
C1
11
8
8
0
8
2
21
4
9
8
12
11
8
5
24
MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171
7
1 agente
6
0
9
0
Número de casos em que a proposta ALS-QLearning é melhor que a proposta ALS-LA, por cenário, considerando a distância total percorrida.
8
5 1
C2
RC2
24
4
12
Número de casos em que a proposta ALS-QLearning é melhor que a proposta ALS-LA, por cenário, considerando o Número de Rotas.
1
11
Total de instâncias por conjunto com número de rotas igual Cenários
Machine Translated by Google
do que aqueles encontrados pela ALS-LA, seja considerando um único agente ou
Em 92,9% dos casos, o teste paramétrico utilizado mostra
makespan de 273,00, obtido por ALS-LA, para 221,00, que é
processo cooperativo, ou seja, no trabalho em equipe, o número de vezes que o
Para 83,33% dos casos, o teste paramétrico utilizado mostra que
dois ou mais cenários também foi apresentado. Este valor é apresentado
o número de agentes envolvidos na busca pela solução em-
ALS-QLearning.
utilização de dois ou mais agentes para resolver o problema, em 75,00% dos casos
A proposta do QLearning foi melhor do que um único agente usando a proposta
ALS-LA foi avaliada (valor apresentado na terceira coluna e
do que ALS-LA quando se usa apenas um agente para resolver o problema.
também avaliado para o UPMSP-ST. A Tabela 11 apresenta esta análise.
a quinta coluna da tabela.
por cenário do que o ALS-LA foi avaliado, considerando o período de validade
que há evidências estatísticas de que o ALS-QLearning é melhor
processo cooperativo, ou seja, no trabalho em equipe, o número de vezes que o
obteve melhores resultados na maioria das instâncias analisadas. O
foi apresentada também a proposta ALS-LA (valor apresentado no
vezes que um único agente usando a proposta ALS-QLearning foi
A Fig. 24 apresenta um exemplo das distâncias percorridas obtidas
os resultados encontrados na proposta ALS-LA em todas as instâncias destes
6.3.2. UPMSP-ST
coluna da tabela).
das propostas testadas (ALS-LA e ALS-Q Learning). É importante notar como os
valores alcançados pelo ALS-QLearning são melhores
A Fig. 25 mostra um bom exemplo onde os valores das soluções ALS-QLearning
são significativamente melhores do que as soluções ALS-LA. Neste caso específico,
há uma redução na média
tabela. Sobre a avaliação da influência da aprendizagem na
valor makespan, a proposta ALS-QLearning obteve os melhores resultados na
maioria das instâncias e cenários para UPMSP-ST.
o valor percentual correspondente na quarta coluna da tabela). Quanto à avaliação
da influência da aprendizagem no
Em relação à utilização de dois ou mais agentes para resolver o problema, em
96,4% dos casos utilizados o teste paramétrico mostra
das soluções analisadas. A Tabela 10 mostra os valores referentes a 24 instâncias.
Como pode ser visto, a proposta ALS-QLearning
que há evidências estatísticas de que a aprendizagem ALS-Q é melhor
considerando dois ou mais agentes. Além disso, os valores melhoram à medida que
equivalente ao melhor resultado conhecido da literatura, obtido por
A proposta ALS-QLearning foi melhor do que a proposta ALS-LA em
há evidências estatísticas de que o ALS-Q Learning é melhor do que o ALS-LA
quando se usa apenas um agente para resolver o problema. Em relação à
os resultados para grupos de instâncias com 100 empregos devem ser destacados
porque a proposta ALS-QLearning foi capaz de melhorar
A proposta ALS-QLearning foi melhor, em dois ou mais cenários, do que
do que ALS-LA
O desempenho da aprendizagem individual e da aprendizagem em equipe dos
agentes no ambiente multiagente apresentado foram
Assim como no VRPTW, no que diz respeito à aprendizagem individual, o número de
na quinta coluna e o valor percentual correspondente está em
quinta coluna e o valor percentual correspondente na quinta
nas 30 execuções da instância R203, para os quatro cenários usados em cada
Em relação à análise estatística e considerando a
grupos.
melhor do que um único agente usando a proposta ALS-LA foi avaliado. Este valor
é apresentado na terceira coluna e o valor percentual correspondente é mostrado
na quarta coluna do
vincos.
Neste contexto, o número de vezes que o ALS-QLearning foi melhor
166
54
7
Fig. 23. Comparação entre as propostas ALS-LA e ALS-QLearning em relação à distância percorrida – instância R201.
100,0
87,5
91,7
90,9
87,5
100,0
RC1
Número de vezes que o ALS-QLearning é melhor que o ALS-LA individualmente e em equipe
Total
Valor %
87,5
Conjunto de instâncias Total de instâncias por conjunto Cenários
7
90,9
11
10
7
8
MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171
9
100,0
C2 8
Valor
56
100,0
Tabela 9
Em grupo
R2
RC2
92,9
VRPTW.
9
96,4
100,012
11
8
8
C1
Individualmente
100,0
12
10
8
8
52,0
R1
9
%
Machine Translated by Google
Tal como no VRPTW, é importante notar como os valores alcançados por
execuções da instância I_100_10_S_1-9_1, para os quatro cenários
O ALS-QLearning é melhor do que o encontrado pelo ALS-LA, seja
na busca pela solução aumenta. Neste caso, a ELA-
evidências de que o ALS-QLearning é melhor que o ALS-LA.
usado em cada uma das propostas testadas (ALS-LA e ALS-QLearning).
A Fig. 26 apresenta um exemplo do makespan obtido no período de 30
casos usados o teste paramétrico mostra que há estatística
além disso, os valores melhoram à medida que aumenta o número de agentes envolvidos
considerando um único agente ou considerando dois ou mais agentes. Em
MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171
8
12
Fig. 24. Comparação entre as propostas ALS-LA e ALS-QLearning em relação à distância percorrida – instância R203.
8 18
Valor %
Tabela 10
Valor %
Total de instâncias 24
8
75,00
cenário, considerando o período total de produção UPMSP-ST.
10
Tabela 11
Total
62,50
167
1 agente 2 agentes 4 agentes 8 agentes
UPMSP-ST.
12
8 20
Total de instâncias por conjunto
8 100,0
8 19
Fig. 25. Comparação entre as propostas ALS-LA e ALS-QLearning em relação ao valor makespan – instância I_100_10_S_1-124_1.
50 empregos
100 empregos
Individualmente Em grupo
12
Número de casos em que a proposta ALS-QLearning é melhor do que a proposta ALS-LA, por
16
20
20
50 empregos
100 empregos
10
18
11
Conjunto de instâncias
Número de vezes que o ALS-QLearning é melhor que o ALS-LA individualmente e como uma equipe para
16
100,0 8 83,33
24
Conjunto de instâncias Total de instâncias por conjunto Cenários
8
75,00
Cenários
Machine Translated by Google
A Tabela 12 apresenta os custos médios da distância percorrida (DT) e
os custos médios das soluções obtidas com as 30 execuções da proposta
ALS-QLearning. Além disso, a Tabela 14 mostra
Além disso, uma questão notável são os resultados relativos à
número de veículos e a distância total percorrida, e independentemente
o valor médio do makespan para as soluções obtidas
análise estatística, também é demonstrada pela apresentação direta do
No caso do VRPTW, este fato é válido tanto em relação ao total
resultados nestas tabelas.
número de rotas (NR) das soluções obtidas com as 30 execuções da
proposta ALS-AL. A Tabela 13, por outro lado, mostra
A primeira observação, válida tanto para problemas quanto para propostas,
é o efeito do aumento do número de agentes em uso no framework.
aliás, também é relatado em aplicações da arquitetura A-Teams (Barbucha
et al., 2010).
momento. A distância total percorrida encontrada com o aplicativo
A Tabela 15 inclui o valor médio do makespan para as soluções obtidas com
as 30 execuções com o ALS-QLearning
com as 30 execuções com a proposta ALS-AL. Por sua vez,
da classe de instância avaliada. Claramente, portanto, há uma identificação
de um efeito de escalabilidade no número de agentes. Este efeito,
duas técnicas de aprendizagem e seus resultados. Independentemente da
As instâncias VRPTW da Tabela 13 são fortemente competitivas no que diz
respeito à distância total percorrida, embora esta não seja uma questão
colocada como objetivo no desenvolvimento desta estrutura neste momento.
Para completar, os resultados computacionais associados ao
Da análise dessas tabelas, surgiram algumas observações. A
A segunda observação diz respeito à comparação entre os
6.4. Resultados computacionais para a solução média
proposta.
para todos os quatro cenários.
Há uma melhora considerável nos resultados obtidos pelo aumento do
número de agentes em ação no quadro. No
obtidos com a proposta ALS-QLearning em relação aos obtidos com a
proposta ALS-AL, já comprovados anteriormente no
soluções médias são mostradas.
A proposta do QLearning encontrou o resultado mais conhecido da literatura
classe do problema avaliado, a grande superioridade dos resultados
C1
4743,31
14611,01
10562,31
11153,62
2 agentes
16051,72 158,93
Não.
90,00
24,00
151,37
8 agentes
150,70
BKS
7455,42
Custo
R1
8953,92
7459,83
4720,42
10722.16
Custo
ALS-QLearning
RC2 11220,26 28,47
Custo
11161,14 97,53
Não.
Fig. 26. Comparação entre as propostas ALS-LA e ALS-QLearning em relação ao valor makespan – instância I_100_10_S_1-9_1.
Tabela 13
11468,59 95,00 4718,87
5963,84 25,00 14524,02 143,00 16372,98 162,6 10461,33
11073,32 8953,92
4811,19
14743,21
10787,21
11179,67
Não.
CustoCusto
5336,65 24,00
7466,76
4 agentes
24.00
Aula
5462,43 24,00 16448,36
158,6 12310,51 34,37
12325,24 106,33
10600,83 28,9
C2
Não.
BKS
7489,31
33,57
100,93
27,00
92,5
4 agentes
Não.
RC1 12097,62 103,87
7446,39
4716,08
14567,00
10522,76 33,03
MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171
Não.
24.00
1 Agente
12729,07 35,1 12565,04
107,47 11045,54 30,17
Custo
90,00
24,00
143,00
30,00
92,00
26,00
90,00
24,00
153,53 14566,80 152,63 34,00 10567,68
102,80 11186,01 27,00
9295,74
Custo
2 agentes
11106,00
5760,4
16303,79
12400,8
12355,63
10931,74
90,00
Custo
Custos médios da proposta ALS-AL para VRPTW.
24,57
162,67
34,17
105,43
29,47
C1
C2
Aula
R1
R2
RC1
RC2 9353,25 TOTAL 57186,88 405,00 58542,02 431,33 57853,99 428,13 57832,76 424,10
57730,25 422,27
Custo
10867,57
R2 12.000,78 34,00
Não.
Custo
33,87
97,87
27,00
7455,42
4718,87
14524,02
10461,33
11073,32
168
90,00
ALS-AL
Não.
30,00
92,00
26,00
9531,43
90,00
24,00
8 agentes
1 Agente
93,57
9316,87 27,00
Não.
Não.
Tabela 12
TOTAL 57186,88 405,00 70145,06 455,34 68858,36 450,04 68014,94 444,40 67429,19 442,84
93,73
Custos médios da proposta ALS-QLearning para VRPTW.
Machine Translated by Google
que a proposta ALS-QLearning obteve os melhores resultados na maioria
Heurística VND.
outras estruturas na literatura como uma alternativa à necessidade de
FAPEMIG (Concessão PPM CEX 676/17). Este estudo foi financiado em
possibilita também pesquisas futuras, como a introdução
otimização usando metaheurísticas. Sua principal característica é
forma de aprendizagem incorporada no agente é confirmada pelos experimentos,
tanto do ponto de vista individual, quanto do ponto de vista
logue é o espaço de busca da otimização combinatória que está sendo resolvida.
Cada agente atua de forma autônoma neste ambiente
o estudo da inserção de metodologias de aprendizagem por reforço
O objetivo principal deste artigo foi propor novas habilidades autoadaptativas
para os agentes do framework. Por meio dessas habilidades, os
com o aprendizado por reforço, foram realizados experimentos computacionais,
utilizando, como estudo de caso para esse fim, o Problema de Roteamento de
Veículos com Janela de Tempo e Máquinas Paralelas Não Relacionadas.
Amélia Lopes Silva, Sérgio Ricardo de Souza Marcone Jamilson Freitas Souza e
Ana Lúcia C. Bazzan declaram não ter conflito de interesses. Este artigo não contém
nenhum estudo com participantes humanos ou animais realizado por nenhum dos
autores.
Declaração de contribuição de autoria de crédito
A interação entre os agentes permite a hibridização metaheurística. A versão mais
recente do framework AMAM, lançada
encontrado na literatura para a solução VRPTW usando metaheurísticas. O número
total de rotas é 4,26% acima (ou seja, 17 rotas) de
objetivo principal dos experimentos foi avaliar o desempenho do agente adaptativo.
Para esta avaliação, os testes foram
a interação com o ambiente e com os demais agentes.
Adicionalmente, para as duas propostas avaliadas (ALS-LA e ALS-QLearning),
os cenários com 2 ou mais agentes foram significativamente
Análise formal, Investigação, Metodologia, Software, Validação,
307915/2016-6) e Fundação de Amparo à Pesquisa do Estado de Minas Gerais -
adaptar os métodos a aspectos específicos do problema. Desta forma,
O algoritmo Q-Learning é central na definição dessas novas
AMAM-Multiagente-Architecture-for-Metaheuristics, licenciado sob a licença GNU
LGPLv3.
Conformidade e padrões éticos
chamado ALS-QLearning, e com o agente adaptativo proposto em
Assim, confirma-se que a cooperação entre os agentes influencia na qualidade das
soluções e na escalabilidade do sistema.
literatura.
facilitar a hibridização de metaheurísticas por meio de uma estrutura multiagente.
Cada agente implementa uma heurística/metaheurística e o ambiente em que os
agentes atuam e dia-
de novas formas de aprendizagem para melhorar as capacidades adaptativas, bem como
das estruturas de vizinhança da busca local com base na
Os resultados obtidos mostram que existem evidências estatísticas
capacidades para agentes colocam a estrutura AMAM um passo à frente
parte pela Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES)
- Brasil - Código Financeiro 001. Autores Maria
e interage cooperativamente com ele e com os outros agentes.
entre os agentes.
os agentes modificam suas ações com base na experiência adquirida em
Problema de agendamento com tempos de configuração dependentes de sequência. O
ponto de vista do trabalho em equipe.
Para realizar a validação do framework AMAM
instâncias e cenários. Desta forma, a influência direta do
da proposta ALS-QLearning está apenas 0,95% acima do melhor valor
o número total de rotas associadas aos melhores resultados no
em janeiro de 2018, está disponível em https://github.com/mamelials/
melhor desempenho do que os cenários com 1 único agente.
Os conceitos de aprendizagem por reforço, mais especificamente usando o
feito com o agente adaptativo apresentado na proposta atual,
Este artigo apresentou o AMAM, uma estrutura multiagente para
Maria Amélia Lopes Silva: Conceptualization, Data curation,
habilidades adaptativas. O aprendizado é usado para selecionar a ordem de aplicação
Silva e outros. (2015), denominado ALS-LA.
Este estudo foi financiado por agências brasileiras Conselho Nacional de
Desenvolvimento Científico e Tecnológico - CNPq (Conceder
framework, pois, com a adição de novos agentes, há uma melhoria nos resultados.
O uso da aprendizagem para atribuir adaptativos
7. Conclusões e direções futuras
77,80
66,83
1 agente
73,37
112,47
136,33
156,70
66,60
139,90
22,60
78,90
66,70
130,93
57,15
67,00
77,00
118,00
22,67
130,34
56,37
Exemplo
75,65
87,55
123,47
78,00
75,00
31,00
39,00
49,00
52,00
2 agentes
137,87
72,43
116,66
236,00
44,95
170,63
115,13
80,45
49,00
52,00
22,00
25,00
35,00
37,00
Português:
Eu_50_10_S_1-9_1
Eu_50_10_S_1-49_1
Eu_50_10_S_1-99_1
Eu_50_10_S_1-124_1
Eu_50_15_S_1-9_1
Eu_50_15_S_1-49_1
Eu_50_15_S_1-99_1
Eu_50_15_S_1-124_1
Eu_50_20_S_1-9_1
Eu_50_20_S_1-49_1
Eu_50_20_S_1-99_1
Eu_50_20_S_1-124_1
Eu_50_25_S_1-9_1 Eu_50_25_S_1-49_1
25,00 I_50_25_S_1-99_1 35,00
I_50_25_S_1-124_1 37,00
I_100_10_S_1-9_1 131,00 131,60 I_100_10_S_1-49_1
159,00 179,83 I_100_10_S_1-99_1 212,00 239,37
I_100_10_S_1-124_1 221,00 243,07 I_100_15_S_1-9_1
I_100_15_S_1-49_1 I_100_15_S_1-99_1
I_100_15_S_1-124_1
141,00
50,43
59,37
60,93
144,73
118,00
43,85
68,40
85,40
90,30
37,90
Custos médios da proposta ALS-AL para UPMSP-ST.
65,60
69,37
79,70
116,67
50,17
121,50
60,65
25,87
4 agentes
132,00
Tabela 15
75,17
Eu_50_10_S_1-9_1
Eu_50_10_S_1-49_1
Eu_50_10_S_1-99_1
Eu_50_10_S_1-124_1
Eu_50_15_S_1-9_1
Eu_50_15_S_1-49_1
Eu_50_15_S_1-99_1
Eu_50_15_S_1-124_1
Eu_50_20_S_1-9_1
Eu_50_20_S_1-49_1
Eu_50_20_S_1-99_1
Eu_50_20_S_1-124_1
Eu_50_25_S_1-9_1
Eu_50_25_S_1-49_1
Eu_50_25_S_1-99_1
I_50_25_S_1-124_1
I_100_10_S_1-9_1
I_100_10_S_1-49_1 159,00 201,07 I_100_10_S_1-99_1
212,00 266,53 I_100_10_S_1-124_1 221,00 270,87
I_100_15_S_1-9_1 I_100_15_S_1-49_1
I_100_15_S_1-99_1
I_100_15_S_1-124_1
141,00
254,23
44,17
Cenários
75,57
115,50
141,50
158,37
141,80
42,57
66,50
68,20
54,83
42,35
8 agentes
60,80
23,57
128,10
Custos médios da proposta ALS-QLearning para UPMSP-ST.
77,70
88,25
35,43
35,25
65,35
68,05
24,90
50,20
59,50
59,40
44,27
54,30
56,53
130,90
172,27
227,30
121,30
119,30
114,00
36,00
59,00
78,00
75,00
31,00
39,00
1 agente
22h00
118,00
81,77
137,37
72,77
76,13
37,07
58,67
65,53
66,13
67,00
77,00
150,97
188,33
Exemplo
45,37
75,43
131,00
59,00
Tabela 14
46,50
70,30
87,05
92,85
40,15
71,10
81,60
118,20
118,34
65,55
66,15
23,55
46,30
56,40
59,60
42,20
124,40
43,35
67,00
82,45
84,85
36,45
63,65
2 agentes
131,85
27,87
68,00
97,00
123,00 167,50 189,07
70,27
81,15
92,80
66,63
246,33
BKS
68,00
97,00 116,27 123,00
146,60 163,93
123,05
BKS
64,63
73,55
151,70
4 agentes
78,70
80,60
41,10
61,43
65,37
66,23
108,63
196,77
256,83
265,30
81,20
132,40
159,27
179,20
41.30
231,77
114,00
51,05
MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171
224,80
81,25
68,05
71,70
26,60
49,70
60,30
62,30
72,03
83,50
123,25
48,23
56,10
60,27
131,13
175,53
230,30
Cenários
65,53
85,70
74,23
77,80
38,40
60,50
65,50
65,90
169,40
8 agentes
134,77
191,40
253,30
258,83
79,07
128,80
153,70
174,13
66,17
229,13
57,20
36,00
169
Machine Translated by Google
Redação - revisão e edição.
Agradecimentos
Os autores gostariam também de agradecer à Coordenação pela
Fundação de Amparo à Pesquisa do Estado de Minas Gerais (FAPEMIG), o
Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq), o Centro
Federal de Educação Tecnológica de Minas Gerais (CEFET-MG),
Escrita - rascunho original, Escrita - revisão e edição. Ana Lúcia C.
Bazzan: Conceitualização, Metodologia, Redação - rascunho original,
Metodologia, Supervisão, Redação - rascunho original, Redação - revisão e edição.
Marcone Jamilson Freitas Souza: Conceitualização, Curadoria de dados,
Aquisição de financiamento, Metodologia, Supervisão,
Visualização, Escrita - rascunho original, Escrita - revisão e edição. Sérgio Ricardo
de Souza: Conceitualização, Curadoria de dados,
of Viçosa (UFV) and the Federal University of Rio Grande do Sul
(UFRGS) pelo apoio ao desenvolvimento do presente estudo.
Referências
Aperfeiçoamento de Pessoal de Nível Superior (CAPES), do Ministério da Educação
the Federal University of Ouro Preto (UFOP), the Federal University
Wierzbowska, I. (2010). Middleware JABAT como ferramenta para resolver problemas de otimização
(Ed.), Metaheurísticas paralelas: uma nova classe de algoritmos (pp. 347–370). John Wiley
2009.5345934.
(2010). Manual de metaheurísticas. Em M. Gendreau, & J.-Y. Potvin (Ed.). Série internacional em pesquisa
operacional e ciência de gestão: 146 (2ª ed.). Springer.
doi:10.1016/j.ejor.2015.04.004.
Dorigo, M., & Stützle, T. (2019). Otimização de colônias de formigas: Visão geral e avanços recentes. Em
M. Gendreau, & J.-Y. Potvin (Eds.), Manual de meta-heurísticas (pp. 311–351)). Springer International
Publishing. doi:10.1007/
projeto de metaheurísticas paralelas e distribuídas. Journal of Heuristics, 10(3),
Mladenovic,
Horizonte, Brasil: Centro Federal de Educação Tecnológica de Minas Gerais
Editoras.
Narendra, KS e Thathachar, MAL (1974). Autômatos de aprendizagem – uma pesquisa. IEEE
Radac, M.-B., Precup, R.-E., & Roman, R.-C. (2018). Controle de referência de modelo baseado em dados
de sistemas de tanques verticais mimo com vrft sem modelo e q-learning. ISA
Lotfi, N., & Acan, A. (2015). Sistema multiagente baseado em aprendizagem para resolver problemas de
otimização combinatória: Uma nova arquitetura. Em E. Onieva, I. Santos,
Melab, N., Luong, TV, Boufaras, K., & Talbi, E. (2013). ParadisEO-MO-GPU: Uma estrutura para
metaheurísticas de busca local baseadas em GPU paralela. Em Proceedings of the
gramática. Nova York, NY: John Wiley & Sons.
9(3), 227–244.
problemas. Em NT Nguyen, & R. Kowalczyk (Eds.), Transações em computação
& Filhos.
Silva, MAL, de Souza, SR, de Oliveira, SM, & Souza, MJF (2014). Uma abordagem metaheurística baseada
em agente-t aplicada ao problema de roteamento de veículos com
Allahverdi, A., Ng, C., Cheng, T., & Kovalyov, MY (2008). Uma pesquisa sobre agendamento
John, AA, Faulin, J., Grasman, SE, Rabe, M., & Figueira, G. (2015). Uma revisão de
357–380.
978-3-319-91086-4_10.
Imprensa.
(CEFET-MG) Dissertação de mestrado.
Transações em Sistemas, Homem e Cibernética, SMC-4(4), 323–334. doi:10.1109/
Transações, 73, 227–238.
Allahverdi, A. (2015). O terceiro levantamento abrangente sobre problemas de agendamento com
e comparação conceitual. ACM Computing Surveys, 35(3), 268–308.
abordagem ao problema do caixeiro viajante. Transações IEEE em Evolução
Bellman, R. (1957). Programação dinâmica. Princeton, NJ, EUA: Princeton University
Silva, MAL (2007). Modelagem de uma arquitetura multiagente para a solução, via
& GA Kochenberger (Eds.), Manual de metaheurísticas. Em série internacional em
N., & Hansen, P. (1997). Busca de vizinhança variável. Computadores &
Radac, M.-B., & Precup, R.-E. (2018). Controle de deslizamento livre de modelo baseado em dados de antibloqueio
Liefooghe, A., Jourdan, L., & Talbi, E. (2011). Uma estrutura de software baseada em um modelo unificado
conceitual para otimização multiobjetivo evolucionária: paradisEO–
Revista de Pesquisa Operacional, 254(1), 169–178.
527–561.
Gambardella, LM e Dorigo, M. (1995). Ant-q: Uma abordagem de aprendizagem por reforço
Barbucha, D., Czarnowski, I., Je¸ drzejowicz, P., Ratajczak-Ropel, E., &
Kazemitabar, SJ, Taghizadeh, N., & Beigy, H. (2018). Uma abordagem da teoria dos grafos
Cotta, J., Talby, EG., & Alba, E. (2005). Metaheurísticas híbridas paralelas. Em E. Alba
sobre sistemas, homem e cibernética (SMC 2009) (pp. 3071–3076). doi:10.1109/ICSMC.
para o design flexível de algoritmos de busca local. Software: Practice and Experi-ence, 33(8), 733–
765.
tempos/custos de configuração. European Journal of Operational Research, 246(2), 345–378.
Cahon, S., Melab, N., & Talbi, E.-G. (2004). Paradiseo: Uma estrutura para o reutilizável
Computação, 1(1), 53–66. doi:10.1109/4235.585892.
´
metaheurísticas, de problemas de otimização combinatória (in portuguese). Belo
pesquisa operacional e ciência da gestão: 57 (pp. 321–353). Kluwer Academic
sistemas de frenagem usando q-learning de reforço. Neurocomputing, 275, 317–329.
Pesquisa Operacional, 24(11), 1097–1100.
MOEO. Revista Europeia de Pesquisa Operacional, 209(2), 104–112.
Meignan, D., Créput, J.-C., & Koukam, A. (2008). Uma visão organizacional de meta-heurísticas. Em N.
Jennings, A. Rogers, A. Petcu, & SD Ramchurn (Eds.), Primeiro workshop internacional sobre
otimização em sistemas multiagentes, AAMAS'08 (pp. 77–85).
Puterman, ML (1994). Processos de decisão de Markov: processos dinâmicos estocásticos discretos
para aquisição autônoma de habilidades em aprendizagem por reforço. Sistemas em evolução,
ao problema do caixeiro viajante. Em Anais do décimo segundo congresso internacional
Bellifemine, F., Poggi, A., & Rimassa, G. (2007). Desenvolvimento de sistemas multiagentes com
Samma, H., Lim, CP, & Saleh, JM (2016). Um novo aprendizado de reforço baseado em
doi:10.1007/978-3-319-19644-2_27.
Milano, M., & Roli, A. (2004). MAGMA: Uma arquitetura multiagente para metaheurísticas. IEEE Transactions
on Systems, Man, and Cybernetics, Parte B: Cibernética, 34(2),
Rabadi, G., Moraga, RJ, & Al-Salem, A. (2006). Heurística para o paralelo não relacionado
Martin, S., Ouelhadj, D., Beullens, P., Ozcan, E., Juan, AA, & Burke, EK (2016).
1109/ICNN.1995.488968.
Computação suave, 23, 444–451.
Fink, A. e Voß, S. (2002). Hotframe: uma estrutura de otimização heurística. Em S. Voss,
Kaelbling, LP, Littman, ML, & Moore, AW (1996). Aprendizagem por reforço: Uma
Applegate, D., Bixby, R., Chvátal, V., & Cook, W. (2007). O problema do caixeiro viajante: Um estudo
computacional. Série Princeton em matemática aplicada (2ª ed.).
problemas de otimização combinatória. Em Proc. do VII workshop alioeuro sobre
//dl.acm.org/citation.cfm?id=3091622.3091654.
Uma arquitetura multiagente para resolver problemas de otimização combinatória
de Computação e Aplicações Inovadoras, 1(1), 74–85.
Blum, C., & Roli, A. (2003). Metaheurísticas em otimização combinatória: visão geral
Dorigo, M., & Gambardella, LM (1997). Sistema de colônia de formigas: um aprendizado cooperativo
otimizador de enxame de partículas meméticas. Applied Soft Computing, 43, 276–297.
JADE. John Wiley & Filhos.
Lourenço, HR, Martin, OC, & Stützle, T. (2003). Busca local iterada. Em F. Glover,
925–941.
problema de programação de máquinas com tempos de configuração. Journal of Intelligent
Manufacturing, 17(1), 85–97.
Li, X., Epitropakis, MG, Deb, K., & Engelbrecht, A. (2017). Buscando múltiplas soluções: Uma pesquisa
atualizada sobre métodos de nicho e suas aplicações. IEEE Trans-actions on Evolutionary
Computation, 21(4), 518–538.
Uma abordagem cooperativa baseada em múltiplos agentes para agendamento e roteamento. Europeu
Parejo, JA, Ruiz-Cortés, A., Lozano, S., & Fernandez, P. (2012). Frameworks de otimização metaheurística:
Uma pesquisa e benchmarking. Soft Computing, 16(3),
& DL Woodruff (Eds.), Bibliotecas de classes de software de otimização. Em Operations Re-search/
Computer Science Interfaces Series: 18 (pp. 81–154). Springer US.
pesquisa. Revista de Pesquisa em Inteligência Artificial, 4, 237–285.
Imprensa da Universidade de Princeton.
através de metaheurísticas. Em Anais da conferência internacional IEEE de 2009
otimização combinatória aplicada (pp. 51–54). ALIO/EURO 2011.
Gaspero, LD, & Schaerf, A. (2003). EASYLOCAL++: Um framework orientado a objetos
15ª conferência anual sobre computação genética e evolutiva (pp. 1189–1196).
E. Osaba, H. Quintián, & E. Corchado (Eds.), Hybrid artificial intelligent sys-tems: Proceedings of the
10th international conference (HAIS 2015). Em Lecture
Queiroz dos Santos, JP, de Melo, JD, Neto, ADD, & Aloise, D. (2014). Reactive
Kennedy, J., & Eberhart, R. (1995). Otimização de enxame de partículas. Em Proceedings of
inteligência coletiva ii. Em Notas de aula em Ciência da Computação: 6450 (pp. 181–195).
Noel, MM, & Pandian, BJ (2014). Controle de um sistema de nível de líquido não linear usando
simheurística: Estendendo metaheurísticas para lidar com combinatórias estocásticas
time-windows. In Proceedings of the 2014 brazilian conference on intelligent sys-tems - enc. nac. de
inteligência artificial e computacional (BRACIS-ENIAC 2014). São
problemas com tempos de configuração ou custos. European Journal of Operational Research,
Coelho, IM, Munhoz, PLA, Haddad, MN, Coelho, VN, Silva, MM,
Durillo, JJ, & Nebro, AJ (2011). Jmetal: Um framework Java para otimização multiobjetivo. Advances in
Engineering Software, 42(10), 760–771.
conferência sobre conferência internacional sobre aprendizado de máquina. Em ICML'95 (pp. 252–
Alba, E., Luque, G., Garcia-Nieto, J., Ordonez, G., & Leguizamon, G. (2007). MALLBA: A
Dorigo, M., Caro, GD e Gambardella, LM (1999). Algoritmos Ant para discreto
Bertsekas, D. (1987). Programação dinâmica: modelos determinísticos e estocásticos. En-glewood Cliffs,
NJ: Prentice-Hall.
170
TSMC.1974.5408453.
Salgado, M., & Clempner, JB (2018). Medindo o estado emocional entre agentes interagindo: Uma
abordagem de teoria de jogos usando aprendizado por reforço. Expert Systems with Applications, 97,
266–275.
notas em ciência da computação: 9121 (pp. 319–332). Springer International Publishing.
ACM.
estratégias de busca usando aprendizado por reforço, algoritmos de busca local e busca de vizinhança
variável. Expert Systems with Applications, 41(10), 4939–4949.
ICNN'95 – conferência internacional sobre redes neurais: 4 (pp. 1942–1948). doi:10.
Berlim, Heidelberg: Springer.
Carlos, SP, Brasil.
uma nova abordagem de aprendizagem por reforço baseada em rede neural artificial. Aplicado
problemas de otimização. Operations Research Perspectives, 2, 62–72.
187(3), 985–1032. doi:10.1016/j.ejor.2006.06.060.
Souza, MJF, & Ochi, LS (2011). OptFrame: Um framework computacional para
Fernandes, FC, de Souza, SR, Silva, MAL, Borges, HE, & Ribeiro, FF (2009).
biblioteca de software para projetar algoritmos de otimização eficientes. Revista Internacional
260). São Francisco, CA, EUA: Morgan Kaufmann Publishers Inc.. URL: http:
Blum, C., Puchinger, J., Raidl, GR, & Roli, A. (2011). Metaheurísticas híbridas em otimização combinatória:
uma pesquisa. Computação suave aplicada, 11(6), 4135–4151.
otimização. Vida Artificial, 5(2), 137–172. doi:10.1162/106454699568728.
MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171
Machine Translated by Google
Talukdar, S., & Souza, PS (1990). Equipes assíncronas. Em Anais do segundo
Subramanian, A., Drummond, LM, Bentes, C., Ochi, LS, & Farias, R. (2010). Uma heurística paralela para o
problema de roteamento de veículos com coleta e Pesquisa Operacional, 211(3), 612–622. doi:10.1016/j.ejor.2011.01.011.
2015.64. Natal, Brasil.
Sociedade de Matemática Industrial e Aplicada.
Talbi, E.-G. (2009). Metaheurísticas: Do design à implementação (1º). John Wiley &
Toth, P., & Vigo, D. (2002). O problema de roteamento de veículos. Filadélfia, EUA: SIAM -
Silva, MAL, de Souza, SR, Souza, MJF, & de Oliveira, SM (2015). Um framework de otimização metaheurística
multiagente com cooperação. Em 2015 brazil-ian conference on intelligent systems (BRACIS) (pp. 104–
109). doi:10.1109/BRACIS.
metaheurísticas e sistemas multiagentes para resolução de problemas de otimização: A
Filadélfia, PA, EUA: SIAM - Sociedade de Matemática Industrial e Aplicada.
problema de agendamento com tempos de configuração dependentes da sequência. European Journal of
Watkins, CJCH, & Dayan, P. (1992). Aprendizado de máquina, 8(3), 279–292.
Silva, M. A. L., de Souza, S. R., Souza, M. J. F., & de França Filho, M. F. (2018). Hybrid
Filhos.
Toth, P., & Vigo, D. (2014). Roteamento de veículos: problemas, métodos e aplicações (2ª
com restrições de janela de tempo. Pesquisa Operacional, 2(35), 254–264.
433–459. doi:10.1016/j.assoc.2018.06.050.
Watkins, CJCH (1989). Aprendendo com recompensas atrasadas. Cambridge, Inglaterra: Tese de doutorado
da Universidade de Cambridge.
Cambridge, MA, EUA: MIT Press.
revisão de frameworks e uma análise comparativa. Computação Suave Aplicada, 71,
Solomon, MM (1987). Algoritmos para problemas de roteamento e programação de veículos
Sutton, RS, & Barto, AG (1998). Aprendizagem por reforço: Uma introdução (1º).
entrega. Pesquisa em Computação e Operações, 37(11), 1899–1911.
171
Conferência SIAM sobre álgebra linear: Sinais, sistema e controle. São Francisco, EUA.
Vallada, E., & Ruiz, R. (2011). Um algoritmo genético para a máquina paralela não relacionada
Talukdar, S., Baerentzen, L., Gove, A., & Souza, PD (1998). Equipes assíncronas: Esquemas de cooperação
para agentes autônomos. Journal of Heuristics, 4(4), 295–321.
MA Lopes Silva, SR de Souza e MJ Freitas Souza et al. / Sistemas Especialistas Com Aplicações 131 (2019) 148–171
Machine Translated by Google