AÌrvore BinaÌria de Busca OÌtima - Uma ImplementacÌ§aÌƒo
DistribuÄ±Ìda
Felipe Reis e Caio Valentim
Departamento de InformaÌtica
PUC-Rio
7 de novembro de 2010

1

IntroducÌ§aÌƒo

O problema de encontrar a aÌrvore binaÌria de busca oÌtima para um conjunto de frequeÌ‚ncias
eÌ estudado haÌ anos no meio acadeÌ‚mico. O melhor algoritmo exato conhecido eÌ uma programacÌ§aÌƒo dinaÌ‚mica de complexidade O(n2 ) [3].
Contudo, o algoritmo sequencial naÌƒo eÌ facilmente paralelizaÌvel. Existem algumas propostas, como em [2] e [1]. Mas, em geral, o apresentado naÌƒo eÌ uma solucÌ§aÌƒo exata ou eÌ
complicado demais.
Neste documento apresentamos nosso desenvolvimento e implementacÌ§aÌƒo de uma versaÌƒo
distribuÄ±Ìda simples para o problema.

2

DefinicÌ§aÌƒo do Problema

O problema em questaÌƒo eÌ o de encontrar uma aÌrvore binaÌria de busca oÌtima para um conjunto
de chaves dadas as frequeÌ‚ncias de cada chave. Formalmente, temos as chaves A = {a1 â‰¤
a2 â‰¤ . . . â‰¤ an }, as frequeÌ‚ncias de buscas com sucesso de cada chave F = {f1 , f2 , . . . , fn } e as
frequeÌ‚ncias de buscas sem sucesso Q = {q0 , q1 , . . . , qn , qn+1 }. Onde qi representa a frequeÌ‚ncia
de buscas por chaves entre ai e ai+1 .
Nesse trabalho iremos assumir que todas as buscas saÌƒo com sucesso. Ou seja, iremos
descartar o conjunto Q. Desta forma, queremos criar uma aÌrvore binaÌria de busca que
minimize a seguinte funcÌ§aÌƒo:
n
X

fi Ã— n(i)

i=1

onde n(i) eÌ o nÄ±Ìvel do noÌ i na aÌrvore.

1

3

Artigos Relacionados

Os dois principais artigos estudados foram [2] e [1].
No primeiro, M. Karpinski propoÌƒe uma solucÌ§aÌƒo paralela capaz de calcular a solucÌ§aÌƒo oÌtima
em tempo O(n1âˆ’ ) para uma constante arbitraÌria 0 <  â‰¤ 1/2. Ele agrupa as diagonais em
conjuntos e calcula cada conjunto de forma eficiente a partir do preÌ-processamento de uma
estrutura que ele chama â€œsub-aÌrvores especiaisâ€. Apesar de interessante, a arbodagem naÌƒo eÌ
faÌcil de implementar, especialmente para quem naÌƒo possui grande experieÌ‚ncia com codificacÌ§aÌƒo
em paralelo.
Em outra linha, C. Mitica propoÌƒe em [1] uma solucÌ§aÌƒo paralela que comecÌ§a dividindo o
conjunto de chaves em intervalos menores, calculando as aÌrvores oÌtimas para cada intervalo
menor e, a partir das aÌrvores de cada sub-intervalo, constroi uma solucÌ§aÌƒo para todo conjunto.
Essa solucÌ§aÌƒo naÌƒo foi implementada pois acreditamos que a abordagem estaÌ errada e, de fato,
naÌƒo produz uma aÌrvore oÌtima.

4

SolucÌ§aÌƒo Sequencial

Abaixo dois lemas uÌteis para construcÌ§aÌƒo do algoritmo sequencial.
Lemma 4.1 (CriteÌrio de Otimalidade). Seja OP T (i, j) o custo da aÌrvore oÌtima para as
chaves {ai , . . . , ajâˆ’1 }. A seguinte recorreÌ‚ncia vale:
OP T (i, j) = min {OP T (i, k) + OP T (k + 1, j)} +
iâ‰¤k<j

jâˆ’1
X

ft

t=i

Lemma 4.2 (PrincÄ±Ìpio da Monoticidade). Seja r a chave que seraÌ a raiz da aÌrvore oÌtima
do intervalo [i, j) e [i0 , j 0 ) outro intervalo tal que i â‰¤ i0 â‰¤ j â‰¤ j 0 com raiz oÌtima r0 . Vale que
r â‰¤ r0 .
Com o primeiro resultado eÌ faÌcil construir uma tabela com a solucÌ§aÌƒo oÌtima em tempo
O(n3 ), usando o segundo lema podemos reduzir a complexidade para O(n2 ). Contudo, nossa
versaÌƒo distribuÄ±Ìda se basea na construcÌ§aÌƒo O(n3 ) implementada no capÄ±Ìtulo 8 do livro [4].

5

Algoritmo DistribuÄ±Ìdo

O algoritmo, derivado do segundo lema, na praÌtica, consiste em preencher diagonal por
diagonal de uma tabela de dimensoÌƒes n Ã— n. Desta forma, como o tempo para calcular cada
diagonal eÌ proporcional a O(n2 ), decidimos paralelizar esse caÌlculo. Existem n diagonais
entaÌƒo, de forma geral, nosso algoritmos consiste em:
for i = 0 to n
do
1.
Cada processo calcula um pedacÌ§o da i-eÌsima diagonal
2.
Cada processo distribui o seu pedacÌ§o entre os outros processos
done
2

Ou seja, cada processo fica resposaÌvel por um pedacÌ§o da diagonal que estaÌ sendo calculada no momento. ApoÌs o caÌlculo, os processos teÌ‚m que comunicar sua parcela aos outros
processos. O procedimento se repete ateÌ que naÌƒo existam mais diagonais para calcular.
Com isso, o tempo esperado para o caÌlculo de cada diagonal eÌ proporcional a O(n2 /p+np).
A primeira parcela da soma se refere ao processamento paralelo da diagonal e a segunda ao
custo extra de comunicacÌ§aÌƒo.

6

ImplementacÌ§aÌƒo

6.1

Detalhes do Cluster

O programa foi rodado no cluster da PUC-Rio que conta com 64 noÌs. Abaixo temos uma
descricÌ§aÌƒo teÌcnica do cluster retirada do site do suporte do departamento. A versaÌƒo do MPI
usada foi LAM 7.0.6/MPI 2 C++.
O site da PUC eÌ formado atualmente por treÌ‚s clusters: o primeiro, com 12 estacÌ§oÌƒes; o
segundo, com 20 estacÌ§oÌƒes; e o terceiro, com 32 estacÌ§oÌƒes. Todos estaÌƒo situados fisicamente no
server farm do DI. A arquitetura de cada cluster eÌ homogeÌ‚nea: os noÌs do primeiro cluster
teÌ‚m CPUs Intel Core 2 Duo 2.16 GHz e 1 GB de RAM; os do segundo teÌ‚m CPUs Intel
Pentium IV 1.70 GHz e 256 Mb de RAM; e os do terceiro teÌ‚m CPUs Intel Pentium II 400
MHz (Deschutes) e 280 Mb de RAM.
Os sistemas operacionais instalados saÌƒo: no primero cluster, Fedora Core 8 kernel 2.6.25.4-10;
e, nos demais, Red Hat Linux vers~
ao 9 kernel 2.4.20-31.9. A versaÌƒo de globus eÌ 2.4
em todos os clusters. As versoÌƒes de MPI disponÄ±Ìveis saÌƒo lam-7.1.2, lam-7.0.6 e mpich-1.2.6.
O domÄ±Ìnio do cluster eÌ par.inf.puc-rio.br. Os noÌs estaÌƒo numerados do n00 ateÌ o n63. O
ponto de acesso (via SSH) eÌ a maÌquina server.par.inf.puc-rio.br. A partir dela, os noÌs podem
ser acessados via RSH ou SSH. As conexoÌƒes entre os noÌs e o switch saÌƒo de 100Mbs.
Cada usuaÌrio possui uma aÌrea proÌpria de trabalho em cada noÌ, localizada no /home/local/login_usuaÌr
e acesso remoto a sua aÌrea de trabalho na maÌquina server.par.inf.puc-rio.br, atraveÌs
da pasta /home/server/login_usuario.

6.2

Executando CoÌdigo MPI

Uma vez que usamos a implementacÌ§aÌƒo LAM do MPI disponÄ±Ìvel no cluster, alguns passos
foram necessaÌrios para conseguir efetivamente executar nosso coÌdigo.
Em primeiro lugar, antes de rodar o programa, deve-se criar uma associacÌ§aÌƒo entre um
conjunto de maÌquinas (LAM). Isso eÌ feito com o camando lamboot -v machines.txt. O
flag -v eÌ de verbose e o arquivo machines.txt conteÌm as maÌquinas que seraÌƒo utilizadas.
Para o comando funcionar, uma seÌrie de requisitos devem ser observados. Por exemplo, as
maÌquinas devem permitir acesso ssh sem senha. Para verificar se o ambiente estaÌ corretamente configurado, podemos usar o camando recon -v que lista possÄ±Ìveis problemas.
AleÌm do ambiente de execucÌ§aÌƒo, precisamos de um coÌdigo compilado. Para compilar
utilizamos o mpiCC, compilador C++ para coÌdigo MPI. Com o coÌdigo compilado, o proÌximo
passo eÌ distribuir o executaÌvel entre as maÌquinas do cluster. O suporte do DI disponibiliza
o script mrcp para realizar esta tarefa.

3

Por fim, com todos os passos anteriores feitos, podemos executar, por exemplo, mpirun -np 8 a.out,
que roda o programa a.out de forma distribuÄ±Ìda entre 8 processos.

7

Experimentos

Os experimentos foram executados com 1, 4 e 8 processos rodando em maÌquinas separadas.
Testamos com entradas de tamanho 10, 50, 100, 500, 1000 e 5000. O tempo de execucÌ§aÌƒo para
cada par entrada x nuÌmeros de processos eÌ a meÌdia de 10 execucÌ§oÌƒes.
Abaixo a tabela dos resultados obtidos:
10
50
100
500
1000
3000
5000
p
1 0.521 0.522 0.520 0.850 3.344 107.159 503.236
4 0.555 0.571 0.580 0.910 2.569 63.275 295.765
8 0.58 0.610 0.650 1.061 2.379 38.024 172.599
E os resultados acima estaÌƒo dispostos tambeÌm na figura 1.

4

Tempo x Tamanho entrada
600

1 processo
4 processos
8 processos

500

+
Ã—
âˆ—

+

400
300

Ã—

200

âˆ—
+
Ã—
âˆ—

100
âˆ—Ã—
âˆ—Ã—
âˆ—
+
+
+
0Ã—
0

+
âˆ—
Ã—
âˆ—
Ã—
+
500 1000 1500 2000 2500 3000 3500 4000 4500 5000
Numero de chaves
Figura 1: Tempos de ExecucÌ§aÌƒo

8

ConclusaÌƒo

De acordo com os experimentos realizados, eÌ possÄ±Ìvel notar uma melhora significativa no
tempo de execucÌ§aÌƒo total para entradas superiores a 1000 chaves, como esperaÌvamos alcancÌ§ar
utilizando computacÌ§aÌƒo distribuÄ±Ìda.
Analisando o teste de 5000 chaves, por exemplo, temos um ganho de tempo de 291, 5%,
no comparativo entre a execucÌ§aÌƒo sequencial e a execucÌ§aÌƒo distribuÄ±Ìda em 8 processos.
Outro resultado que tambeÌm eÌ possÄ±Ìvel identificar atraveÌs dos experimentos realizados,
eÌ o oÌ‚nus introduzido pela comunicacÌ§aÌƒo entre processo, necessaÌria para manteÌ‚-los sincronizados, na versaÌƒo distribuÄ±Ìda do algoritmo. Se observarmos a execucÌ§aÌƒo do algoritmo para um
conjunto de 500 chaves, por exemplo, percebemos que a execucÌ§aÌƒo entre 8 processos eÌ cerca
de 11% mais custosa do que a execucÌ§aÌƒo para 4 processos, e 20% mais ineficiente do que a
versaÌƒo paralela.
Em suma, o algoritmo distribuÄ±Ìdo, para o problema de encontrar a aÌrvore de binaÌria de
busca oÌtima, apresentou-se bem mais eficiente, se comparado a sua versaÌƒo sequencial para
um conjunto grande de chaves. No entanto, executando-o com poucas chaves, sua utilizacÌ§aÌƒo
naÌƒo compensa o tempo gasto com comunicacÌ§aÌƒo e sincronizacÌ§aÌƒo entre processos.

RefereÌ‚ncias
[1] Mitica Craus. Parallel and distributed solutions for the optimal binary search tree problem. In IWCC â€™01: Proceedings of the NATO Advanced Research Workshop on Advanced Environments, Tools, and Applications for Cluster Computing-Revised Papers,
pages 103â€“117, London, UK, 2002. Springer-Verlag.

5

[2] Marek Karpinski and Wojciech Rytter. On a sublinear time parallel construction of
optimal binary search trees, 1994.
[3] Donald E. Knuth. Optimum binary search trees. Acta Inf., 1:14â€“25, 1971.
[4] Michael Quinn. Parallel Programming in C with MPI and OpenMP. Elizabeth A. Jones,
2004.

6

