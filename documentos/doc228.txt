Community-Oriented edge computing platform
Author links open overlay panel
Abdalla A. Moustafa
, 
Sara A. Elsayed
, 
Hossam S. Hassanein
Abstract
Democratizing the edge by capitalizing the underutilized computational resources of end devices, referred to as Extreme Edge Devices (EEDs), can foster various IoT applications. In this paper, we propose the Community Edge Platform (CEP). CEP is the first platform that exploits business, institutional, and social relationships to build communities of requesters and EEDs to eliminate recruitment costs and preserve privacy in EED-enabled environments. CEP promotes service-for-service exchange and utilizes a hierarchical control paradigm to prioritize the enrollment of nearby devices as workers. CEP also considers the fact that community-imposed constraints can lead to unbalanced work distribution. To alleviate this issue, we propose the Community-Oriented Resource Allocation (CORA) scheme. CORA accounts for community restrictions and strives to minimize the execution time and makespan while retaining a reasonable scheduler runtime. Towards that end, we formulate the resource allocation problem as a Bipartite Graph Matching problem. Comprehensive qualitative evaluations demonstrate the superiority of CEP compared to 12 prominent edge computing platforms in terms of various system architecture and performance features. Additionally, extensive simulations show that CORA outperforms six prominent resource allocation schemes by up to 44% and 7% in terms of makespan and execution time, respectively, while achieving a much faster runtime, outperforming the best of the six baseline resource allocation schemes by a factor of six.
Keywords
Edge computingExtreme edge devicesEdge PlatformsResource allocationGraph matching
1. Introduction
With the advent of the Internet of Things (IoT), it is foreseen that 125 billion IoT devices will be connected to the Internet by 2030 [1]. This proliferation is expected to lead to a surge in IoT applications and services with heavy processing and stringent Quality of Service (QoS) requirements, including augmented reality, Tactile Internet, smart cities, healthcare applications, and autonomous vehicles [2], [3], [4]. Cloud Computing (CC) might fail to accommodate the severe QoS requirements of such applications, since CC requires the transmission of an excessive amount of data to distant data centers, which can significantly increase latency and cause a huge traffic influx at backhaul links [5], [6].
Edge Computing (EC) has transpired as a propitious paradigm that can resolve the aforementioned issues by providing computing services closer to end users [7]. However, the dominant majority of existing EC platforms are infrastructure-based platforms that fall solely under the control of cloud service providers and/or network operators [8]. Challenging this monopoly by recycling the ample yet underutilized computational resources of end devices, referred to as Extreme Edge Devices (EEDs), can democratize the edge and open a new market for more players to manufacture and administer their own edge cloud [8], [9], [10], [11]. This market can enable individuals, businesses, and even municipalities to act as edge service providers and/or monetize their computing resources [9]. In addition, parallel processing at multiple EEDs can increase the computational power and bring the computing service much closer to end users, thus drastically diminishing the delay [10].
Despite its advantageous impact, EED-enabled computing is less secure than infrastructure-based EC paradigms. This lack of security is due to relying on dubious machines that trigger more privacy concerns. In addition, the need to recruit many EEDs for parallel execution of a single partitioned task can trigger significant recruitment costs. Recently, some EED-enabled computing platforms have addressed these problems by offloading tasks to other devices in the local network owned by the same user [12]. This approach ensures a higher level of privacy, since the devices and their applications are trusted and vindicated by the user. Moreover, latency can be drastically reduced because of the proximity factor. Finally, users do not pay to use their own devices. However, restricting the scope to the local network and the user’s own devices severely limits the resource pool, reducing the utilization gain and the chance of finding a suitable device for task offloading, which can significantly reduce the QoS.
In addition to security and privacy, there are other major gaps in the system architecture and performance features of existing edge computing platforms. Many infrastructure-based EC solutions suffer from full centralized control, which not only introduces bottlenecks and single points of failure but also limits the flexibility and scalability needed to handle the growing number of IoT devices and applications [13]. Moreover, these centralized architectures are often costly, as they rely heavily on expensive infrastructure-based edge nodes and ecosystems [13], which can stifle innovation, create monopoly by cloud service providers and network operators, and prevent smaller players from participating in the edge computing market.
Furthermore, many existing edge computing platforms also face limitations regarding deployment location flexibility, worker ownership models, limited application areas, and compatibility with multiple operating systems. These restrictions can hinder the deployment of edge services across diverse environments and limit the scalability needed to accommodate a growing number of IoT devices. Additionally, platforms that impose specific workload formats or restrict application areas fail to provide the versatility required for a wide range of IoT applications. Excessive costs, as well as security and privacy concerns further exacerbate these issues, as many platforms do not offer robust mechanisms to protect sensitive data or ensure secure execution of tasks, which hinders wide adoption and leads to increased reluctance by many clients/requesters to subscribe to the service. As a result, there is a pressing need for cost-efficient edge computing solutions that can fully harness the underutilized resources of EEDs, provide enhanced deployment flexibility, support diverse workload formats, and guarantee high levels of privacy, security, and scalability to meet the demands of modern IoT ecosystems.
In this paper, we propose the Community Edge Platform (CEP), where we interweave the notion of community with edge computing. In particular, we exploit the wide range of business, institutional, and social relationships among individuals to harvest the underutilized computational resources of user-owned EEDs that form communities of trustworthy and cost-free devices. A community can be a neighborhood, a group of friends, a hospital, or devices owned by an organization in different geographic locations worldwide with different time zones or load peak times.
CEP addresses the aforementioned gaps in existing edge computing platforms. Such gaps present significant barriers to the widespread adoption and effective operation of IoT ecosystems. Full centralized control, high costs, limited deployment flexibility, and insufficient privacy and security mechanisms hinder the scalability, performance, and security of IoT applications, which require fast, reliable, and secure processing of massive amounts of data from numerous distributed devices. CEP provides a novel solution that directly addresses these challenges by incorporating the concept of community into edge computing and offering a hierarchical control paradigm that leverages the underutilized resources from EEDs. CEP fosters the concept of service-for-service exchange. The goal is to create a global network where users are allowed to form separate communities of trusted users, each owning one or more devices. This conception of community expands the scope of the available pool of resources, while preserving privacy and eliminating any recruitment costs. Additionally, CEP supports diverse deployment scenarios, ensuring flexibility in terms of workload formats and application areas, fostering operability with multiple operating systems, and enhancing privacy and security. This enables CEP to advance the field of IoT by fostering more efficient, scalable, and cost-effective edge computing with high security and privacy standards tailored to the unique needs of IoT applications.
To fully leverage the notion of community in CEP, it is imperative to consider the special requirements of resource allocation in community-based environments. In particular, it is crucial to consider the restrictions imposed by communities, in terms of the order of assignment and valid requester-worker associations based on community affiliations. Existing resource allocation schemes are not tailored for community-based environments, and thus do not consider community-based restrictions on task assignments. This omission can lead to unbalanced work distribution, which can significantly affect the QoS, particularly in terms of makespan (i.e., the maximum time needed to execute all tasks) and execution time. Thus, we propose the Community-Oriented Resource Allocation (CORA) scheme and incorporate it within the scheduler module in CEP to account for community-driven restrictions and constraints.
Our contributions can be summarized as follows:
•
We propose a new edge computing platform, CEP, which integrates the notion of communities into the edge computing ecosystem. CEP is the first edge computing platform to exploit the concept of communities to enhance security, protect privacy, and remove recruitment costs in EED-enabled computing environments.
•
We foster a hierarchical control paradigm rather than full centralized control. The introduced hierarchical control paradigm consists of clusters and communities. This approach alleviates the problems associated with full centralized control, and allows adjacent devices to be grouped into clusters, with each cluster comprising devices owned by a single user. This setup allows each device to prioritize offloading tasks to other devices within its cluster, ensuring lower latency and enhanced security, while retaining the option to offload tasks to devices globally, provided they belong to one of the user’s communities.
•
We allow the use of containers without requiring any modifications, making it easy for users to package their code within a container and to leverage the millions of premade containers available online as they are, or customize them to handle new tasks. Additionally, CEP offers flexible deployment options, optimizing location choice and worker ownership, and enhances network coverage and adaptability across diverse environments, including wide area networks. CEP also ensures compatibility with multiple operating systems, promoting seamless integration with various IoT devices, supporting interoperability, and simplifying deployment in heterogeneous ecosystems. By eliminating recruitment costs and imposing no restrictions on application areas or containerized workload formats, CEP provides a versatile and cost-effective solution for scaling IoT applications, ranging from real-time analytics to complex processing tasks. Furthermore, CEP guarantees high levels of privacy, security, and scalability, ensuring that sensitive IoT data is protected and that the system can efficiently accommodate the growing number of IoT devices, meeting the evolving demands of modern IoT ecosystems.
•
We develop various modules, such as user authentication, cluster manager, community management portal, data manager, scheduler, benchmark manager, notification handler, and service execution time estimator. We provide the source code via GitHub1 to enable open access to CEP and make it readily available for the research community.
•
We address the constraints imposed by communities by introducing a new resource allocation scheme called CORA. CORA employs a graph-based approach to allocate containerized services within CEP and community-centric edge computing environments. Its goal is to ensure high QoS by minimizing the makespan and execution time of assigned services while maintaining a practical and efficient scheduler runtime.
We compare CEP to 12 prominent edge computing platforms by analyzing a total of 13 features grouped into two categories: (1) system architecture, deployment, and application features, and (2) performance features. Our comparative study shows that CEP stands out as a flexible and easily deployed and managed system that can cover wide area networks. The comparison also shows the leverage of CEP in eliminating recruitment costs and illustrates its ability to provide a high level of scalability, privacy, and security. Furthermore, extensive simulations show that CORA, which is incorporated into the scheduler module in CEP, outperforms six baseline resource allocation schemes by up to 44% and 7% in terms of makespan and execution time, respectively, in scenarios with high service heterogeneity and/or high worker heterogeneity. Additionally, CORA achieves a significantly faster runtime, being six times quicker than the best-performing scheme among the six baseline resource allocation schemes.
The remainder of the paper is organized as follows. In Section 2, we highlight some related work. In Section 3, we provide a detailed description of the proposed platform (CEP). In Section 4, we present the qualitative evaluation of CEP. In Section 5, we describe the underlying resource allocation scheme (CORA). In Section 6, we discuss the performance evaluation of CORA. In Section 7, we highlight our conclusions and future directions.
2. Related work
2.1. Edge Computing (EC) systems and platforms
Several EC systems and platforms have been proposed in the literature. Some EC platforms are comprised of provider-owned workers [13], [14], [15], [16]. Such platforms rely on the use of edge nodes that are owned by network providers or cloud providers. This category involves three platforms, namely, Akraino Edge Stack [14], Mutable [15], and MobiledgeX [16]. We discuss each of these platforms and the underlying methods used in them below.
Akraino Edge Stack, initiated by AT&T and now hosted by Linux Foundation, is a project creating an open-source software stack that supports high-availability cloud services optimized for edge computing systems and applications [14]. For example, Akraino can be used with the TARS architecture for Vehicle-to-Everything (V2X) applications [14]. Akraino supports multiple workload types, such as Virtual Machines (VMs) and containers. Fig. 1 shows the architecture of the Akrino stack and the underlying methods used for its implementation. As shown in the figure, Akraino is implemented by using three layers. The first layer is the Infrastructure as a Service (Iaas) layer, which allows the deployment of hardware, virtual machines, as well as containers. The second layer is the Platform as a Service (PaaS) layer, which uses the TARS framework, a flexible microservices framework that facilitates scalable and efficient deployment and management of distributed services in edge and cloud environments [14]. Consequently, it can support high-performance Remote Procedure Calls (RPCs), deploy services in scale-out scenarios, and provide user-friendly service management features. The third layer is the Software as a Service (SaaS) layer, which contains the available applications and enables the policy of data distribution and offloading to be configurable based on different applications [14]. Additionally, the SaaS layer incorporates the implementation of orchestration and management Application Programming Interfaces (APIs), enabling application management and cluster management by handling microservices deployed as a set of containers or VMs.
Mutable [15] is another prominent provider-owned workers platform. Mutable’s implementation revolves around the concept of a Public Edge Cloud, which aims to bring cloud computing resources closer to end users by utilizing existing network infrastructure. This is achieved by transforming servers owned by network operators (e.g., ISPs and telecom companies) into edge nodes that can handle computing tasks, thereby reducing latency and improving performance for next- generation applications such as video and audio recognition, virtual reality (VR), augmented reality (AR), IoT, robotics, autonomous vehicles, and drones [15]. Additionally, Mutable utilizes Advanced RISC Machines (ARM) and Graphics Processing Unit (GPU) architectures to support a wider range of applications and offer competitive pricing compared to traditional cloud computing [15]. Mutable leverages ISP infrastructure to execute user requests before they reach the public Internet, which reduces latency and enhances security. By utilizing ARM and GPU architectures, Mutable delivers cost-effective, high-performance solutions tailored to the needs of next-generation applications. It also leverages Software Defined Networking (SDN) for efficient cross-network optimization, enabling data transfer between known networks at reduced costs. This approach eliminates the need for upfront capital expenditures, avoiding the high costs of building and maintaining in-house infrastructure, including personnel, hardware, and real estate. Mutable’s competitive pricing strategy allows for shifting workloads based on current pricing conditions to maintain performance while managing costs effectively. Furthermore, Mutable supports agile product development by using a container and microservices-based architecture, facilitating faster iteration and quicker deployment of new products.

Download: Download high-res image (308KB)
Download: Download full-size image
Fig. 1. Akranio edge stack.

Adapted from [14].
Another provider-owned workers platform is MobiledgeX [16]. Accedian and MobiledgeX [16] joined forces to enable enterprises to deliver applications to end users on a large scale with consistent performance, while maintaining an acceptable level of security by utilizing the edge. The MobiledgeX Edge-Cloud R2.0 platform provided by MobiledgeX acts as the application environment deployed at data centers and commodity clouds [16]. Accedian contributes to the common trust model of the network and application performance monitoring. MobiledgeX is implemented by using a distributed architecture that leverages local edge resources provided by telecom operators, ensuring that computing happens closer to the end-users to reduce latency. Additionally, MobiledgeX implements the use of a global control plane for centralized management and orchestration, which coordinates the deployment, scaling, and monitoring of applications across multiple edge locations. As illustrated in Fig. 2, Accedian and MobiledgeX provide an open-source ecosystem for low latency 5G digital experiences by utilizing edge devices. The MobiledgeX control plane unifies the management and development for different Independent Software Vendors (ISVs) and bridges the gap between EEDs, On-Premise Edge, Telcom Edge, and Centralized Cloud Edge [16]. Furthermore, MobiledgeX maintains ubiquitous computing by granting developers access to Application Programming Interfaces (APIs) that can utilize the underlying infrastructure capabilities and orchestration across any environment. Developers use the underlying Software Development Kit (SDK) to write their application once, and then it can be deployed anywhere the MobiledgeX platform is deployed.

Download: Download high-res image (359KB)
Download: Download full-size image
Fig. 2. Accedian-MobiledgeX.

Adapted from [16].
The aforementioned platforms, Akraino Edge Stack, Mutable Public Edge Cloud, and MobiledgeX, rely on provider-owned machines as workers. Those machines can be owned by network providers, data centers, or cloud providers. This is one step away from cloud computing because worker devices are closer to the end user and the workers are at the edge of the network. This falls under the umbrella of edge computing. However, the main challenges encountered by this type of platforms include the limited resources in the face of the increasingly growing demands, as well as the high costs and privacy risks from sharing data with provider-owned devices. In particular, the limitation in terms of resources stems from the expected excessive demands due to the rapid growth in Internet-connected devices. A recent study estimates that by 2030, there will be 125 billion Internet-connected devices, which will both generate and rely on these platforms and services [17]. Moreover, the high costs involved in establishing the edge computing infrastructure associated with platforms that use provider-owned workers present a major challenge, discouraging communication service providers from making substantial investments in EC, which, in turn, hampers the widespread adoption and scalability of this type of edge computing platforms [18]. This situation worsens the scarcity of edge resources, and during periods of high demands, it makes these platforms less suitable for handling sudden spikes of compute-intensive tasks.
Other platforms rely on requester-owned workers, and utilize the client’s (i.e., requester’s) machines to do their own work [12], [19], [20], [21], [22], [23]. Such platforms provide a way to facilitate deployment, management, and monitoring of deployed services, as well as some pre-built modules that can be adapted to suit the requester’s needs. This category involves six platforms, namely, EdgeX Foundry [19], Azure IoT Edge [20], Apache Edgent [21], Kubernetes [22], AWS IoT Greengrass [23], and HomeEdge [12]. These systems provide the users with platforms, frameworks, and services to facilitate the communication, scheduling, and monitoring of varying devices in different environments [12]. We discuss these platforms and the underlying methods used to implement them below.
EdgeX Foundry is a standardized open-source interoperability framework for IoT edge computing, best suited for edge nodes such as routers, gateways, and hubs [19]. It has the ability to connect, manage and collect data from various sensors and devices via a variety of protocols. It can also export the data to local applications at the edge of the cloud for further processing. EdgeX is agnostic to hardware, CPU architecture, operating system, and environment. Fig. 3 illustrates the architecture of EdgeX foundry and the underlying methods used to implement it. EdgeX foundry starts with the “South Bound” at the bottom, where all EEDs and IoT objects operate. The network connects with those devices and sensors to collect data. At the other end of the architecture, there is the “North Bound”, which contains the cloud used to store, aggregate, and analyze data to turn it into useful information. EdgeX Foundry is implemented by acting as a link between these two sides regardless of the differences in hardware, software, and network. EdgeX utilizes the device profile concept to define key information about the EED, namely, the object type, the collected data format, the stored data format, and the commands that can be used to manipulate this object. In addition to storage, EdgeX has an SDK that allows third-party developers to create and manage device services. The device service handles data formatting and translation of commands to operations that are executable by the devices specified by the service.

Download: Download high-res image (339KB)
Download: Download full-size image
Fig. 3. EdgeX foundry.

Adapted from [19].
As shown in Fig. 3, EdgeX is implemented by using a total of six layers. Four of those are service layers, accompanied by two augmenting layers. The layers in a bottom-up order are as follows [19]: (1) Device Services Layer: This layer converts the data format from collected to stored, forwards the formatted data to the core services layer, and translates the commands from the core services layer. (2) Core Services Layer: This layer includes four components, core data, command, metadata, and configuration. The core data component stores and manages the data collected from the EEDs. The command component offers the API for command requests from the northbound to EEDs. The metadata component holds and manages the metadata, such as device profiles and services. The registry and configuration component allows configuration and modifications to all the other microservices operating parameters. (3) Supporting Services Layer: The supporting services provide edge analytics and intelligence. The rules engine handles cases where some commands need to be triggered for a specific range of collected data. Meanwhile, alerting and notifications provide the ability to notify other people or systems by email or using a Representational State Transfer (REST) callback. The scheduling component can trigger periodic operations on a specified schedule to clean the data for example. Logging stores the running information and warnings. (4) Export Services Layer: This layer acts as the connecting bridge between EdgeX and the northbound. Client registration allows cloud or other applications to be included in the data recipients list of one or more devices, While Distribution delivers the actual data to those registered clients. (5) System Management: This augmenting layer handles the management operations for EdgeX, such as installation, upgrade, starting, stopping, and monitoring. (6) System Security: This augmenting layer is implemented to protect the data and device information within EdgeX.
Azure IoT Edge is a cloud service provider managed by Microsoft Azure [20]. Its purpose is to migrate cloud analytics to edge devices. These edge devices can be gateways, routers, or any other device that can provide computing resources. The programming model of Azure IoT Edge is similar to other Azure IoT services. That is, it allows users to move their existing application hosted on Azure to the edge for lower latency. Additionally, combined with other Azure services, Azure IoT Edg can be used to deploy advanced tasks on edge devices, such as machine learning and image recognition [20]. As depicted in Fig. 4, Azure IoT Edge is implemented by using three components: the IoT Edge modules, IoT Edge runtime, and IoT Edge cloud interface. The IoT Edge modules and IoT Edge runtime run on edge devices. Meanwhile, the IoT Edge cloud-based interface runs in the cloud. The IoT Edge modules are containerized instances that run the user code, where a module image is a docker image containing the customer code. The IoT Edge runtime is the local manager on EEDs. It includes two modules, the IoT Edge hub and the IoT Edge agent. The IoT Edge hub is a local proxy for the hub and is a central message hub in the cloud. It allows modules to communicate with each other and send data to the IoT hub. The IoT Edge agent handles the deployment and monitoring of the IoT Edge modules. The IoT Edge cloud interface acts as a portal for users to manage their applications by creating, deploying, and monitoring the applications on edge devices.

Download: Download high-res image (186KB)
Download: Download full-size image
Fig. 4. Azure IoT edge.

Adapted from [20].
Kubernetes [22], also known as K8s, is an open-source system for automating the deployment, scaling, and management of containerized applications. It was originally designed by Google but is now maintained by the Cloud Native Computing Foundation. Fig. 5 depicts the architecture of the Kubernetes cluster, which consists of a set of workers (i.e., nodes). These nodes run containerized Docker applications, and there is at least one node per cluster. The implementation of the Kubernetes cluster relies on the control plane, which manages the worker nodes and the pods within the cluster. The control plane can run across multiple computers, with multiple nodes per cluster, to ensure fault tolerance and high availability. The API server provides access to the Kubernetes API, which is used to facilitate communication with the nodes. Etcd is a consistent key–value store used for storing all cluster data [22]. The scheduler monitors for new unassigned pods and assigns them to suitable nodes for execution. The controller manager handles various management tasks, such as detecting when nodes fail and managing the lifecycle of pods and other resources. The cloud controller manager integrates the control plane with a cloud provider’s API to manage cloud-specific services. To ensure seamless operation and scalability in its implementation, Kubernetes uses kube-proxy, a key component of its networking model. Kube-proxy runs on each worker node and manages network communication within and between nodes, enabling service discovery and load balancing. Additionally, the kubelet, an agent that runs on each worker node, continuously monitors the health of containers and ensures they are running according to the specifications defined in the pod configuration files.
The main challenges faced by the aforementioned platforms that use requester-owned workers include the fact that the requester still needs to rent/purchase, maintain all the workers, as well as having administrative control over these devices to configure and sustain the work environment. Although this approach enhances privacy and security by allowing data to typically remain on the user’s own machines, it makes these systems costly, since all the worker devices must be either purchased or rented [12]. Some platforms may eventually transfer the data to external cloud services, which can also be managed by the user for an additional cost [19], [21]. Additionally, limiting resources to devices within the same network or those owned by the same user can lead to inefficient resource utilization.

Download: Download high-res image (142KB)
Download: Download full-size image
Fig. 5. Kubernetes.

Adapted from [22].
In addition to the aforementioned platforms, there are EC platforms that rely on user-owned workers and execute workloads on user-owned machines [24], [25], [26]. Such platforms allow any user to offer their machine’s computational resources to requesters in exchange for profit. EED-enabled computing that relies on the use of EEDs can be fostered by such platforms. This category involves three platforms, namely, Golem Network [24], iExec [25], and OTOY [26].
Golem Network [24] is a global, open-source, decentralized supercomputer available to the public. This supercomputer combines the computing power of users’ devices, which can range from personal PCs to data centers. Users in Golem are classified into two roles: requesters and providers. Fig. 6 depicts the different possibilities for each role and how communication between roles can occur regardless of machine type. In Golem, the Golem Daemon on both the requester and provider sides is implemented to manage task submission, execution, and communication between parties. The Python API, Nodejs API, and REST API on the requester side facilitate interaction with the network, while the VM, Docker container, and application code on the provider side handle the isolated execution of tasks to ensure compatibility. Additionally, Web Assembly allows application code to run efficiently in a low-level execution environment on the provider’s side. Golem uses Polygon, a decentralized Ethereum scaling platform, to handle the recruitment costs paid to workers. It implements a decentralized market where users can exchange services by offering or accepting others’ offers. This model allows users to run their workloads on other people’s machines at a lower cost compared to traditional cloud computing. Moreover, the market can scale to a very large number of users. Despite these benefits, Golem has technical barriers for new users, who must re-implement their workloads to use the provided Golem SDK. Additionally, the data privacy of requesters and the security of providers’ machines could be at risk due to running unverified workloads on untrusted machines. In contrast, our proposed platform CEP supports unrestricted workloads in the form of containers and ensures a higher level of security and privacy.
The iExec platform [25] uses a technology that relies on the implementation of Ethereum smart contracts and building a virtual cloud infrastructure with high-performance computing services on-demand. iExec’s computing providers, also known as workers, provide their machine’s computing powers to execute computational tasks in exchange for a reward in Run on Lots of Computers (RLC) tokens. Worker machines are grouped into worker pools led by pool managers. A pool manager is a lead entity that organizes the workload, signals how many tasks it can process, and determines the price for each task. The pool manager receives a fee for managing the worker pool despite not executing the actual workload. Pool managers compete to attract workers to their worker pool by providing efficient management and guaranteeing earnings for workers. The pool manager approach lifts the burden of scheduling from the system to users acting as pool managers. However, it reduces the system’s reliability since the pool manager acts as a single point of failure for all connected workers. iExec is also more prone to privacy and security risks due to the nature of the system’s openness to new requesters and providers without a real way of verifying workloads and workers. In contrast, our proposed platform CEP utilizes a more robust hierarchical control paradigm, eliminates recruitment costs, and ensures a higher level of security and privacy.

Download: Download high-res image (273KB)
Download: Download full-size image
Fig. 6. Golem.

Adapted from [24].
OTOY [26] implements the use of blockchain with Ethereum smart contracts technology with proof-of-render work on the OctaneRender Cloud (ORC). Limiting the workload to rendering reduces the risk of malicious workloads. Additionally, splitting the work into render tokens reduces the possibility of data leaks. However, OTOY is limited to rendering tasks and cannot be used for general tasks or services and it still requires a payment to execute the workload. In contrast, CEP does not impose restrictions on workload format or application types and does not require incurring any payments.
As opposed to the existing edge computing platforms, we propose CEP that leverages the notion of communities to foster service-for-service exchange in EED-enabled computing environments. CEP utilizes a robust hierarchical control paradigm, removes the need for recruitment expenses, and provides a high degree of security and privacy.

Download: Download high-res image (251KB)
Download: Download full-size image
Fig. 7. Clusters, users, and communities in CEP.

2.2. Resource allocation schemes in edge computing
In recent years, the Docker container technology has gained significant momentum [27]. Commercial container orchestrators, such as Docker Swarm [28] and Google Kubernetes [22], guarantee users the freedom to execute a wide range of jobs and services. However, such orchestrators rely on simple and generic resource allocation algorithms. Kaur et al. [29] introduce the Kubernetes-based Energy and Interference Driven Scheduler (KEIDS), which uses a multi-objective Integer Linear Programming (ILP) formulation for Industrial Internet of Things (IIoT) applications in edge computing ecosystems. KEIDS aims to minimize the emission of carbon footprints, interference, and energy consumption by minimizing the energy utilization of edge nodes in IIoT for optimal utilization of green energy. In [30], Liu et al. introduce a resource allocation scheme that aims to jointly optimize edge server downtime, communication latency, and resource wastage when allocating a unified set of IoT applications across several edge cloud platforms. In [31], Cang et al. introduce a framework for resource allocation that combines communication and computation, specifically designed for Mobile Edge Computing (MEC) systems with a focus on semantic awareness. In this setup, tasks randomly arrive at each terminal device (TD), which must either process them locally or offload them to the MEC server. To reduce the load on data transmission, each TD sends only the small-sized, extracted semantic information of the tasks to the server, rather than the large-sized original raw data. The authors formulate an optimization problem involving the joint management of semantic-aware partitioning, communication, and computational resources. The goal is to minimize the overall energy consumption of the system while ensuring compliance with long-term delay and processing rate constraints. In [32], De’bas et al. propose a resource allocation scheme that optimizes response delay, service capacity, and worker retention in EED-enabled edge computing by guaranteeing that each worker receives a demanded reward, thereby mitigating worker attrition. They introduce the notion of multitiered computational capabilities that can be solicited from each worker depending on the financial incentive tied to the offloaded task. This method acknowledges and adjusts to the diverse preferences and limitations of users. By allowing multitiered computational capability solicitation, this approach optimizes resource allocation while accounting for the dynamic user access behavior inherent in extreme edge environments. In [33], Zhu et al. explore the use of Intelligent Reflecting Surface (IRS) technology within vehicular networks supported by MEC, with a focus on minimizing task scheduling delays by enhancing the distribution of limited processing and IRS resources. Their strategy involves developing a dynamic task scheduling algorithm that incorporates both communication and computation elements, taking into account factors like vehicle movement patterns, transmission quality, and the sizes of tasks. Lu et al. [34] formulate an ILP problem for offline resource allocation optimization. Extensive evaluations show that given sufficient resources for job execution, this approach can guarantee predefined deadlines for all defined jobs while minimizing the total interruption overhead. However, it is worth noting that optimization-based approaches are often NP-complete, rendering them impractical for solving large-scale problems in polynomial time [35]. In contrast, heuristic schemes tend to provide more time-efficient solutions.
Multiple heuristics have been proposed to allocate containers [22], [36], [37]. These approaches are generally faster and more scalable than the aforementioned approaches. However, the solutions are not guaranteed to be optimal. One of the simplest yet most prominent heuristics is WorkQueue, which is used in Google Kubernetes [22]. WorkQueue selects a task randomly and assigns it to the device that has the minimum workload and/or maximum available resources. Min-min [36] uses the minimum completion time as a metric, meaning that the task that can be completed the earliest is given a higher priority. Max–min [36] starts like the Min-min by calculating the minimum completion time for every service, but then proceeds to select the one rendering the maximum-minimum completion time. Longest Job to Fastest Resource-Shortest Job to Fastest Resource (LJFR-SJFR) [37] is a combination of both the Min-min and Max–min heuristics, where it alternates between them by assigning the longest service to the fastest available device, then the shortest service to the fastest available device, and so on. In contrast, in the Suffrage heuristic scheme [37], priority is given to the service that suffers the most from not assigning it at the current step. This is done by calculating the difference between the minimum and second minimum completion times for every service and choosing the one with the maximum suffrage. In [38], Mohamed et al. propose a heuristic-based resource allocation approach using game theory to facilitate service replication among various service providers in edge computing. They aim to achieve scalable service deployment in a cost-effective manner, while considering the strict Quality of Service (QoS) requirements of real-time applications that serve multiple user groups. In [39], Liu et al. examine the combined optimization challenge of Service Migration and Resource Allocation (SMRA) within MEC environments, aiming to minimize latency for IoT users. They introduce a joint SMRA scheme utilizing deep reinforcement learning (DRL), which takes into account IoT user mobility and determines service migration decisions, migration destinations, and resource allocation by employing long short-term memory (LSTM) and parameterized deep Q-network (PDQN) techniques.
To the best of our knowledge, existing resource allocation schemes do not cater to the unique needs of community-based edge computing environments. Unlike existing approaches that overlook the restrictions associated with community-based edge computing environments, we introduce a novel resource allocation scheme tailored for such scenarios, and we incorporate it into the scheduling module in our community edge computing platform CEP.
3. Community Edge Platform (CEP)
3.1. System overview
In CEP, an edge device can be any machine, whether stationary or portable. Any device running the software can serve as a requester, worker, or both. Clusters and communities consist of these devices. Each cluster is composed of a group of devices within the local network owned by the same user, with one device acting as the local scheduler for this network, referred to as the cluster head. The cluster head acts as a communication gate between the cluster and the server that connects it to other clusters. A community is composed of one or more clusters and represents a group of users open to exchanging services and executing offloaded tasks among each other. Note that a user can be a member of multiple communities.
To further illustrate the concept of clusters and communities in CEP, Fig. 7 depicts a system of four clusters owned by a total of three users (User A, User B, and User C), which form two communities (Community X and Community Y). User A is a member of both Community X and Community Y, whereas User B is a member of Community X, and User C is a member of Community Y. Any device that is a member of a cluster owned by User B can offload services to any device owned by Users A or B. This is since User A and User B are both members of the same community (Community X). In contrast, the devices of User A can exchange services with Users B and C. Evidently, CEP enables the requesters to offload their tasks to trusted devices that are members of their community.
Task offloading is done regardless of whether the devices are within the same cluster or in a remote location, or whether all clusters in this community are owned by a single user, a single organization, or by several entities, as long as all cluster owners trust their community members. All connected devices can act as requesters, workers, or cluster heads, depending on the configuration and the running scenario. At the cluster level, every group of devices on a LAN can be handled by a single device, known as the cluster head. This cluster head in turn has access to the server (i.e., scheduler), which can receive and schedule services between different clusters within the same community. Note that the notion of community enables fostering a wide range of user custom services and applications.
Fostering user custom applications in CEP requires overcoming some new challenges, especially when dealing with a wide range of devices and operating systems. Thus, we opt to use the containers ecosystem. This ecosystem allows the requester to provide almost any source code as a task that can be offloaded in a containerized form that can run on a large and diverse group of smart devices, with no regard to the programming language, required libraries, or application domain [35]. In addition to flexibility on the software side, this setting enables the services to run on any Docker Container-enabled devices. Docker containers are selected due to their quick deployment, easy management, safety, and hardware independence [35]. Containerized services are triggered by the requesters and are offloaded to workers that share the same community.
Let 
 denote the set of 
 containerized services in the server queue that need to be offloaded to workers within communities. We assume independent services with no inter-service data dependencies. In addition, preemption is not allowed, since the container’s state cannot be relocated to a new device without wasting additional resources. Each service originates from a requester within a cluster that belongs to a user with a set of valid communities, each of which has candidate workers to which the service can be offloaded safely. The set of available workers is denoted 
, where 
 is the total number of available workers. Each worker has its updated benchmarks, which indicate the available resources, as well as a cluster identifier. The latter can be traced back to the corresponding requester and thus to the associated set of communities. On the machine level, the assigned services are executed in a First-Come, First-Served (FCFS) order.
To fully leverage service assignment within communities, CEP consists of different components, including user authentication, cluster management, community management, data management, scheduling, benchmarking, notification handling, and service execution time estimation. The architecture of CEP and its underlying components and modules are explained in the next sections. Note that a detailed description of CEP can be found in [40].
3.2. System architecture
Fig. 8 depicts the architecture of CEP. It is split into two different applications; the client-side, which runs on the user’s devices within clusters, and the server-side, which runs on the server that is accessible by all clusters. The client-side has two modes, one for all devices to allow the machine to act as a worker or requester. The other mode is used to enable a device to run as a cluster head. The cluster head collects benchmarks and service requests from cluster members and forwards them to the server. In addition, the cluster head forwards notifications and results from the server to the original requester, and has a separate scheduler module to act as the first and preferred level of offloading handling. The server-side of the system is hosted on a public server that can be accessed by potential clusters. The server manages user authentication, keeps track of active clusters, stores and provides service data, allocates services to workers, estimates execution time, and handles service notifications. Additionally, there are a few components that facilitate communication between the two application sides (i.e., client and server), including a message queue and Docker repositories.

Download: Download high-res image (466KB)
Download: Download full-size image
Fig. 8. CEP Architecture and Flow.

1- Client-side (Devices and Cluster Head)
The client-side, implemented at the devices, leverages some of the modules used in HomeEdge [12]. This is since such modules can be deployed as needed, and they provide a reliable and tested implementation for basic functionalities, including device discovery, data storage, basic benchmarking, docker container pulling and execution, and network communication between devices. However, in contrast to HomeEdge, CEP ensures a centralized rather than a decentralized approach. This is because although decentralized systems require fewer network messages to communicate and are more resilient against one point of failure issues, they are more vulnerable to malicious attacks. This is since any device can send requests directly to any other device. Thus, if anyone gets access to the devices’ network, they can run any code they want on all devices. Furthermore, in a decentralized approach, each device needs to constantly keep track of all the other devices in the network and ask for their benchmarks whenever there is a request. This traffic increases the load on the network, which exponentially increases with the number of devices, making it unsuitable for large-scale networks.
In CEP, centralizing the cluster around the cluster head is essential to facilitate communication with the server. As a result, we adopt a centralized paradigm. Furthermore, we introduce several new modules at the cluster head to perform various tasks, including collecting workers’ information and requests, authenticating users, registering each cluster to a designated server, forwarding benchmarks, requests, and results to the server, assigning services to workers, and managing resource allocation within the cluster using a community-aware scheduler. Note that the scheduler module is implemented at the server.
2- Server-side (Scheduler and Community Manager)
The server side acts as a communication gateway between clusters. Moreover, it manages communities, centralized databases, and execution time estimation. We adopt a stateless approach, in which each session is carried out as if it is the first time, and responses are not dependent on data from a previous session. In contrast to a stateful approach, a stateless approach provides high scalability, scheduling in high availability environments, easier caching, less need for active storage, and better failure recovery [35]. The modules implemented at the server side are defined as follows:
1.
User Authentication and Authorization: This component handles user login and registration for the platform. Additionally, it manages user sessions and requests from clusters to ensure the separation between clusters in different communities and prevent malicious activities from unknown devices. Users are authenticated using a username and password to access server APIs, including account management, token generation, and community creation and management. To enable clusters to authenticate automatically, a Personal Access Token (PAT) can be generated and encapsulated in the cluster head, facilitating faster and automated connections between the cluster and the server. Furthermore, storing the PAT on the cluster head enhances security, as it is typically the most secure device within the cluster. PATs are less susceptible to brute force attacks due to their length and randomness. Additionally, they can be reused across multiple clusters.
2.
Cluster Manager: This module maintains clusters, keeps track of active clusters, and identifies new connecting clusters upon request. Thus, it can provide the scheduler with potential cluster candidates to offload tasks to.
3.
Community Management Portal: This module provides a fully-fledged API endpoint that allows community owners to manage their communities while giving members the option to view and leave communities when desired.
4.
Data Manager: This module, alongside the message queue that receives messages from all local storage databases, are responsible for directing the data to the correct place and retrieving it upon request. We provide a centralized data approach with two-level database architecture. The first level is the cluster level, which enables low latency offloading when available. The second level is the global level, which fosters a large centralized data storage that local cluster databases can be synchronized to on a fixed schedule using message queues. This offers flexibility for on-premises data, thus increasing the level of data privacy for critical applications.
5.
Benchmark Manager: This module is responsible for collecting benchmarks about the available devices from active clusters. Later, it combines all the gathered results in a single list that can be passed to the service scheduler to calculate scores and make decisions. The benchmarks used in our system include the CPU usage, CPU count, CPU frequency, available memory, network bandwidth, the device’s read and write speeds over the network, and the round trip time (RTT).
6.
Scheduler: The scheduler is one of the core modules that maps services to devices within communities and clusters. The underlying scheme in the scheduler module, referred to as the Community-Oriented Resource Allocation (CORA) scheme, is discussed in detail in Section 5. Note that after acquiring the device benchmarks from the score manager, the scheduler needs to prioritize offloading services to clusters owned by the same entity, followed by clusters owned by members of the same community as the requester. The implemented scheduler is required to be online to allocate resources in real time. We use a time-driven approach with time cycles rather than an event-driven approach. This is because a time-driven approach has more potential in terms of optimization, and can thus help address the constraints imposed by communities more efficiently. This optimization potential can be attributed to the larger number of services and devices available for the scheduler at the time of allocation. The scheduler needs to achieve the best possible allocation of resources. In particular, the scheduler strives to minimize the makespan and execution time, while being fast enough to run within the cycle time frame. The time of each cycle can be configured depending on the application type and the number of concurrent users.
7.
Notification Handler: Some services last for a long time. Thus, relying on requests’ responses for the results can result in timeout exceptions. This justifies the need for a notification module. The service requesters need to be notified when their service is terminated, whether it fails or succeeds. The notification handler module passes the final notification from the worker to the cluster head with the required information to deliver to the requester. In addition to notification passing, the handler receives periodic notifications from running clusters to ensure that the services are still in progress, notify the scheduler of any network failure, and take the necessary actions.
8.
Estimator: Utilizing a combination of analytic benchmarking and statistical prediction, we make use of historical knowledge from prior executions of the same services to estimate the execution time on a machine with different benchmarks, either due to machine variation or device usage during offloading [41]. To improve estimation accuracy, we employ a two-stage machine learning approach that includes a random forest model [42]. A few adaptations are applied to fit this model into our case. For example, because we have no control over the containers offloaded to embed a tracking code, we replace the resource tracking built into services with docker monitoring technologies. We also create models for new services once they are introduced to the system and automatically select training variables by analyzing the service parameters.
3.3. System flow
Fig. 8 illustrates the flow throughout the system in CEP. The process begins with the initiation of a cluster, a prerequisite for service requests to be processed (1-Initiate Cluster). When a service request is issued, it is directed to the cluster head (2-Request Job), which can either handle the request on a cluster level if possible or offload it to the global system. In the global system, the user authentication module verifies the cluster owner’s identity and retrieves a list of the owner’s communities. Following this, during the scheduling cycle, the scheduler module queries the benchmark manager, which maintains benchmarks for all active cluster workers, accessed via the cluster manager, and compiles the benchmark data into a comprehensive list. This list, along with variables related to the services queued for allocation, is transmitted to the estimator module. The latter calculates the execution time estimations and forwards them to the scheduler to make a decision (3-Get Estimations). Finally, the scheduler makes resource allocation decisions and passes the service information to the cluster head of the selected device (4-Assign Job).
When the cluster head receives the service request, it passes it to the selected worker device, which in turn gets the service image from the Docker repository (5-Get Image), downloads any required data (6-Get Data), and starts processing. After processing is completed, the results, if any, are uploaded to the centralized data storage (7-Save Results), and a notification is sent to the cluster head with the state in which the service finished processing (8-Notify Server). The cluster head then forwards this notification to the notification handler, where it gets passed to the original requester (9-Notify Requester).
During the aforementioned procedure, resource allocation is performed using a time-driven scheduler that executes once every cycle of a specified amount of time. Each cycle has four events that run in order; update benchmarks, allocate jobs, check assignment, and update cluster state. During any time in the cycle, the requester can send service requests, which are queued in a service queue waiting to be scheduled. Starting at the ‘update benchmarks’ event, all clusters are required to send the updated benchmarks of all the available workers to the benchmark manager at the server. After a configurable amount of time, the server triggers the scheduler to make the resource allocation decision. At this point, the services are pulled from the queue and allocated to the available workers. Following this step, the cluster heads check for any service assigned to their workers. Workers are expected to start executing the services as soon as the cluster head distributes them to the corresponding workers. The following step in the cycle is triggered at the server within the cluster manager. In this step, the cluster states are updated. Finally, the cycle goes back to the update benchmarks event.
4. Qualitative evaluation of CEP
We conduct a qualitative evaluation of CEP by performing a comparative study to highlight and illustrate the differences between CEP and 12 prominent edge computing systems, namely, Akraino [14], EdgeX Foundry [19], HomeEdge [12], Azure Edge [20], Apache Edgent [21], Kubernetes [22], AWS Greengrass [23], Mobiledgex [16], Mutable [15], Golem [24], OTOY [26], and iExec [25]. Our qualitative evaluation is primarily derived from existing literature, white papers, and official documentation of the platforms. This method is standard practice for qualitative assessments of platforms, focusing on architectural, deployment, and performance-related features rather than direct data collection. This approach enables a systematic assessment of each platform’s strengths and limitations. The evaluation is grounded in a thorough analysis of the platforms, their architecture, design, implementation, formats, and deployment methods, where all this information is collected from platform-specific documentation, technical reports, academic publications, and white papers produced by the developers of each platform [12], [14], [15], [16], [19], [20], [21], [22], [23], [24], [25], [26]. This provides a consistent basis for comparison and ensures that our analysis is based on credible and reliable information.
We compare the platforms based on 13 features grouped into two categories: (1) System architecture, deployment, and application features, and (2) performance features. The 13 features of such categories are analyzed below under their corresponding category.
Table 1. Comparison of edge computing platforms - Architecture, deployment, and application features.

System	Deployed At	Worker ownership	Control	OS support	Network coverage	Workload format	Application area
Akraino [14]	Cell towers	Network Providers	Hierarchical	Linux	Unrestricted	VM, Container, Bare Metal	Unrestricted
EdgeX Foundry [19]	Gateway	Requester	Centralized	Various OS	Local Network	Predefined Commands	IoT
HomeEdge [12]	EEDs	Requester	Decentralized	Various OS	Local Network	Container	Unrestricted
Azure Edge [20]	Gateway	Requester	Centralized	Various OS	Unrestricted	Restricted Container	Unrestricted
Apache Edgent [21]	Gateway	Requester	Centralized	Various OS	Local Network	Java Application	IoT
Kubernetes [22]	EEDs and Server	Requester	Centralized	Various OS	Unrestricted	Container	Unrestricted
AWS Greengrass [23]	EEDs and Cloud	Requester	Hierarchical	Various OS	Unrestricted	AWS Lambda Function	Unrestricted
Mobiledgex [16]	Data centers	Cloud Providers	Centralized	Various OS	Unrestricted	Ubiquitous computing	Unrestricted
Mutable [15]	Network operators’ servers	Network Providers	Centralized	Mutable OS	Unrestricted	Container	Various
Golem [24]	Cloud	Users	Decentralized	Ubuntu	Unrestricted	Restricted Container	Unrestricted
OTOY [26]	Cloud	Users	Centralized	Various OS	Unrestricted	Render Token	Rendering
iExec [25]	EEDs and Cloud	Users	Decentralized	Various OS	Unrestricted	Restricted Container	Tasks
CEP	EEDs and Server	Community	Hierarchical	Various OS	Unrestricted	Container	Unrestricted
4.1. System architecture, deployment, and application features
As depicted in Table 1, we compare the aforementioned systems based on 7 architecture, deployment, and application features. These features are the deployment location, worker ownership, control paradigm, worker OS support, network coverage, workload format, and application areas. Such features are analyzed as follows:
1.
Deployed At: The deployment location shows the type of devices suitable for the system. This feature clarifies where the controller is deployed. Deployment location is a key factor that determines the suitability of a system for certain applications, depending on the user’s access to such devices. Akraino edge can be deployed at cell towers, central offices, and other service providers’ locations, such as wire centers. Edgex Foundry, Apache Edgent, and Azure IoT Edge must be deployed at edge nodes such as gateways, hubs, and routers. HomeEdge is deployed on EEDs that need to run the system on the local network. Kubernetes has a kubelet on each node, combined with a Kube controller manager on one device that acts as an entry point to distribute workload, and gives the user access to monitor and manage nodes. This controller can be on-site or at a remote location, such as a cloud. AWS IoT Greengrass requires a combination of a controller on one device, acting as the core device, and AWS services running on the cloud to monitor, manage, and deploy new applications. Mobiledgex aims to deploy its system in data centers, commodity clouds, and multi-access edge computing (MEC) locations. Mutable deploys its worker manager on network operators’ servers. Golem and OTOY maintain their core controller in the cloud, where all workers have to communicate with it directly. iExec allows users to operate their machines as workers’ pool managers where they schedule work for other workers, while the main server is used as a manager for pool managers, applications, and data providers. In contrast to those systems, CEP has two controllers; the first one, which is deployed at the cluster head, manages resource allocation within the cluster and handles communication with the second controller. The latter is deployed on a separate server that is accessible by all clusters. This deployment location gives users control over the system instead of relying on a provider, which makes CEP much more flexible.
2.
Worker Ownership: The workers that are responsible for executing the offloaded computation task belong to different beneficiaries in each system. Some systems limit the workers to devices owned or managed by the requester. These systems are EdgeX Foundry, HomeEdge, Azure IoT Edge, Apache Edgent, Kubernetes, and AWS Greengrass. Such systems are expensive, since all workers must be owned or rented by the requesters. Moreover, limiting the resources to devices on the same network or those owned by the same user hinders the system’s reliability and resource utilization potential, especially when involving inconsistent workloads. Other systems deviate from this pattern, including Akriano Edge Stack and Mutable, where they utilize network provider-owned servers to perform the computation. Mobiledgex allows specific kinds of entities to provide workers, namely, cloud service providers or network operators, where offloaded tasks are executed on data centers, clouds, and edge servers. Such systems lead to a monopoly by cloud service providers and/or network operators and do not facilitate democratizing the edge. In addition, the density of available servers in the geographical location of end users and how close they are compared to cloud servers can vary depending on the region and time, which severely limits the range and scalability of such platforms [5]. Golem, OTOY, and iExec opt for an open public approach. The workers in those systems are EEDs owned by any user willing to share their computation resources. This breaks the aforementioned monopoly. However, privacy concerns and high recruitment costs can cause requesters to be reluctant to subscribe to such systems. In CEP, these issues are resolved by restricting the workers available for each requester to any EED owned by a member of one of the requester’s communities.
3.
Control: Three main control paradigms are used across existing systems; centralized, decentralized, and hierarchical (i.e., multi-level control). In centralized systems, one server acts as the sole source of resource allocation decisions. EdgeX Foundry, Azure IoT Edge, Apache Edgent, Kubernetes, Mobiledgex, Mutable, and OTOY all follow this approach, which is a simple yet effective way of managing a large number of devices. Decentralized systems allow each requester to choose the workers that are suitable for their needs. In HomeEdge, each device can request benchmarks from all devices on the local network and decide which device is the most suitable for the workload. This process has high reliability since no machine acts as a central point of failure for the system. However, this process cannot be scaled to a large number of machines. Golem and iExec follow an open market model where requesters and workers are free to present their offers or accept offers already extended. This approach allows the market price to adjust depending on supply and demand, removing the burden of handling resource allocation from the system maintainers. However, this approach tends to require more actions from users, and may not lead to optimal resource allocations. In addition, user intervention is needed in case of workers’ failure or migration. In contrast, hierarchical control allows the system to fully leverage the advantages of edge computing by minimizing the amount of data transmitted over the network, allowing for faster decision-making and computation in latency-critical applications. Systems that follow this approach are Akraino, AWS IoT Greengrass, and CEP. Akraino has one controller on each edge site, and all controllers at one level are connected to a higher-level controller. AWS IoT Greengrass relies on the core device as a management point before the cloud. CEP operates a full controller and scheduler within each cluster, and it only falls back to the central scheduler in cases where no suitable worker is available in the requester’s cluster.
4.
OS Support: Akraino is limited to Linux, which is reasonable since most servers are Linux-based, and thus there is no need to support other operating systems. Golem only supports Ubuntu. However, there is a plan to extend its support in the future. Mutable depends on its own OS, called Mutable OS, which is built on the NixOS open-source Linux-based operating system with built-in infrastructure-specific advantages. In contrast, EdgeX, HomeEdge, AzureEdge, Edgent, Kubernetes, AWS Greengrass, Mobiledgex, OTOY, iExec, and CEP support various OS on edge workers, including Linux, Windows, and Mac OS. This enables them to be more widely-adopted.
5.
Network Coverage: Network coverage is a key factor when deciding which system is suitable for a specific use case. A few systems are limited to a local network where all communications and logic reside. Such systems include EdgeX Foundry, HomeEdge, and Apache Edgent. All other systems can be extended and linked to different requesters and workers outside the local network. EdgeX Foundry, HomeEdge, and Apache Edgent can be extended as well but they require some programming or workarounds to reach this level, which is already provided in other systems by default.
6.
Workload Format: The workload can act as a barrier for systems that hinder attracting new users, depending on the workload that the requesters want to offload and whether they are willing to rewrite the workload in a different format or not. Akraino Edge Stack fosters the three most common workload formats by providing bare metal machines, virtual machines (VMs), and containers. Edgex Foundry is limited to a predefined set of commands and functions defined by each device, and it lacks the ability to execute custom operations or code provided by the requester. OTOY is limited to render tokens that are used for rendering images or videos. Apache Edgent can only run applications written in JAVA and compatible with a Java Virtual Machine (JVM). AWS Greengrass allows any AWS lambda function to be sent as a workload. Consequently, the user code needs to be rewritten as an AWS lambda function first. Mobiledgex currently lacks a defined workload method, but it emphasizes the importance of ubiquitous computing. This approach enables developers to write their code once, allowing it to run on various machines and in different environments. Golem, iExec, and Azure IoT Edge allow the use of containers. However, they impose some restrictions on those containers, such as a specific SDK or package to use or a template to follow so that the workload can interact with the system correctly. In contrast, HomeEdge, Kubernetes, CEP, and Mutable enable the use of containers without requiring any modifications. This allows users to easily package their code inside a container, leveraging millions of pre-made containers available online as is or customizing them to perform new tasks.
7.
Application Area: Apache Edgent and EdgeX Foundry both prioritize IoT edge. EdgeX Foundry is tailored for communication with various sensors and devices, while Edgent is optimized for data analysis. These systems are suitable for intelligent transportation, intelligent manufacturing, and smart city applications, where various sensors and devices generate an enormous amount of data, and not all of this data is required to be sent to the cloud. Azure IoT Edge and AWS IoT Greengrass can be considered as an extension of Azure Cloud and AWS Cloud, respectively. They share extensive application areas but are limited by the computation resources of edge devices. Furthermore, they make it significantly more convenient to deploy edge applications, such as Machine Learning (ML) and image recognition to edge devices with the help of Azure and AWS services. OTOY is limited to rendering, due to its workload format. Mutable indicates a large number of application areas on their site, such as cloud gaming, autonomous vehicles, IoT, smart cities, drones, data processing, video conferencing, video and audio processing, Domain Name System (DNS), and networking. However, Mutable does not clearly state that it can be used for any application. Thus, it is not unrestricted. iExec states that their system is currently limited to on-off tasks, not long-running services. In contrast, Akraino, HomeEdge, Kubernetes, CEP, Mobiledgex, and Golem impose no restrictions on the kind of applications that can run on them.
4.2. Performance features
As depicted in Table 2, we compare the systems based on the following 6 performance features: data privacy, worker security, deployment, scalability, mobility, and cost. Such features are analyzed as follows:
Table 2. Comparison of edge computing platforms - Performance features.

System	Data privacy	Worker Security	Deployment	Scalability	Mobility	Cost
Akraino [14]	Shared with network provider	Run untrusted code	Static	Poor	Good	N/A
EdgeX Foundry [19]	Shared with cloud	Requester verified	Dynamic	Poor	Not supported	Cloud storage
HomeEdge [12]	On-premises	Requester verified	Dynamic	Not Scalable	Not supported	Cloud storage
Azure Edge [20]	Shared with cloud	Requester verified	Dynamic	Moderate	Poor	Azure Services
Apache Edgent [21]	Shared with cloud	Requester verified	Dynamic	Poor	Poor	Cloud Rent
Kubernetes [22]	Private	Requester verified	Dynamic	Good	Moderate	Rent/Maintan
AWS Greengrass [23]	Shared with cloud	Requester verified	Dynamic	Moderate	Poor	AWS Services
Mobiledgex [16]	Shared with cloud	Verified applications	Static	Poor	Good	Rent
Mutable [15]	Shared with network provider	Verified applications	Static	Poor	Poor	Rent
Golem [24]	Shared	Run untrusted code	Dynamic	Good	Poor	Rent
OTOY [26]	Shared	Limited Functionality	Dynamic	Good	Moderate	Usage
iExec [25]	Shared	Run untrusted code	Dynamic	Good	Moderate	Usage
CEP	Shared with Community	Community verified	Dynamic	Good	Moderate	N/A
1.
Data Privacy: Data privacy is a major deciding factor for requesters who deal with sensitive data. Note that the data shared with another party does not automatically mean it is easy to access, but it can be vulnerable to a broader range of security attacks. In contrast, systems that keep data on-premises reduce the possibility of data transmission attacks over the network, making it even more secure. Akraino Edge Stack and Mutable execute the workload on network providers’ machines. Hence, the data is available to the network provider. EdgeX Foundry typically involves offloading the data to a server afterward, and this server can be a public cloud. However, the data can be kept on-premises or can be offloaded to a private cloud, which increases the privacy but significantly increases the cost. Apache Edgent continues to process data at the cloud, which increases the risk of a data breach. Azure IoT Edge and AWS IoT Greengrass utilize cloud services at the edge and at the cloud, thus making the data available to cloud providers. HomeEdge is a Local Area Network (LAN) system, and thus the data is kept on-premises. Kubernetes machines are managed by the requester. Hence, the data is kept private from other parties unless those machines are rented from a cloud provider for example. Mobiledgex runs the system on private data centers and commodity clouds. Despite utilizing Accedian security protocols, which ensure a high level of security, the possibility of data attacks still exists due to the nature of the worker devices. Golem, OTOY, and iExec share the data with devices owned by end users, making them the least secure option. In contrast, despite CEP sharing data with workers, it ensures that the data is never sent to someone outside the requester’s community. Hence, CEP adds an additional level of privacy that can be controlled by the requesters themselves by defining and verifying their own communities.
2.
Worker Security: Data privacy and worker security are two sides of the same coin. On one side, there is the risk that the requesters are taking by offloading the workload to another device. On the other side, there is the risk that the workers are exposed to when running other people’s code on their machines. Most systems rely on virtualization technology as a baseline for worker security by isolating the offloaded workload in a separate environment [2]. However, there is a trade-off between granting the requesters more options in terms of the types of workloads that can be offloaded and isolating the execution of the code. Akraino does not take security precautions. This could be because it creates and maintains new machines specifically for edge workloads, which is similar to how cloud providers operate but at the edge of the network. Hence, there is no personal or organizational data or code running on those machines. EdgeX Foundry, HomeEdge, Azure IoT Edge, Apache Edgent, Kubernetes, and AWS Greengrass run the requester’s code on their own machines and assume that the requester has already verified the code before executing it. Consequently, there is no need for extreme security measures. EdgeX Foundry is limited to predefined commands, and thus running custom code is not an option in the first place. Mobiledgex and Mutable require application verification before they can be executed on workers, which shifts the responsibility to the verifying entity rather than requesters or workers. Golem runs untrusted custom code. However, interacting with system resources is managed through their SDK, making it harder to work around. OTOY limits the workload to render tokens, thus significantly reducing the risk of any possible security attacks. iExec does not take any additional precautions to secure the workers besides virtualization. In contrast, CEP relies on the mutual trust between the requesters and workers of the same community. Thus, there is no need for restrictive precautions, which in turn leads to more flexibility in workload options.
3.
Deployment: Deployment reflects how easy and quick the system can be deployed and extended for new workers. Akraino Edge Stack is primarily designed for network providers who may need to incorporate additional hardware, such as access devices and network cards, in addition to computing machines. Thus, adding new workers to this system can be a slow process that requires substantial human intervention. Mobiledgex and Mutable target provider-owned devices to serve as workers, which results in a more complex procedure for integrating new workers into the system. EdgeX Foundry, HomeEdge, and Kubernetes are designed to run on edge devices, allowing end users to deploy and modify the system themselves. For example, extending workers in HomeEdge can be as simple as running the HomeEdge application on a new machine connected to the same network. EdgeX Foundry requires API-based communication to define a new device profile. Kubernetes requires changes to the cluster manager’s machine, in addition to deploying the new worker node. Azure Edge and AWS Greengrass require users to utilize the cloud-based interface to develop and deploy their applications. In contrast, Golem, OTOY, iExec, and CEP allow workers to participate in the system by deploying a worker instance on their machine and logging into their account, making the process relatively straightforward.
4.
Scalability: This feature indicates how well and easy is it to scale the system to serve a larger number of connected users. Akraino Edge Stack can theoretically be scaled to any size. However, it requires hardware modifications and deployment of new controllers on network centers, which is a slow process. HomeEdge is limited to LANs, and is decentralized, rendering it not scalable beyond a few devices. EdgeX Foundry and Apache Edgent are limited to LANs as well, but they can be extended to a larger number of users due to their central nature of control. It is worth mentioning that EdgeX Foundry’s deployment is scalable since it consists of multiple microservices, which the users can dynamically add or remove to adapt to their needs. Mobiledgex and Mutable require negotiating with new providers before scaling the number of workers, thus leading to poor scalability. Azure Edge and AWS Greengrass are connected to the cloud. Thus, they are highly scalable. However, offloading tasks between different groups of devices is somewhat challenging. In contrast, Kubernetes, Golem, OTOY, iExec, and CEP can easily scale to accommodate new requesters and workers, and communication between users anywhere on the network is built into those systems by default.
5.
Mobility: Mobility measures how well a workload can be migrated or restarted on a new device. Akranio and Mobiledgex support migration of workload from one worker to another easily, due to the residence of workers on the network provider’s infrastructure and being built with mobility in mind. Kubernetes, OTOY, iExec, and CEP, cannot migrate a workload once it starts. However, they can restart the workload on a different machine without user intervention in case of failure. Azure Edge, Apache Edgent, AWS Greengrass, Mutable, and Golem have some recovery mechanisms and failure handling that require user intervention. EdgeX Foundry and HomeEdge do not provide a process for failure recovery. Instead, they require user intervention to handle failures and redeploy the workload by hand.
6.
Cost: This reflects the cost that must be paid in exchange for the resources used to execute the offloaded jobs. This is another major deciding factor for users when choosing which platform to use. HomeEdge depends on smart home devices owned and maintained by the requester. Although it can be argued that this system utilizes already existing resources, it is not guaranteed that the user has enough underutilized computational resources to execute all the workloads. In that case, the user would need to purchase more devices for computational power. This is in addition to the cost of maintaining the existing devices. Kubernetes can connect devices across different sites, including the cloud. Thus, those machines can be built and maintained by the requester or rented from a cloud provider, reducing the cost in case of small workloads. Azure IoT Edge and AWS IoT Greengrass require requesters to pay for Azure services and AWS services, respectively. EdgeX Foundry and Apache Edgent perform some processing at the edge but later perform further analysis or store the data in the cloud, which triggers significant costs. Currently, Mutable and Mobiledgex do not provide a clear pricing model. However, they both promise revenues for the workers’ owners: data center owners in Mobiledgex and network operators in Mutable. Hence, it can be safely assumed that they will charge renting costs from the requesters. Meanwhile, Akraino Edge Stack does not mention its pricing or promise profits to the network providers, and thus the same assumption cannot be made. Golem allows requesters to rent computation power from workers to execute their code. OTOY and iExec use the pay-per-task model, where requesters pay for the number of render tokens in OTOY and per-task execution in iExec. In contrast, CEP utilizes the concept of communities and service-for-service exchange to use the existing computation power available within communities to execute the workload.
Analyzing and comparing platform features is essential for evaluating the validity and efficacy of various distributed platforms within specific applications. For example, features like deployment methods, worker ownership models, workload format, and application area, can significantly influence the suitability of a platform for particular use cases, potentially rendering some options ineffective. Moreover, features such as cost considerations and mobility are crucial for enhancing end-user experience and system accessibility. The selection of an appropriate distributed scheduling platform involves balancing the interests of multiple stakeholders, including the platform host, edge device owners, and the clients requesting computational resources. Diverse configurations can work for different scenarios highlighting CEP capabilities to address community-based environments effectively and innovatively. Based on the aforementioned qualitative evaluation, it is evident that CEP offers superior architecture and deployment features, particularly regarding deployment location and the flexibility of worker ownership. Moreover, CEP is compatible with multiple operating systems and is capable of covering wide area networks. Its control paradigm enables CEP to fully capitalize on the benefits of edge computing. In terms of application features, CEP eliminates extra costs and imposes no limitations on the application area or workload format. Additionally, CEP guarantees a high level of privacy, security, scalability, and deployment flexibility.
5. Community-Oriented Resource Allocation (CORA)
In this section, we discuss the Community-Oriented Resource Allocation (CORA) scheme, which acts as the scheduler module in CEP. We provide a detailed presentation of how CORA is designed to enable the realization of community-based edge computing, where communities are facilitated via the community manager. Note that a detailed description of CORA can be found in [40]. The notion of communities in CEP adds another dimension to resource allocation, which is overlooked by existing schemes. This is since such schemes are not designed for community-driven scenarios, and thus they overlook the restrictions imposed by communities. The main challenge that stems from such imposed restrictions is unbalanced work distribution. Fig. 9 depicts an illustrative scenario that demonstrates this problem. In the depicted scenario, consider the case where the requester of Job 1 belongs to Community X and the requester of Job 2 belongs to Community Y. In addition, Worker A belongs to Community X, whereas Worker B belongs to both Community X and Community Y. The time it takes to execute Job 2 on Worker B is 65 s, and the time to execute Job 1 on Worker A and Worker B is 60 s and 50 s, respectively. If the scheduler uses one of the existing resource allocation schemes, such as Min-min [36], Job 1 will be assigned to Worker B because it is 10 s faster than Worker A. The scheduler will then be forced to allocate Job 2 to worker B. This is due to the imposed constraint prohibiting Job 2 from being allocated to Worker A since Worker A and the requester of Job 2 do not share the same community. Allocating both jobs to Worker B achieves a better total execution time (115 s), which is desirable. However, it results in a significantly high makespan (115 s), because all jobs are allocated to the same worker. In contrast, a better allocation would be Job 1 to Worker A and Job 2 to Worker B. This results in a slightly higher execution time sum of 125 s. However, it achieves a significant improvement in the maximum time it takes for all jobs to finish executing (i.e., makespan), reducing it to only 65 s.

Download: Download high-res image (115KB)
Download: Download full-size image
Fig. 9. An illustrative scenario of resource allocation in communities.

In order to address the aforementioned problem, it is imperative to have a resource allocation scheme that is tailored for community-driven scenarios. In this context, we propose the Community-Oriented Resource Allocation (CORA) scheme. In this section, we provide a description of CORA2 and the underlying method used to address the issue of imposed restrictions.
1- Graph Representation of Resource Allocation in CORA
In order to address the imposed restrictions issue caused by communities, CORA acknowledges the need to first determine the set of workers that are eligible to execute each service. This can be done by checking for union values between a service set of communities and a worker set of communities. This problem can be better represented by a graph of vertices, which is the union of the set of services 
 and the set of workers 
. This graph can be divided into two groups, one for services and one for workers. The vertices from the first group (i.e., services) can only have an edge with vertices from the second group (i.e., workers). Thus, the graph can be defined as a bipartite graph.
The Expected Time to Compute (ETC) is a function that can be used to calculate the estimated time to execute service 
 on worker 
. A two-stage machine learning approach is used for this estimation, as discussed in Section 3.2. Using this function, we construct the Edges matrix, denoted 
, which is an 
 matrix in which 
 is the number of services and 
 is the number of workers. Each entry 
 in the matrix represents the estimated execution time of service 
 on worker 
, as given by Eq. (1).
(1)
In the constructed bipartite graph, an edge between two vertices exists only if they share at least one community, as reflected by Eq. (2), where 
 is a set of all valid edges that the scheduler can choose from during the resource allocation process. For each service 
 and each worker 
, 
 is the estimated execution time of 
 on 
. We define 
 as the set of communities for the requester of service 
 and 
 as the set of communities for worker 
. Each edge in the graph connects one service to one worker, where the intersection between 
 and 
 is not empty, and the edge weight is the corresponding value extracted from the ETC function.
(2)
We define 
 as the set of services assigned to worker 
 by the scheduler. Note that the time it takes worker 
 to complete all assigned services is denoted 
, and is given by Eq. (3).
(3)
CORA strives to minimize the makespan and average execution time. The average execution time is the average of the execution time of all services on their selected workers, as given by Eq. (4).
(4)
The makespan is the time needed for the system to finish executing the last service [37], which can be calculated as the longest time that any worker in the system takes to finish its assigned services. Thus, it is important to minimize this number to ensure that the average user service is completed in a timely manner. It is noteworthy that the makespan can be heavily impacted by communities if a generic resource allocation scheme that is not specifically designed for communities is used. This is since it can pile workload on a few workers, due to their available resources, before realizing that those workers are the only available option for the unallocated services.
2- Solution of the Bipartite Graph Matching Problem
Formulating the problem as a bipartite graph matching renders the Munkres algorithm [43] a suitable solution. The Munkres algorithm, also known as the Hungarian algorithm, is a combinatorial optimization algorithm capable of solving the classical bipartite graph matching, which is the assignment problem, in polynomial time, specifically with time complexity of 
. However, in contrast to classical bipartite graph matching, where every vertex from group A is matched with a single vertex from group B, this is not always the optimal case for resource allocation. This is since multiple services can be assigned to the same worker within the same cycle. On top of that, the Munkres algorithm is relatively slow, due to its multiple steps and calculations.
To address the aforementioned issues, we take the problem a step back to graph matching, better known as the maximum flow algorithm [44]. More specifically, we consider the multi-source multi-sink variation of the problem, where we add an imaginary source that connects to all the sources and an imaginary sink that connect to all sinks. To apply this to bipartite graph matching, we can set the capacity of those new edges to one, limiting the flow to one per source and one per sink (i.e., service). In CORA, we want the graph to match the sinks (services) once, in order to avoid assigning redundant work to workers. However, we strive to allow workers to have multiple services. Thus, we introduce our first parameter 
, which allows the user to set the capacity of the number of services assigned per worker. By default, this can be set to the number of services. Thus, any number of services can be assigned to the same worker if needed.
The maximum flow algorithm with the worker capacity set to one can maximize the number of matches between workers and services, overriding previous matches if services assigned so far result in a dead-end, where some services are left unmatched. Hence, the maximum flow algorithm can result in the maximum possible number of matching, but they cannot be guaranteed to be the best matches. Replacing the pathfinding portion of the maximum flow algorithm with the shortest path alternative, such as the Bellman-Ford algorithm, elevates it to the minimum cost maximum flow (MCMF) algorithm, which guarantees matches that result in the lowest cost (i.e., service execution time). However, with the cap on source edges at the number of services, MCMF can assign all services to one worker. The first approach of bipartite graph matching tends to optimize the makespan, while MCMF optimizes the execution time. CORA bridges the gap between the two approaches. Whenever a service 
 is matched with a worker 
, we change the cost on the edge between the added “super-source” and 
, denoted 
, from zero to the sum of the edge weights of all the services assigned to this worker, multiplied by 
, as given by Eq. (5), where 
 is a tuneable parameter.
(5)
Algorithm 1 illustrates the resource allocation procedure in CORA. The number of all vertices in the extended MCMF graph is denoted 
. The set of all vertices includes services, workers, a super source, and a super sink. The source and sink vertices are assigned the index 
 (i.e., the first element in the set) and 
 (i.e., the last element in the set), respectively (lines 13–15). A capacity matrix, denoted 
, is created for the max flow algorithm. This matrix indicates the available capacity on each edge in the graph. Hence, its size is 
, where the element 
 indicates the capacity of the directed edge that connects vertex 
 with vertex 
. The values for the capacity matrix are set to 1 for all edges between the super source and all service vertices, in addition to any valid edge between a service and a worker that shares a community, based on Eq. (2). Other values in the matrix are set to 
 in the case of edges between the worker vertices and the sink, or 0 if none of the previous conditions applies (line 16). Another matrix of the same size is required for MCMF, denoted 
. Each element in this matrix is the edge cost that the algorithm is trying to minimize. For any valid edge between a service vertex and a worker vertex, the value of the cost matrix is set to the estimated execution time of that service on that worker. The reverse edge between the worker and the service is given the cost of the negative value of the estimated execution time. These reverse edges allow the algorithm to backtrack previously allocated resources, and reallocate them if a better matching is found. The remaining cost matrix values are set to zero (line 17).

Download: Download high-res image (314KB)
Download: Download full-size image
As long as paths from the source to the sink exist, the following steps are iteratively repeated (line 18): First, a list of all current possible edges is calculated based on the capacity matrix (line 19). Second, the Bellman-Ford algorithm is executed, given the list of edges, the source, and the sink vertices, to find the current shortest path from the source to the sink. The Bellman-Ford algorithm returns an array of distances from the source to all reachable vertices, in addition to the shortest path from the source to the sink, represented as an array of vertex indices. The sink vertex is unreachable from the source vertex when the distance to the sink is 
 or more, in which case we break from the main loop and return the services allocated so far (lines 20–22). Third, the last index in the path array is set to a variable denoted 
, while a variable, denoted 
 is set to the lowest capacity on the found path (i.e., path bottleneck) (lines 23 & 24). Fourth, we loop over the shortest path found, reducing the capacity of each edge along the path by 
, and increasing the capacity of each reverse edge by the same amount. This is done to allow the backtracking of this assignment later if a better allocation can be made. This backtracking can be achieved due to the combination of reverse edges in capacity, and negative costs for the reverse edges. The cost of edges connecting the workers to the sink vertex is updated using the stacking penalty parameter 
. This update works for forward and reverses edges to adjust the current cost of allocating another service to this worker, since any allocation to the worker has to pass through the edge connecting the worker vertex to the sink (lines 25–28). Any reverse capacity of 
 means that this service is allocated to this worker, since the reverse capacity is added after the allocation (line 29). A set of allocation sets is then returned, which can be used to pass service information to the assigned workers (lines 32 & 33).
6. Performance evaluation
In this section, we evaluate the scheduler and community manager in CEP by evaluating the performance of CORA compared to six prominent heuristic-based resource allocation schemes that are tweaked to fit into CEP and community-oriented EED-based computing environments. These schemes are WorkQueue [22], Min-min [36], Max–min [36], LJFR-SJFR [37], and Sufferage [37]. In addition, we compare CORA to the Munkres algorithm [43]. We use the following performance metrics: (1) makespan, which is the longest time taken by workers to finish their assigned services, (2) the average execution time, which is calculated as given by Eq. (4), and (3) the scheduler runtime, which is the time it takes the resource allocation module to allocate all services to the available devices.
Table 3. Makespan and execution time (in Seconds) of CORA, Min-min, Max–min, LJFR_SJFR, WorkQueue, Sufferage, and Munkres for 600 Services.

Instance	Min-min	Max–min	LJFR_SJFR	WorkQueue	Sufferage	CORA	Munkres
Empty Cell	ms.	et.	ms.	et.	ms.	et.	ms.	et.	ms.	et.	ms.	et.	ms.	et.
lo-lo-c-s	936	505	922	514	902	511	995	529	2663	501	691	503	691	503
lo-lo-c-d	892	505	884	513	799	511	965	531	1883	502	692	502	692	502
lo-lo-i-s	1092	528	1066	539	1065	537	1218	600	1471	527	589	523	589	523
lo-lo-i-d	1054	523	1034	531	1035	528	1180	599	1103	520	563	517	563	517
lo-hi-c-s	959	510	943	520	877	517	1047	546	1997	504	697	505	697	505
lo-hi-c-d	918	508	896	519	799	515	936	548	1519	505	696	503	696	503
lo-hi-i-s	1106	534	1073	545	1083	543	1268	617	1381	532	616	529	616	529
lo-hi-i-d	1081	529	1046	535	1053	534	1205	616	1187	524	585	522	585	522
hi-lo-c-s	975	510	955	522	893	518	1021	545	2128	507	699	505	699	505
hi-lo-c-d	906	508	897	521	801	515	932	547	2160	505	691	503	691	503
hi-lo-i-s	1095	534	1071	553	1067	546	1248	615	1689	535	614	530	614	530
hi-lo-i-d	1049	529	1053	544	1043	538	1186	616	1199	527	583	522	583	522
hi-hi-c-s	1736	607	1517	697	1545	661	8840	2109	3384	619	1289	623	1380	639
hi-hi-c-d	1416	574	1259	646	1314	621	9022	2162	2192	587	997	579	1013	582
hi-hi-i-s	1906	679	1640	767	1668	744	9060	2224	2774	696	1419	690	1514	699
hi-hi-i-d	1586	638	1368	710	1403	698	9250	2229	1262	651	1066	637	1057	638
Mean	1169	545	1102	573	1084	565	3086	977	1875	546	780	543	793	545
The instance format is ww-xx-y-z, where ww is service heterogeneity (high/low), xx is worker heterogeneity (high/low), y is matrix consistency (consistent/inconsistent), and z is community density(sparse/dense). The notation ms. stands for makespan while et. indicates the execution time column under each approach.
6.1. Simulation setup
Unless otherwise specified, the number of services and workers is set to 600 each, and the tuneable parameter 
 is set to 1.5. The parameter 
 is set to the number of services. We generate 16 classes of instances by varying the following 4 parameters as follows:
•
Worker heterogeneity: This represents the variance among the execution time of a given service across all workers. We set this value to 1 for low worker heterogeneity and 50 for high worker heterogeneity.
•
Service heterogeneity: This parameter describes the possible variation among the execution time of all services on a given worker, where a low variance indicates that a given worker can run all services with a small execution time gap, while a high variance indicates a case of vastly dissimilar services. We set this value to 1 and 50 for low and high service heterogeneity, respectively. The execution time of each service on each worker is uniformly distributed in the range of [500, 700] for low service heterogeneity and in the range of [500, 10000] for high service heterogeneity.
•
Execution time consistency (
 matrix consistency): This parameter can have one of two possible values; consistent or inconsistent. In a consistent 
 matrix, worker 
 executes all services faster than worker 
. In contrast, an inconsistent matrix characterizes the case where worker 
 is faster than worker 
 for some services and slower for others.
•
Community density: To vary the number of valid communities that each service can be allocated to, we vary the number of different communities that each device is a member of. This value is set to 3 for a sparse range of communities and 5 for a dense range of communities. Note that the total number of communities is set to one tenth of the number of services.
6.2. Results and analysis
To evaluate CORA in different scenarios, we assess its performance compared to WorkQueue, Min-min, Max–min, LJFR-SJFR, Sufferage, and Munkres over varying classes of instances, varying number of services, varying 
, and in the presence of only one community. This helps evaluate the effect of the scheduler and community manager on the overall performance in such scenarios. All experiments are repeated 10 times for each instance, and simulation results are presented at a confidence level of 95%. The yielded confidence intervals are depicted in Fig. 3(a) and (b). Since the confidence intervals are negligible, they are not explicitly shown in the other figures for clarity of presentation.
1- Impact of Instance Classes
Table 3 shows the makespan and execution time results obtained from running the scheduler evaluations for every instance. The mean value per approach across all instance classes is shown in the last row. The underlined bold value is the minimum yielded value of the makespan or execution time per instance.
Table 3 depicts the makespan and execution time of CORA, Min-min, Max–min, LJFR SJFR, WorkQueue, Sufferage, and Munkres for 600 Services. Note that the instance format is ww-xx-y-z, where ww is service heterogeneity (hi = high/lo = low), xx is worker heterogeneity (hi = high/lo = low), 
 is matrix consistency (c = consistent/i = inconsistent), and z is community density(s = sparse/d = dense). As depicted in Table 3, CORA and Munkres share the minimum makespan across all instances. This is due to their common nature of prioritizing distributing services over different workers. In addition, they both share the possibility of reallocating services to consider the allocation order, which prevents service stacking that significantly increases makespan. However, it is not always desirable to have one service per worker. This idea is highlighted by the instances with high service heterogeneity and worker heterogeneity alike. This can be attributed to the fact that high worker heterogeneity increases the possibility of having workers that are powerful enough to execute multiple services before other workers execute a single one. Additionally, high service heterogeneity can result in short services that can be stacked and completed before or close to relatively long services. This is reflected in instances with high service heterogeneity, as well as those with high worker heterogeneity, where it can be observed that CORA has the ability to adapt and achieve even lower makespan results. For example, in the hi-lo-i-d instance, CORA yields significant improvements, reaching 51.4%, 50.8%, 44%, 45%, and 44.4% makespan reduction compared to Suffrage, WorkQueue, LJFR SJFR, Max–min, and Min-min, respectively. Furthermore, in the hi-hi-c-d instance, CORA outperforms Suffrage, WorkQueue, LJFR SJFR, Max–min, and Min-min by 55%, 89%, 24%, 21%, and 30% in terms of makespan, respectively. Note that the instances where community density is set to dense tend to yield better results than the corresponding instances where communities are sparse, demonstrating the impact of communities with high density. This can be attributed to the higher pool of potential service-worker associations belonging to the same community that the scheduler can select from to minimize the makespan and execution time. On average, it can be observed that CORA achieves significant makespan reductions, reaching 58%, 75%, 28%, 29%, and 33% compared to Suffrage, WorkQueue, LJFR SJFR, Max–min, and Min-min, respectively. The Munkres algorithm is not considered because of its significantly long runtime (i.e., high complexity), which renders it impractical, whereas CORA’s runtime is up to six times faster than Munkres, which is discussed later in detail. This is in addition to the fact that Munkres lacks the ability to assign multiple services to a worker, which, as opposed to CORA, significantly reduces its potential for generalization.
As shown in Table 3, the execution time of most approaches, with the exception of WorkQueue, is observed to be relatively close to each other. However, CORA still outperforms all the other approaches due to the same aforementioned reasons. Moreover, CORA retains the minimum execution time for 11 out of 16 instances, which means that in those cases, it is the best option on the two fronts (i.e., makespan and execution time). For example, in the hi-hi-c-d instance, CORA reduces the execution time by 2%, 73%, 7%, 10%, and 1% compared to Suffrage, WorkQueue, LJFR SJFR, Max–min, and Min-min, respectively. On average, CORA significantly improves the execution time by 1%, 44%, 4%, 7%, and 0.4% compared to Suffrage, WorkQueue, LJFR SJFR, Max–min, and Min-min, respectively. Note that due to the default values of 
 and 
 used in the experiments, CORA prioritizes minimizing the makespan over the execution time. Other configurations for CORA that showcase better execution time results are discussed later in this section, where it is shown that CORA can reach up to 7% (compared to the aforementioned 0.4%) execution time reduction compared to the second-best heuristic scheme (Min-min), while maintaining the same makespan improvement. Based on the use case, CORA can be tailored to achieve a much better makespan with execution time similar to the other schemes, or it can prioritize faster execution time while keeping the makespan comparable to the other approaches. This adaptability is yet another significant advantage that CORA offers over competing approaches.

Download: Download high-res image (301KB)
Download: Download full-size image
Fig. 10. Performance results of CORA, Min-min, Max–min, LJFR_SJFR, Sufferage, and Munkres over varying number of services.

2- Impact of the Number of Services
Fig. 10(a) depicts the makespan over varying number of services. Note that as the number of services increases, the makespan increases slowly in WorkQueue, Min-min, LJFR_SJFR, Max–min, CORA, and Munkres. This can be attributed to the fact that makespan is capped at the maximum value of the minimum execution time of each service. There is no way to go lower than this cap, since it will take at least this time if this service is allocated to a worker with no other work alongside it. Evidently, the possibility of allocating multiple services to the same worker increases as the number of services increases, which increases the makespan. It can be observed that Sufferage witnesses a relatively large increase in makespan as the number of services increases. This can be attributed to the effect of the corresponding increase in the number of communities, which leads to a higher chance of isolated services with a limited number of available devices. Those isolated services tend to be a challenge for the Sufferage scheme due to the underlying process of selecting the next service to be allocated.
Fig. 10(b) depicts the effect of varying the number of services on the average execution time. It shows that the execution time remains almost the same across different number of services. This is since varying the number of services does not affect the average time a service takes to execute on any suitable worker. Hence, the same ratio of services that get allocated to less optimal workers remains the same, resulting in the same average execution time across all services within the corresponding instances.

Download: Download high-res image (253KB)
Download: Download full-size image
Fig. 11. Performance results of CORA, Min-min, Max–min, LJFR_SJFR, Sufferage, and Munkres over varying 
.

Fig. 10(c) depicts the runtime over varying number of services. Intuitively, the runtime increases as the number of services increases in all schemes. It can be observed that Munkres yields the longest runtime. This is since Munkres has a time complexity of 
, where n is the number of services and m is the number of workers. This high complexity is due to factors resulting from the multiple steps that Munkres has to go through before the assignment. In contrast, CORA does not encounter the same problem, achieving a runtime that is up to six times faster than Munkres. Note that WorkQueue is significantly faster than all the other approaches, due to its relatively low time complexity. This is since WorkQueue sacrifices optimality for speed and simplicity, where it has a complexity of 
. CORA is around 16% faster than Sufferage, while Min-min, LJFR_SJFR, and Max–min are around 40% faster than CORA in terms of the scheduler runtime. This can be attributed to the fact that Min-min, Max–min, and LJFR_SJFR schemes all share a complexity of 
, whereas Sufferage has a slightly higher complexity of 
, and CORA has a time complexity of 
, where E is the number of edges in the formulated graph, which can vary depending on the graph density.
3- Impact of the Tuneable Parameter (
)

Download: Download high-res image (208KB)
Download: Download full-size image
Fig. 12. Performance results of CORA, Min-min, Max–min, LJFR_SJFR, Sufferage, and Munkres with only one community.

In this experiment, we study the impact of the tuneable parameter 
, since it can significantly affect the execution of the allocated services in CORA. Fig. 11(a) demonstrates the effect of varying 
 on the makespan of CORA. Note that 
 has no impact on the other schemes, since it is only used in CORA. It can be observed that as 
 increases, the makespan of CORA decreases, reaching a decrease of around 24%. This is because as 
 increases, the scheduler increases the penalty for allocating multiple services to the same device by increasing the weight of the edge connecting the device node to the sink. The makespan at 
 is more than ten-fold the makespan at any other 
. Thus, it is omitted from the figure, since it is rendered impractical.
Fig. 11(b) depicts the effect of varying 
 on the average execution time of CORA. It can be observed that at 
, CORA can achieve a 10% lower execution time compared to 
. However, this configuration is useless in real-life scenarios due to its unreasonable makespan. Note that the allocation at 
 can be achieved by assigning all services to the device yielding the minimum execution time among all the devices, regardless of the other allocated services. This minimum execution time acts as the hard cap for the minimum execution time that can be achieved. As 
 increases, the average execution time increases, reaching an increase of up to 2% when 
 compared to 
. This increase can be attributed to the higher penalty for allocating multiple services to the same device. Note that a low penalty allows more services to be allocated to the device with less execution time for the given service regardless of the other allocated services. At 
, CORA achieves around 3% lower execution time than the second best heuristic scheme (Min-min), while maintaining a 12% lower makespan. CORA can reach up to 7% lower execution time compared to the second best heuristic scheme (Min-min), while maintaining a similar makespan. Depending on the use case, CORA can be optimized for a better makespan with execution time similar to the other approaches or a better execution time with a makespan on par with the other approaches. This flexibility is another strong advantage that CORA has over competing approaches.
4- Impact of One Community
CORA is optimized for an environment with multiple communities. However, it still provides acceptable performance in generic cases of having one community or no communities at all, which are the same case from the scheduler’s point of view, since any service can be assigned to any worker. Note that in the case of one community, the instances are reduced to eight because community density is omitted from the simulation variables. Fig. 12(a) depicts the effect of having only one community on the makespan. It can be observed that CORA outperforms all the other schemes in terms of makespan, with an improvement of up to 78%, 9%, 23%, 15%, and 10% compared to WorkQueue, Sufferage, Min-min, LJFR_SJFR, and Max–min, respectively. Despite the fact that the case of one community lacks the main challenge of community constraints that CORA normally addresses, CORA manages to pull a lead on the other heuristics. This lead can be explained by CORA’s ability to backtrack and reassign services, which addresses some of the cases where the other heuristic approaches fail.
Fig. 12(b) demonstrates the effect of having only one community on the average execution time. As shown in the figure, CORA achieves the minimum execution time among all approaches, with an improvement of up to 50% and 2% compared to WorkQueue and Max–min, respectively. CORA yields less than 1% lower execution time compared to Sufferage, Min-min, LJFR_SJFR, and Munkres.
7. Conclusion
In this paper, we have proposed the Community Edge Platform (CEP) to foster cost-free and privacy-preserved democratized edge computing. CEP exploits the notion of communities by leveraging the wide range of business, institutional, and social relationships between users. In addition, we have introduced the Community-Orientated Resource Allocation (CORA) scheme. CORA uses a graph-directed approach to allocate container-based services in CEP. We have conducted a comprehensive comparative study to compare CEP to 12 prominent edge computing platforms, where it has been shown that CEP has superior architecture and deployment features, including deployment location and worker ownership. Additionally, CEP does not impose any restrictions on the workload format or application types. It also supports various OS and can cover wide area networks. Moreover, CEP’s control paradigm allows it to fully leverage the advantages of edge computing. In terms of performance features, CEP can eliminate recruitment costs and ensure a high level of privacy, security, scalability, and deployment flexibility. Extensive simulations have shown that CORA yields a 44% lower makespan and yields an improvement of up to 7% in terms of execution time compared to six prominent resource allocation schemes, while maintaining adequate runtime.
CRediT authorship contribution statement
Abdalla A. Moustafa: Conceptualization, Methodology, Software, Formal analysis, Investigation, Visualization, Writing – original draft, Writing – review & editing. Sara A. Elsayed: Writing – review & editing, Methodology, Resources, Supervision. Hossam S. Hassanein: Writing – review & editing, Methodology, Funding acquisition, Resources, Supervision.
Declaration of competing interest
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
Acknowledgments
This research is supported by a grant from the Natural Sciences and Engineering Research Council of Canada (NSERC) under grant number ALLRP 549919-20, and a grant from Distributive, Ltd.
Appendix A. Supplementary data
The following is the Supplementary material related to this article.
Download: Download Acrobat PDF file (228KB)
MMC S1. .

Data availability
We provide the source code via GitHub to enable open access to CEP and make it readily available for the research community. A link to the GitHub repository is available in the footnote on page 2.
References
[1]
K. Gyarmathy
Comprehensive guide to IoT statistics you need to know in 2020
(2024)
[Online]. Available: https://www.vxchnge.com/blog/iot-statistics. (Accessed 15 January 2024)
Google Scholar
[2]
H. Zhou, Z. Zhang, D. Li, Z. Su
Joint optimization of computing offloading and service caching in edge computing-based smart grid
IEEE Trans. Cloud Comput., 11 (2) (2023), pp. 1122-1132
CrossrefView in ScopusGoogle Scholar
[3]
N. Norouzi, G. Bruder, B. Belna, S. Mutter, D. Turgut, G. Welch
A systematic review of the convergence of augmented reality, intelligent virtual agents, and the internet of things
Al-Turjman F. (Ed.), Artificial Intelligence in IoT, Transactions on Computational Science and Computational Intelligence, Springer, Cham, Switzerland (2019), pp. 1-24
CrossrefGoogle Scholar
[4]
M.L.M. Peixoto, A.H.O. Maia, E. Mota, E. Rangel, D.G. Costa, D. Turgut, L.A. Villas
A traffic data clustering framework based on fog computing for VANETs
Veh. Commun., 2214-2096, 31 (2021)
Google Scholar
[5]
Y. Li, K. Hwang, K. Shuai, Z. Li, A. Zomaya
Federated clouds for efficient multitasking in distributed artificial intelligence applications
IEEE Trans. Cloud Comput., 11 (2) (2023), pp. 2084-2095
View at publisherCrossrefView in ScopusGoogle Scholar
[6]
M. Bahrami, M. Singhal
A dynamic cloud computing architecture for cloud-assisted internet of things in the era of big data
Big Data and Computational Intelligence in Networking, CRC Press (2017), pp. 107-124
CrossrefView in ScopusGoogle Scholar
[7]
T. Liu, Y. Zhang, Y. Zhu, W. Tong, Y. Yang
Online computation offloading and resource scheduling in mobile-edge computing
IEEE Internet Things J., 8 (8) (2021), pp. 6649-6664
CrossrefView in ScopusGoogle Scholar
[8]
L. Peterson, T. Anderson, S. Katti, N. McKeown, G. Parulkar, J. Rexford, M. Satyanarayanan, O. Sunay, A. Vahdat
Democratizing the network edge
ACM SIGCOMM Comput. Commun. Rev., 49 (2) (2019), pp. 31-36
CrossrefView in ScopusGoogle Scholar
[9]
M. De’bas, S.A. Elsayed, H.S. Hassanein, Multitiered Worker-Oriented Resource Allocation at the Extreme Edge, in: Proceedings of the IEEE Global Communications Conference, GLOBECOM, Rio de Janeiro, Brazil, 2022, pp. 5674–5679.
Google Scholar
[10]
R. Kain, S.A. Elsayed, Y. Chen, H.S. Hassanein
RUMP: Resource usage multi-step prediction in extreme edge computing
Comput. Commun., 210 (2023), pp. 45-57
View PDF
View articleView in ScopusGoogle Scholar
[11]
A.A. Moustafa, S.A. Elsayed, H.S. Hassanein, Community-Oriented Resource Allocation At the Extreme Edge, GLOBECOM, Rio de Janeiro, Brazil, 2022, pp. 5583–5588.
Google Scholar
[12]
HomeEdge
(2023)
[Online]. Available: https://www.lfedge.org/projects/homeedge/. (Accessed 16 March 2023)
Google Scholar
[13]
F. Liu, G. Tang, Y. Li, Z. Cai, X. Zhang, T. Zhou
A survey on edge computing systems and tools
Proc. IEEE, 107 (8) (2019), pp. 1537-1562
CrossrefView in ScopusGoogle Scholar
[14]
Akraino
(2024)
[Online]. Available: https://www.lfedge.org/projects/akraino. (Accessed 15 January 2024)
Google Scholar
[15]
Mutable
(2024)
[Online]. Available: https://mutable.io. (Accessed 15 January 2024)
Google Scholar
[16]
Accedian mobiledgex
(2024)
[Online]. Available: https://accedian.com/mobiledgex. (Accessed 15 January 2024)
Google Scholar
[17]
IHS Markit
The internet of things: A movement, not a market
(2024)
[Online]. Available: https://cdn.ihs.com/www/pdf/IoTebook.pdf. (Accessed 20 August 2024)
Google Scholar
[18]
GSMA Intelligence
Edge computing coming to a place near you
(2024)
[Online]. Available: Edge computing coming to a place near you. (Accessed 20 August 2024)
Google Scholar
[19]
Edgex foundry
(2024)
[Online]. Available: https://www.edgexfoundry.org. (Accessed 15 January 2024)
Google Scholar
[20]
Azure IoT
(2024)
[Online]. Available: https://azure.microsoft.com/en-us/overview/iot. (Accessed 15 January 2024)
Google Scholar
[21]
Apache edgent
(2024)
[Online]. Available: https://incubator.apache.org/projects/edgent.html. (Accessed 15 January 2024)
Google Scholar
[22]
Kubernetes
(2024)
[Online]. Available: https://kubernetes.io. (Accessed 15 January 2024)
Google Scholar
[23]
AWS IoT greengrass
(2024)
[Online]. Available: https://aws.amazon.com/greengrasso. (Accessed 15 January 2024)
Google Scholar
[24]
Golem network
(2024)
[Online]. Available: https://www.golem.network. (Accessed 15 January 2024)
Google Scholar
[25]
Iexec
(2024)
[Online]. Available: https://iex.ec. (Accessed 15 January 2024)
Google Scholar
[26]
Otoy rendertoken
(2024)
[Online]. Available: https://rendertoken.com. (Accessed 15 January 2024)
Google Scholar
[27]
J. Dogani, R. Namvar, F. Khunjush
Auto-scaling techniques in container-based cloud and edge/fog computing: Taxonomy and survey
Comput. Commun., 209 (2023), pp. 120-150
View PDF
View articleView in ScopusGoogle Scholar
[28]
Docker swarm documentation
(2024)
[Online]. Available: https://docs.docker.com/engine/swarm. (Accessed 15 January 2024)
Google Scholar
[29]
K. Kaur, S. Garg, G. Kaddoum, S.H. Ahmed, M. Atiquzzaman
KEIDS: Kubernetes-based energy and interference driven scheduler for industrial IoT in edge-cloud ecosystem
IEEE Internet Things J., 7 (5) (2019), pp. 4228-4237
Google Scholar
[30]
J. Liu, C. Liu, B. Wang, G. Gao, S. Wang
Optimized task allocation for IoT application in mobile-edge computing
IEEE Internet Things J., 9 (13) (2022), pp. 10370-10381
CrossrefView in ScopusGoogle Scholar
[31]
Y. Cang, M. Chen, Z. Yang, Y. Hu, Y. Wang, C. Huang, Z. Zhang
Online resource allocation for semantic-aware edge computing systems
IEEE Internet Things J., 11 (17) (2024), pp. 28094-28110
CrossrefView in ScopusGoogle Scholar
[32]
M. De’bas, S.A. Elsayed, H.S. Hassanein
Multitiered worker-oriented resource allocation: Mitigating worker attrition at the extreme edge
IEEE Internet Things J. (2024), 10.1109/JIOT.2024.3425799
(Early Access )
Google Scholar
[33]
Y. Zhu, B. Mao, N. Kato
A dynamic task scheduling strategy for multi-access edge computing in IRS-aided vehicular networks
IEEE Trans. Emerg. Top. Comput., 10 (4) (2022), pp. 1761-1771
CrossrefView in ScopusGoogle Scholar
[34]
W. Lu, B. Li, B. Wu, Overhead Aware Task Scheduling for Cloud Container Services, in: IEEE 23rd International Conference on Computer Supported Cooperative Work in Design, CSCWD, 2019, pp. 380–385.
Google Scholar
[35]
I. Ahmad, M.G. AlFailakawi, A. AlMutawa, L. Alsalman
Container scheduling techniques: A survey and assessment
J. King Saud Univ.- Comput. Inf. Sci., 34 (7) (2021), pp. 934-3947
Google Scholar
[36]
S.S. Murad, R. Badeel
Optimized min-min task scheduling algorithm for scientific workflows in a cloud environment
J. Theor. Appl. Inf. Technol., 100 (2022), pp. 480-506
View in ScopusGoogle Scholar
[37]
S. Khurana, R.K. Singh
Survey of scheduling and meta scheduling heuristics in cloud environment
Computational Methods and Data Engineering, Springer (2021), pp. 363-374
CrossrefView in ScopusGoogle Scholar
[38]
S.A. Mohamed, S. Sorour, S.A. Elsayed, H.S. Hassanein
Cost and delay-aware service replication for scalable mobile edge computing
IEEE Internet Things J., 11 (6) (2024), pp. 10937-10950
CrossrefView in ScopusGoogle Scholar
[39]
F. Liu, H. Yu, J. Huang, T. Taleb
Joint service migration and resource allocation in edge IoT system based on deep reinforcement learning
IEEE Internet Things J., 11 (7) (2024), pp. 11341-11352
CrossrefView in ScopusGoogle Scholar
[40]
A. Moustafa
Community-Oriented Edge Computing Platform
(Master’s Thesis)
(2022)
[Online]. Available: http://hdl.handle.net/1974/30378
Google Scholar
[41]
M.A. Iverson, F. Ozguner, L. Potter
Statistical prediction of task execution times through analytic benchmarking for scheduling in a heterogeneous environment
IEEE Trans. Comput., 48 (12) (1999), pp. 1374-1379, 10.1109/12.817403
View in ScopusGoogle Scholar
[42]
T.-P. Pham, J.J. Durillo, T. Fahringer
Predicting workflow task execution time in the cloud using a two-stage machine learning approach
IEEE Trans. Cloud Comput., 8 (1) (2020), pp. 256-268, 10.1109/TCC.2017.2732344
View at publisher
View in ScopusGoogle Scholar
[43]
W. Fangyang, L. Yuming
Extended Kuhn-Munkres algorithm for constrained matching search
J. Beijing Norm. Univ.( Nat. Sci.), 57 (2) (2021), pp. 167-172, 10.12202/j.0476-0301.2019242
View at publisherGoogle Scholar
[44]
A. Bernstein, M.P. Gutenberg, T. Saranurak, Deterministic Decremental SSSP and Approximate Min-Cost Flow in Almost-Linear Time, in: IEEE 62nd Annual Symposium on Foundations of Computer Science, FOCS, 2022, pp. 1000–1008.
Google Scholar