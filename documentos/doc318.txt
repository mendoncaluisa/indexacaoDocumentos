Extensao de um ambiente de computac¸ ˜ ao de alto desempenho ˜ para o processamento de dados massivos 
Lucas M. Ponce1, Walter dos Santos1, 
Wagner Meira Jr.1, Dorgival Guedes1 
1Departamento de Ciencia da Computac¸ ˆ ao˜ 
Universidade Federal de Minas Gerais (UFMG) – Belo Horizonte, MG – Brazil {lucasmsp,walter,meira,dorgival}@dcc.ufmg.br 
Abstract. High performance computing (HPC) and massive data processing (Big Data) are two trends in computing systems that are beginning to con verge. This paper presents our experience on this path of convergence, exten ding COMP Superscalar (COMPSs), a parallel and distributed programming model already known in the world of HPC, for the processing of massive data. For this, it has been integrated to HDFS, the most widely used distributed file system for Big Data, and to Lemonade, a data mining and analysis tool deve loped at UFMG. The results show that the integration with HDFS benefits the COMPSs by the data abstraction provided and the integration with Lemonade facilitates its use and popularization in the area of Data Science. 
Resumo. A computac¸ao de alto desempenho (HPC) e o processamento de da- ˜ dos massivos (Big Data) sao duas tend ˜ encias em sistemas de computac¸ ˆ ao que ˜ estao comec¸ando a convergir. Este trabalho apresenta nossa experi ˜ encia nesse ˆ caminho de convergencia, estendendo o COMP Superscalar (COMPSs), um mo- ˆ delo de programac¸ao paralela e distribu ˜ ´ıda ja conhecido no mundo de HPC, ´ para o processamento de dados massivos. Para isso, ele foi integrado ao HDFS, sistema de arquivos distribu´ıdo mais usado para Big Data, e ao Lemonade, uma ferramenta de analise e minerac¸ ´ ao de dados desenvolvida na UFMG. Os ˜ resultados mostram que a integrac¸ao com o HDFS beneficia o COMPSs pela ˜ abstrac¸ao de dados fornecida e a integrac¸ ˜ ao com o Lemonade facilita sua ˜ utilizac¸ao e popularizac¸ ˜ ao na ˜ area de Ci ´ encia dos Dados. ˆ 
1. Introduc¸ao˜ 
A computac¸ao paralela e distribu ˜ ´ıda tem se mostrado essencial para lidar com processa mentos que exigem alto desempenho em computac¸ao, normalmente associado ao proces- ˜ samento de grandes volumes de dados. As aplicac¸oes de alto desempenho em computac¸ ˜ ao˜ (HPC) sao aquelas que geralmente se valem de hardware de alto n ˜ ´ıvel de paralelismo e alto desempenho, incluindo redes de baixa latencia, para processar dados normalmente ˆ regulares com algoritmos cient´ıficos. Ja em cen ´ arios convencionalmente denominados ´ de Big Data, que buscam processar grandes volumes de dados de diversos tipos, nor malmente nao estruturados, utilizam hardware convencional e se valem fortemente de ˜ tecnicas de paralelismo de dados. Nesse caso, os dados podem ser processados como ´ fluxos individuais e analisados coletivamente em stream ou em lote, para a descoberta de conhecimento, sendo a minerac¸ao em ˜ Big Data uma das tarefas chaves em muitos dom´ınios da ciencia [Kamburugamuve et al. 2017]. ˆ
Considerando suas similaridades, comec¸am a surgir propostas para a convergencia ˆ dessas duas areas em sistemas de computac¸ ´ ao [Fox et al. 2015, Reed and Dongarra 2015, ˜ Tejedor et al. 2017]. Ambientes de HPC em geral oferecem interfaces mais adequadas para o processamento de dados regulares, algoritmos cient´ıficos baseados em modelos de bag-of-tasks ou calculo matricial. Apesar de seu desempenho nesses modelos, ´ e muitas ´ vezes trabalhoso expressar neles aplicac¸oes que manipulam dados irregulares e estruturas ˜ de dados complexas. Por outro lado, ambientes de Big Data oferecem boas soluc¸oes ˜ para tratar tais tipos de dados, bem como para facilitar o desenvolvimento de aplicac¸oes ˜ pelos especialistas no dom´ınio da aplicac¸ao. Neste trabalho trabalhamos buscando uma ˜ integrac¸ao entre um ambiente comumente adotado em cen ˜ arios de HPC, o COMPSs, e ´ recursos usualmente associados com ambientes de Big Data, como o sistema de arquivos HDFS e um ambiente de desenvolvimento visual de aplicac¸oes de minerac¸ ˜ ao de dados. ˜ 
Desenvolvido pelo Centro de Supercomputac¸ao de Barcelona, o COMPSs im- ˜ plementa um modelo de programac¸ao baseado em tarefas, modelo que vem se mos- ˜ trando adequado para aplicac¸oes HPC [Lezzi et al. 2011, Lordan et al. 2016]. Um tra- ˜ balho recente [Conejero et al. 2017] comparou seu desempenho ao Apache Spark, um dos frameworks de Big Data mais utilizados atualmente [Zaharia et al. 2012]. Apesar do trabalho indicar o melhor desempenho do COMPSs nos cenarios testados, uma das ´ principais vantagens do Spark e a ampla gama das bibliotecas que est ´ ao dispon ˜ ´ıveis em torno desse ambiente devido a sua popularidade (p.ex., MLlib, GrapX, Streaming ` e SparkSQL) [Conejero et al. 2017]. Alem disso, Spark conta com uma maior interope- ´ rabilidade com outras ferramentas de Big Data. 
Nesse contexto, propomos uma extensao do COMPSs em duas direc¸ ˜ oes: primeiro, ˜ adicionando o suporte ao HDFS, o sistema de arquivos distribu´ıdo mais usado para Big Data; segundo, integrando-o a um ambiente de desenvolvimento de aplicac¸oes de pro- ˜ cessamento de dados massivos que reduz a necessidade do usuario conhecer os detalhes ´ de uma linguagem de programac¸ao para produzir aplicac¸ ˜ oes ˜ Big Data. Com o suporte ao HDFS, pretendemos facilitar a utilizac¸ao do COMPSs com grandes volumes de dados ˜ nao estruturados comumente usados e que costumam estar dispon ˜ ´ıveis em repositorios ´ usando o HDFS. Com o uso de um ambiente de desenvolvimento visual, pretendemos tornar o COMPSs acess´ıvel a um numero maior de usu ´ arios que normalmente n ´ ao do- ˜ minam os detalhes de uma linguagem de programac¸ao paralela. Para esse fim, adotamos ˜ o ambiente Lemonade (do inglesˆ Live Environment for Mining Of Non-trivial Amount of Data Everywhere), uma ferramenta de analise e minerac¸ ´ ao de dados desenvolvida na ˜ Universidade Federal de Minas Gerais (UFMG) [Santos et al. 2017]. 
Com esse objetivo em mente, a sec¸ao 2 descreve o COMPSs e as sec¸ ˜ oes 3 e 4 apre- ˜ sentam a sua integrac¸ao com o HDFS e com o Lemonade, respectivamente. A avaliac¸ ˜ ao˜ de desempenho das integrac¸oes ˜ e discutida na sec¸ ´ ao 5 e finalmente a sec¸ ˜ ao 6 apresenta ˜ nossas conclusoes e discute trabalhos futuros. ˜ 
2. COMPSs 
O COMPSs e um modelo de programac¸ ´ ao que est ˜ a em constante desenvolvimento, um ´ objetivo do Centro de Supercomputac¸ao de Barcelona ˜ e o lanc¸amento uma nova vers ´ ao˜ a cada seis meses, seja com novas funcionalidades e/ou suportes [Lezzi et al. 2011, Tejedor et al. 2017]. Atualmente na versao 2.2, esse modelo suporta linguagens Java, ˜
Python e C/C++. As aplicac¸oes em COMPSs s ˜ ao escritas de acordo com o paradigma ˜ sequencial, com a adic¸ao de algumas anotac¸ ˜ oes no c ˜ odigo que s ´ ao usadas para informar ˜ que um dado metodo no c ´ odigo pode ser executado em paralelo. No caso de Java e C++, ´ essas anotac¸oes s ˜ ao semelhantes a uma interface de um m ˜ etodo, sendo necess ´ ario infor- ´ mar, por exemplo, os tipos de dado dos parametros de entrada e de sa ˆ ´ıda. Ja no caso do ´ Python, essa anotac¸ao nada mais ˜ e que um ´ decorator iniciado com “@task”em cima de um metodo a ser paralelizado. ´ 
Sua estrutura e baseada no paradigma ´ master-worker. Em execuc¸oes em ˜ cluster ou em nuvem e necess ´ ario definir, por exemplo, o conjunto de m ´ aquinas pertencentes ´ a` execuc¸ao (no caso de ˜ cluster) e as especificac¸oes das VMs (no caso de nuvem). Um n ˜ o´ especificado como central (master) e respons ´ avel pelo gerenciamento da aplicac¸ ´ ao, pela ˜ transferencias de dados, agendamento e pela gest ˆ ao da infra-estrutura. Todos os outros ˜ nos atuam como trabalhadores ( ´ workers) e sao respons ˜ aveis por realizar qualquer ac¸ ´ ao˜ que o master solicite [Conejero et al. 2017]. 
Quando uma aplicac¸ao se inicia, o COMPSs analisa os m ˜ etodos informados pelo ´ usuario como paraleliz ´ aveis e cria durante essa an ´ alise um Grafo Direcionado Ac ´ ´ıclico (DAG) que relaciona a dependencia entre esses m ˆ etodos. Esse DAG pode ser visuali- ´ zado posteriormente pelo usuario e ´ e´ util para entender o comportamento da execuc¸ ´ ao˜ em questao, por exemplo, mostrando quais m ˜ etodos est ´ ao sendo realmente paralelizados. ˜ (Um exemplo desse DAG pode ser visto na fig. 2.) Durante a execuc¸ao, o ambiente ˜ e´ capaz de submeter paralelamente todas as tarefas que nao possuem depend ˜ encias entre ˆ si, caso haja recurso computacional dispon´ıvel; caso contrario, os m ´ etodos que n ´ ao t ˜ emˆ recursos alocados sao escalonados ˜ a medida que suas depend ` encias s ˆ ao satisfeitas. ˜ 
Quando um worker finaliza a execuc¸ao de uma de suas tarefas, o mesmo ˜ e res- ´ ponsavel por salvar o resultado produzido em um arquivo bin ´ ario (serializ ´ avel) e por ´ informar ao computador central sobre a sua finalizac¸ao. O computador mestre, por sua ˜ vez, verifica (a partir da DAG gerada anteriormente), qual sera o pr ´ oximo m ´ etodo que ir ´ a´ precisar desse resultado parcial gerado. O escalonador podera atribuir essa nova tarefa ao ´ mesmo worker que gerou tal dado ou pode atribuir a um novo no. Nesse caso, ser ´ a de ´ responsabilidade do worker produtor o envio de tal dado. Esse e um aspecto importante ´ da abordagem que o COMPSs utiliza, isto e, caso existam depend ´ encias durante uma ˆ execuc¸ao, os dados parciais (que est ˜ ao em espera) n ˜ ao ocupar ˜ ao espac¸o em mem ˜ oria uma ´ vez que estao salvos em disco. Al ˜ em disso, o seu escalonador, al ´ em de outros fatores, ´ leva em considerac¸ao a localidade dos dados para atribuic¸ ˜ ao de novas tarefas. ˜ 
3. Integrac¸ao entre o HDFS e o COMPSs ˜ 
Como ja mencionado, COMPSs ´ e baseado em tarefas, logo, para explorar um maior n ´ ´ıvel de paralelismo e necess ´ ario trabalhar sobre fragmentos de um dado. Em uma execuc¸ ´ ao˜ COMPSs sem o uso de discos compartilhados, quando tarefas paralelizaveis precisam ´ ler arquivos armazenados no sistema de arquivos convencional, esses arquivos precisam estar no computador central. A partir da´ı, o COMPSs os transferem pela rede para cada no solicitante, o que serializa o acesso aos dados. Caso se deseje permitir que diversas ´ tarefas leiam partes distintas do mesmo conjunto de dados, uma abordagem seria enviar o arquivo completo para cada tarefa, juntamente com um mapeamento que delimite a parte atribu´ıvel a cada tarefa. Embora seja mais facil de implementar, esta alternativa ´
desperdic¸a recursos de rede e de armazenamento. Outra alternativa e dividir o arquivo no ´ numero de fragmentos desejados e direcionar cada um desses novos arquivos criados para ´ cada tarefa. Desta forma, apenas a parte respectiva do arquivo sera enviado pela rede, no ´ entanto, a divisao˜ e uma responsabilidade do programador. Ambas as soluc¸ ´ oes exigem ˜ a intervenc¸ao do usu ˜ ario para fazer a distribuic¸ ´ ao dos dados e a decis ˜ ao sobre como os ˜ mesmos podem ser distribu´ıdos entre as tarefas. 
O sistema de arquivos HDFS foi desenvolvido exatamente para gerenciar a di visao e distribuic¸ ˜ ao das partes de arquivos massivos. No HDFS, cada arquivo ˜ e dividido ´ em blocos (128 MB e um tamanho usual), que s ´ ao distribu ˜ ´ıdos entre os nos de armazena- ´ mento. Para fins de tolerancia a falha e para aumentar a disponibilidade dos dados, cada ˆ bloco pode ser replicado em mais de um no. Quando uma aplicac¸ ´ ao distribu ˜ ´ıda deseja acessar o arquivo, ela recebe uma lista identificando todos os blocos que compoem um ˜ arquivo, com informac¸oes sobre a localizac¸ ˜ ao de cada bloco (e suas poss ˜ ´ıveis replicas). A ´ aplicac¸ao, de posse dessa informac¸ ˜ ao, pode decidir como pretende distribuir as partes do ˜ arquivo entre seus nos de processamento. Cada n ´ o recebe alguns blocos para processar e ´ pode acessar o HDFS diretamente para obter o conteudo de tais blocos, paralelamente a ´ outros nos de processamento. ´ 
O principal conceito apresentado na integrac¸ao entre o HDFS e COMPSs ˜ e a ´ delegac¸ao de algumas responsabilidades ao HDFS, tais como, a divis ˜ ao dos arquivos ˜ de entrada em blocos e a transferencia desses blocos para cada tarefa COMPSs. Na ˆ integrac¸ao dos dois sistema, o primeiro passo ˜ e decidir como as abstrac¸ ´ oes do HDFS ˜ se tornam dispon´ıveis para o programador COMPSs. 
3.1. Abstrac¸ao de dados ˜ 
A API de integrac¸ao consiste em prover ao programador COMPSs dois tipos de entidades ˜ com func¸oes bem definidas. A primeira ˜ e respons ´ avel por lidar com o HDFS diretamente, ´ como por exemplo, criar pastas ou obter informac¸oes sobre um arquivo. J ˜ a a outra ´ e´ responsavel pela abstrac¸ ´ ao de arquivos divididos em fragmentos. A primeira entidade ˜ e´ representada pela classe HDFS, enquanto a segunda e representada pela classe Block, que ´ possui metodos como ´ readBlock (le um fragmento de arquivo e o armazena como uma ˆ String) e readDataframe (le um fragmento de arquivo e armazena como um ˆ DataFrame). 
Para a etapa de leitura, o usuario solicita ´ a API que o HDFS lhe informe uma ` lista de n fragmentos de um dado arquivo, onde a quantidade e definida pelo usu ´ ario, ´ geralmente relacionada ao numero de ´ cores dispon´ıveis em seu cluster. No momento da leitura, dentro de uma tarefa COMPSs, a entidade HDFS, com base nas informac¸oes de ˜ cada fragmento, sera respons ´ avel por escolher o melhor fornecedor ( ´ datanode) desse dado parcial e pela transferencia em si. A ideia por tr ˆ as disso ´ e que as tarefas funcionam em ´ blocos e cada bloco sera executado em uma tarefa. O Algoritmo 1 exemplifica um escopo ´ de execuc¸ao COMPSs utilizando a API. ˜ 
Nesse exemplo, cada um dos N fragmentos de um arquivo X armazenado no HDFS sera lido paralelamente dentro da sua tarefa ( ´ task1). A partir da´ı, os proximos passos s ´ ao˜ similares as soluc¸ ` oes j ˜ a existentes na programac¸ ´ ao COMPSs, isto ˜ e, cada resultado parcial ´ pode ser salvo em um arquivo distinto ou pode servir de entrada para uma nova tarefa.
Algoritmo 1: Exemplo de uso da integrac¸ao do HDFS ao COMPSs ˜ Dados: N e X 
in´ıcio 
BLOCKSLIST = recupera a lista com N fragmentos de um arquivo X no HDFS; 
para block ∈ BLOCKSLIST fac¸a 
task1(block); 
fim 
fim 
3.2. Comunicac¸ao com o HDFS ˜ 
Entre as versoes de COMPSs dispon ˜ ´ıvel, nosso alvo foi a versao em Python. Atualmente ˜ o Hadoop/HDFS nao suporta nativamente essa linguagem, o que nos levou a considerar ˜ diferentes alternativas de integrac¸ao. ˜ 
Uma primeira soluc¸ao seria utilizar o webHDFS, uma API nativa do HDFS para a ˜ comunicac¸ao utilizando protocolo HTTP REST [Gonzales 2016]. Apesar de fornecer os ˜ mesmos metodos dispon ´ ´ıveis na API Java, a utilizac¸ao do webHDFS ˜ e significativamente ´ mais lenta. Essa lentidao˜ e tanto devida ao tempo de requisic¸ ´ ao quanto ao ˜ overhead da utilizac¸ao do protocolo HTTP. A segunda abordagem consistiria na criac¸ ˜ ao de um ˜ servic¸o de comunicac¸ao entre a aplicac¸ ˜ ao Python e um processo Java, semelhante ao que ˜ e realizado pelo PySpark utilizando o m ´ odulo Py4J [Rosen 2016]. O PySpark, que possui ´ uma estrutura h´ıbrida de Python e JVM, utiliza o Py4J ja internamente apenas como um ´ driver de requisic¸ao para o processo Spark na JVM. J ˜ a no caso do COMPSs, sem uma ´ mudanc¸a no seu codigo-fonte, n ´ ao seria poss ˜ ´ıvel manter tal n´ıvel de integrac¸ao. Caso ˜ contrario, cada ´ thread criada em COMPSs precisaria se comunicar com um intermediador em Java (que se conectaria ao HDFS) externamente ao COMPSs para solicitar e receber dados. Dessa forma, alem de tais processos externos aumentarem o consumo de mem ´ oria, ´ a transferencias desses dados teria que ser por meio do pr ˆ oprio intermediador, diferente ´ do que ocorre no PySpark. 
A abordagem escolhida foi a utilizac¸ao do ˜ wrapper C da Java API (libHDFS), tambem utilizada em outros trabalhos [Leo and Zanetti 2010, Rocha et al. 2016]. Esse ´ wrapper se comunica diretamente com Java utilizando a Java Native Interface (JNI), uma tecnologia para comunicac¸ao de aplicac¸ ˜ oes diretamente na JVM do Java. Por sua vez, ˜ Python possui, por padrao, m ˜ odulos capazes de interpretar e converter tipos de dados ´ da linguagem C. Com base nisso, foi poss´ıvel utilizar a libHDFS a partir de um mape amento das func¸oes e dos tipos de dados a serem utilizados pela integrac¸ ˜ ao do HDFS. ˜ Dessa forma, os metodos invocados em Python s ´ ao executados internamente em lingua- ˜ gem C a partir desse wrapper. A libHDFS utilizada nessa integrac¸ao cont ˜ em algumas ´ mudanc¸as em relac¸ao˜ a aquela disponibilizada pelo HDFS no seu bin ` ario de instalac¸ ´ ao, ˜ como a remoc¸ao de algumas vari ˜ aveis de ambiente, uma vez que at ´ e a presente vers ´ ao do ˜ COMPSs (v2.2), as variaveis de ambiente definidas no ´ master nao eram carregadas para ˜ as tarefas paralelizadas. A vantagem de se utilizar essa ultima abordagem ´ e que como ´ essas operac¸oes s ˜ ao executadas internamente em C ela ˜ e mais r ´ apida e eficiente que usar ´ a webHDFS ou usar intermediadores de Java para Python como o Py4j.
4. Suporte do Lemonade ao COMPSs 
Na area de ´ Big Data, o usuario que controla o processamento frequentemente carece ´ de uma formac¸ao em Computac¸ ˜ ao, especialmente em programac¸ ˜ ao paralela/distribu ˜ ´ıda. Em geral esse profissional e oriundo da ´ area onde os dados foram obtidos, sendo assim ´ a pessoa mais adequada para interpreta-los. Com isso, um desafio reconhecido pelos ´ pesquisadores na area ´ e a dificuldade de se levar ao especialista no dom ´ ´ınio dos dados uma ferramenta de processamento com que ele possa expressar suas consultas. COMPSs, apesar de reduzir a demanda sobre o programador em termos de programac¸ao paralela, ˜ ainda exige que o desenvolvedor identifique as tarefas que podem ser paralelizadas e as programe em uma linguagem como Java ou Python. Para reduzir essa barreira para o uso do ambiente, decidimos realizar a integrac¸ao do ambiente de execuc¸ ˜ ao COMPSs com o ˜ Lemonade, um ambiente para programac¸ao visual de tarefas ˜ Big Data. 
4.1. O ambiente Lemonade 
Lemonade1e uma ferramenta visual para o cientista de dados e visa usu ´ arios que n ´ ao˜ conhecem a linguagem de programac¸ao ou que precisem desenvolver fluxos de traba- ˜ lho usando o conjunto de ferramentas existente. A plataforma e voltada para a criac¸ ´ ao˜ de fluxos de analise e minerac¸ ´ ao de dados em nuvem, com garantias de autenticac¸ ˜ ao, ˜ autorizac¸ao e contabilizac¸ ˜ ao de acesso. A figura 1 ˜ e um exemplo do tipo de aplicac¸ ´ ao˜ que o Lemonade e capaz de criar. Cada caixa representa uma operac¸ ´ ao ou algoritmo, por ˜ exemplo, ler um arquivo ou treinar um classificador. Para cada caixa, o usuario ´ e capaz ´ de configurar os parametros da respectiva operac¸ ˆ ao, por exemplo, qual o nome do arquivo ˜ a ser lido ou o numero m ´ aximo de iterac¸ ´ oes de um algoritmo. Em sua vers ˜ ao atual, o ˜ Lemonade possui diretrizes capazes de gerar codigo Spark 2.0.2 em linguagem Python ´ (PySpark) a partir de um fluxo de operac¸oes definidas visualmente pelo usu ˜ ario. Com a ´ integrac¸ao com o COMPSs, programas e bibliotecas de algoritmos escritos em COMPSs ˜ podem se tornar rapidamente dispon´ıveis para os usuarios do Lemonade. ´ 
  
Figura 1. Esquema da aplicac¸ao Workflow criada a partir do Lemonade. ˜ 1https://github.com/W3CBrasil/AI-Social/tree/lemonade
A arquitetura do Lemonade define sete componentes individuais que funcionam como micro servic¸os especializados nas areas de interface ( ´ Citron), seguranc¸a/privacidade (Thorn), execuc¸ao ( ˜ Stand), monitorac¸ao de execuc¸ ˜ ao ( ˜ Juicer), base de dados (Limonero), algoritmos (Tahiti) e visualizac¸ao ( ˜ Caipirinha). A presente extensao do Lemonade para o ˜ Suporte do COMPSs envolveu dois desses componentes, Tahiti e Juicer. 
Para o componente Tahiti, responsavel por manter metadados referentes ´ as` operac¸oes e aos fluxos de dados criados por usu ˜ arios, foi necess ´ ario o cadastro de todas ´ as operac¸oes e algoritmos que foram implementadas em COMPSs. Tal cadastro envolve ˜ por exemplo, a categoria da operac¸ao (p.ex., operac¸ ˜ ao sobre textos ou de algoritmos de ˜ aprendizado de maquina) e os par ´ ametros da cada operac¸ ˆ ao (p.ex., nome das colunas ou ˜ valor maximo de iterac¸ ´ ao de um dado algoritmo). ˜ 
O componente Juicer e respons ´ avel por traduzir o fluxo de operac¸ ´ ao visual cri- ˜ ado pelo usuario (representado em um arquivo ´ JSON) em codigo-fonte. Tamb ´ em´ e res- ´ ponsavel por submeter a aplicac¸ ´ ao resultante para execuc¸ ˜ ao em um ˜ cluster. Para esse componente, foi necessaria a criac¸ ´ ao de um novo compilador de c ˜ odigo fonte ( ´ source-to source compiler, ou transpiler) capaz de gerar codigo COMPSs. Uma vez que o usu ´ ario ´ dispare um workflow criado, o transpiler le o arquivo ˆ JSON produzido e identifica se e´ uma execuc¸ao em COMPSs. Em seguida, cria um grafo onde os n ˜ os s ´ ao as caixas de ˜ operac¸oes e executa uma ordenac¸ ˜ ao topol ˜ ogica para criar uma sequ ´ encia l ˆ ogica de escrita ´ do codigo (essa ordem ´ e usada apenas para determinar a estrutura do c ´ odigo, n ´ ao tendo ˜ implicac¸oes sobre a paralelizac¸ ˜ ao realizada pelo COMPSs durante a execuc¸ ˜ ao). ˜ 
O codigo gerado pelo ´ transpiler inclui a importac¸ao dos m ˜ etodos presentes na ´ biblioteca de metodos COMPS para o Lemonade e as instanciac¸ ´ oes desses m ˜ etodos com ´ a correta configurac¸ao dos par ˜ ametros. Em outras palavras, a aplicac¸ ˆ ao final ˜ e composta ´ por duas partes, uma que e gerada dinamicamente pelo ´ transpiler, que e basicamente um ´ metodo principal com todas as chamadas dos m ´ etodos utilizados na aplicac¸ ´ ao, e outra que ˜ e uma biblioteca com implementac¸ ´ oes de operac¸ ˜ oes e algoritmos. ˜ 
4.2. Algoritmos e Operac¸oes ˜ 
Diferente do Spark, que explora o paralelismo a partir de operadores sobre dados, em COMPSs o paralelismo e definido a partir da aus ´ encia de depend ˆ encias entre tarefas. Em ˆ versoes sequenciais usuais, um algoritmo pode ser visto como uma grande tarefa que deve ˜ ser executada sequencialmente sobre um unico (grande) bloco de dados. Assim, a ideia ´ por tras das implementac¸ ´ oes realizadas em COMPSs para o Lemonade foi de aproveitar ˜ a noc¸ao de blocos do HDFS e carregar um grande arquivo de dados dividido em diversos ˜ fragmentos. Dessa forma, em casos gerais, uma tarefa que seria executada sequencial mente em apenas um unico conjunto de dados agora poder ´ a ser executada paralelamente ´ em conjuntos menores. 
Em um caso de exemplo de um algoritmo simples de contagem de palavras (Word Count), um tipo de tarefa necessaria, e que seria paralelizada pelo COMPSs, seria a conta- ´ gem de cada palavra presente em cada segmento. Apos isso, seria necess ´ ario realizar uma ´ reduc¸ao dessas contagens parciais. Al ˜ em de paralelizar a execuc¸ ´ ao, uma outra vantagem ˜ da divisao de dados em segmentos ˜ e que dessa forma, cada bloco caber ´ a completamente ´ em memoria. O n ´ umero de segmentos ´ e relacionado com o n ´ umero de ´ cores dispon´ıveis para uma dada execuc¸ao. Ap ˜ os essa divis ´ ao do dado, caso um fragmento ainda seja maior ˜
que a memoria dispon ´ ´ıvel em uma maquina, basta aumentar o n ´ umero de fragmentos para ´ um valor proporcional ao numero de ´ cores pois, nesse cenario, mesmo que um conjunto ´ desses dados fiquem ociosos, eles nao ocupar ˜ ao espac¸o em mem ˜ oria pois estar ´ ao salvos ˜ temporariamente pelo COMPSs como arquivos binarios em disco. ´ 
Os parametros de cada func¸ ˆ ao podem ser de dois tipos, o dado de entrada e um ˜ dicionario de configurac¸ ´ ao. O dicion ˜ ario de configurac¸ ´ ao˜ e respons ´ avel por armazenar ´ todos campos de configurac¸ao especificados pelo usu ˜ ario pela interface. J ´ a o dado de ´ entrada/sa´ıda sera sempre uma lista de ´ DataFrames, essa padronizac¸ao garante a compa- ˜ tibilidade entre a sa´ıda de uma func¸ao e entrada de outra. Internamente, uma func¸ ˜ ao pode ˜ fazer agregac¸oes e pode executar outras func¸ ˜ oes com outros tipos de par ˜ ametros, mas o ˆ resultado final de uma func¸ao que representa uma caixa do Lemonade ser ˜ a sempre uma ´ lista de DataFrames com o mesmo numero de elementos da entrada. ´ 
A figura 2 e um exemplo de uma aplicac¸ ´ ao gerada pelo Lemonade como parte ˜ da aplicac¸ao mostrada na fig. 1, com o grafo de depend ˜ encias gerado pelo COMPSs. A ˆ caixa 1, mostrada a direita, representa o que e gerado dinamicamente para cada aplicac¸ ´ ao, ˜ contem um m ´ etodo ´ main com todas as definic¸oes de execuc¸ ˜ ao dos m ˜ etodos (p.ex., set- ´ tings 0), e com todas as chamadas dos metodos da aplicac¸ ´ ao (que est ˜ ao dispon ˜ ´ıveis na biblioteca do Lemonade). Ja a caixa 2 ´ e um exemplo de uma dessas func¸ ´ oes que podem ˜ ser utilizadas. A func¸ao em quest ˜ ao˜ e utilizada para remover colunas espec ´ ´ıficas do dado de entrada em todas os fragmentos de dataFrame paralelamente. Como ja mencionado, ´ para o COMPSs saber que tal metodo pode ser executado paralelamente ´ e necess ´ ario adi- ´ cionar uma anotac¸ao sobre o m ˜ etodo. Nesse exemplo, essa anotac¸ ´ ao, ˜ e representado por ´ “@task(returns=list)”e informa ao COMPSs que alem desse m ´ etodo ser paraleliz ´ avel a ´ sua sa´ıda e um tipo de lista. ´ 
def main(): 
1 
 ... 
 from functions.data.ReadData import ReadCSVFromHDFSOperation 
 from functions.ml.FeatureAssembler import FeatureAssemblerOperation  from functions.etl.Drop import DropOperation 
 data0 = ReadCSVFromHDFSOperation(settings_0) 
 data1 = FeatureAssemblerOperation(data0, settings_1)  data2 = DropOperation(data1, settings_2) 
2 
def DropOperation(data, settings): 
 numFrag = len(data) 
 result = [ [] for f in range(numFrag)] 
 for f in range(numFrag): 
 result[f] = Drop_partial(data[f], columns) 
 return result 
@task(returns=list) 
def Drop_partial(df, columns): 
 return df.drop(columns, axis=1) 
Figura 2. Exemplo de codigo gerado pelo Lemonade. ´ 
Ja o grafo na figura 2 ´ e um Grafo Direcionado Ac ´ ´ıclico (DAG) gerado pelo COMPSs durante a execuc¸ao. Nesse exemplo, podemos ver por exemplo que a execuc¸ ˜ ao˜ continha tres m ˆ etodos diferentes, a saber, um para ler o dado em quatro partic¸ ´ oes, outro ˜ para criar um vetor de atributos e por fim, remover algumas colunas. Nesse caso, foram utilizados apenas quatro cores para a execuc¸ao. A escolha dos algoritmos a serem imple- ˜ mentados foi baseada nos algoritmos existentes em Spark no Lemonade e tambem seguiu ´ a lista dos algoritmos mais populares em minerac¸ao de dados [Wu et al. 2008]. Foram ˜
implementadas 44 func¸oes, disponibilizadas e documentadas no GitHub ˜2. 
5. Avaliac¸ao de desempenho ˜ 
Na presente sec¸ao, avaliamos as integrac¸ ˜ oes do COMPSs com o HDFS e com o Lemo- ˜ nade, comparando seu desempenho com dois casos de uso sem tais integrac¸oes. Especifi- ˜ camente, buscamos responder as seguintes perguntas: (1) As soluc¸oes utilizando o HDFS ˜ como o sistema de arquivos possuem tempos de execuc¸ao significativamente diferentes ˜ das soluc¸oes existentes? ˜ (2) Caso exista diferenc¸a, existe algum limiar onde uma soluc¸ao˜ passa a ser melhor que a outra? (3) O tempo de execuc¸ao de uma aplicac¸ ˜ ao codificada ˜ diretamente pode ser significativamente diferente em do tempo de uma aplicac¸ao gerada ˜ automaticamente pelo Lemonade? 
5.1. Metodologia e Base de dados 
As avaliac¸oes de desempenho foram realizadas aplicando duas t ˜ ecnicas experimentais, ´ o teste pareado e a regressao linear, sobre dois casos de uso, denominados WordCount e ˜ Workflow. A primeira tecnica foi utilizada para responder as quest ´ oes 1 e 2, j ˜ a a regress ´ ao˜ linear foi utilizada especificamente para responder a questao 3. O primeiro caso de uso ˜ e um simples algoritmo de WordCount, escolhido por ser uma aplicac¸ ´ ao onde o tempo ˜ de execuc¸ao˜ e basicamente relacionado ao tempo de leitura e de iterac¸ ´ ao sobre os dados ˜ lidos. O segundo caso de uso, o fluxo de operac¸ao mostrado inicialmente na figura 1, ˜ e´ composto por multiplas operac¸ ´ oes e algoritmos, sendo um exemplo t ˜ ´ıpico de execuc¸ao˜ esperada com o uso do Lemonade, consistindo em uma etapa de preprocessamento e uma de classificac¸ao de dados. ˜ 
Os dois casos de uso contem tipos de dados de entrada diferentes: no caso do ˆ WordCount, sua entrada e um arquivo de texto; j ´ a o Workflow recebe um arquivo tabular ´ (csv) com atributos numericos. Ambas as aplicac¸ ´ oes n ˜ ao imp ˜ oem condic¸ ˜ oes dependentes ˜ da base para um melhor ou pior caso de execuc¸ao. Tento isso em vista, a carga de texto ˜ foi criada a partir da junc¸ao de v ˜ arios livros no formato ´ txt disponibilizados pelo Projeto Gutenberg3e depois concatenada repetidas vezes. Ja para a carga num ´ erica, foi utilizado ´ o Data Set Higgs4 que contem inicialmente 11 ´ × 106amostras com 28 dimensoes de ˜ processos de sinal simulado que produzem os Bosons de Higgs. 
Todos os experimentos utilizaram um cluster COMPSs/HDFS com um no´ master dedicado e oito nos´ slaves/workers. As maquinas possuiam processadores Intel E56xx de ´ 2,5 GHz com 4 cores, 8 GB de RAM, executando Ubuntu Linux 16.04 LTS. O HDFS foi configurado com um fator de replicac¸ao igual a tr ˜ es. ˆ 
5.2. Impacto da utilizac¸ao do HDFS ˜ 
Para avaliar o impacto do uso do HDFS sobre execuc¸oes COMPSs foi realizado um teste ˜ pareado comparando os dois casos de uso adotados neste trabalho com suas respectivas versoes com ou sem o HDFS. Para cada algoritmo, realizamos 40 execuc¸ ˜ oes. Essa quan- ˜ tidade de repetic¸oes permite realizar um teste pareado ˜ Z, ou seja, utilizando o coeficiente Z. Apos as execuc¸ ´ oes foi calculado o intervalo de confianc¸a de um lado com 95% de ˜ confianc¸a. A tabela 1 contem o resultado desses c ´ alculos na escala de segundos. ´ 
2https://github.com/eubr-bigsea/Compss-Python 
3http://www.gutenberg.org 
4dispon´ıvel em: https://archive.ics.uci.edu/ml/datasets/HIGGS
Tabela 1. Impacto do HDFS: teste pareado Z (tempos em segundos). Informac¸ao WordCount Workflow ˜ 
Numero de amostras 40 40 ´ 
Media das diferenc¸as -120,5 -173,8 ´ 
Desvio padrao das diferenc¸as 54,2 32,1 ˜ Intervalo de Confianc¸a de um-lado (95%) (-∞, −106, 4) (-∞, −165, 5) Tempo de execuc¸ao m ˜ edia (com HDFS) 99 317 ´ Tempo de execuc¸ao m ˜ edia (sem HDFS) 211 497 ´ 
Para o primeiro algoritmo, o WordCount, o tempo de execuc¸ao utilizando o HDFS ˜ foi em media 120 segundos mais r ´ apido que o convencional. O intervalo de confianc¸a ´ de um lado indica que com 95% de confianc¸a, a versao com HDFS ˜ e no m ´ ´ınimo 106 segundos mais rapido, que em termos percentuais equivale a aproximadamente 50% do ´ tempo de execuc¸ao do WordCount sem utilizar o HDFS. ˜ 
O resultado foi similar para o segundo algoritmo. A versao com HDFS foi no ˜ m´ınimo 166 segundos mais rapida que o mesmo algoritmo utilizando o sistema de ar- ´ quivos convencional com 95% de confianc¸a. Em termos percentuais, equivale a aproxi madamente 33% do tempo de execuc¸ao da aplicac¸ ˜ ao Workflow sem utilizar o HDFS. ˜ E´ interessante observar que esse ganho percentual e menor que o obtido para o WordCount, ´ entretanto, isso pode ser explicado considerando-se que o segundo caso de uso possui uma carga maior de processamento envolvido. 
As figuras 3(a) e 3(b) mostram traces de mapeamento das transferencias de dados ˆ entre os nos do ´ cluster gerados pelo COMPSs durante a execuc¸ao do WordCount com e ˜ sem a integrac¸ao com HDFS. As linhas amarelas representam a transfer ˜ encia de arquivos ˆ durante a execuc¸ao das aplicac¸ ˜ oes, cada ˜ Task presente no eixo vertical dos traces repre sentam um computador. A Task 1.1, no topo do eixo vertical, representa o computador central (o que orquestra e envia os dados) e as demais representam os oito workers utiliza dos na execuc¸ao. As figuras nos ajudam a entender um dos motivos para o COMPSs com ˜ HDFS se mostrar mais rapido que a vers ´ ao utilizando o sistema de arquivos convencional. ˜ 
Comparando as duas imagens podemos observar que na figura 3(b), o master e´ quem transfere todos os arquivos de dados e que parte do tempo de execuc¸ao do algoritmo ˜ e devido ´ a espera pela transfer ` encia desses arquivos. As transfer ˆ encias vistas aproxima- ˆ damente na metade das execuc¸oes s ˜ ao os resultados parciais que foram gerados e que ˜ precisam ser agrupados; esses nao s ˜ ao mais realizados exclusivamente pelo ˜ master e sim pelos nos que produziram tais resultados. Esse comportamento ´ e o que de fato era espe- ´ rado nas versoes utilizando o HDFS, pois nesse cen ˜ ario o COMPSs transfere apenas uma ´ lista contendo informac¸oes sobre os fragmentos dos arquivos que cada n ˜ o dever ´ a acessar ´ e o HDFS e quem vai transferir os dados dos diversos blocos para os n ´ os. ´ 
O segundo tipo de experimento verifica se existe algum limiar onde usar HDFS passa a ser melhor ou pior que nao us ˜ a-lo. Para isso, executamos o Wordcount com ´ diferentes tamanhos de arquivos e realizamos duas regressoes lineares, uma para mapear o ˜ tempo de execuc¸ao de uma aplicac¸ ˜ ao utilizando o HDFS e outra sem o seu uso. Utilizamos ˜ o WordCount como base desse experimento, pois o comportamento dos dois casos de uso foram semelhantes. Entretanto, o WordCount e o que possui o tempo de execuc¸ ´ ao mais ˜
  
(a) WordCount com HDFS: aplicac¸ao termina com menos de 105 s. ˜ 
  
(b) WordCount sem HDFS: aplicac¸ao termina por volta de 163 s. ˜ 
Figura 3. Traces da transferencia de dados durante a execuc¸ ˆ ao do WordCount. ˜ 
dependente do tempo de leitura. O resultado obtido nesse cenario pode representar um ´ indicativo para as outras classes de algoritmos. As regressoes foram feitas coletando ˜ o tempo de execuc¸ao de cada aplicac¸ ˜ ao para diversos tamanhos de arquivo, de 2GB a ˜ 12GB, aumentando 1GB a cada cenario. Para cada tamanho de arquivo, executamos a ´ aplicac¸ao vinte vezes, e computamos o tempo m ˜ edio de execuc¸ ´ ao para cada cen ˜ ario. ´ 
A figura 4 ilustra as medias calculadas para cada cen ´ ario e as regress ´ oes obtidas. ˜ Observamos que a regressao da aplicac¸ ˜ ao utilizando o HDFS possui um comportamento ˜ linear para todos os cenarios, com pouca vari ´ ancia entre as execuc¸ ˆ oes. Por outro lado, ˜ o mesmo nao aconteceu com a aplicac¸ ˜ ao utilizando o sistema de arquivos tradicional ˜ que apresentou um comportamento menos regular. Mesmo assim, as duas regressoes ˜ satisfizeram as premissas de normalidade e de independencia dos erros. ˆ 
A Tabela 2 contem uma sumarizac¸ ´ ao dos resultados obtidos onde, para cada uma ˜ das regressoes, calculamos seus par ˜ ametros e os respectivos coeficientes de determinac¸ ˆ ao. ˜ Alem disso, calculamos o intervalo de confianc¸a de cada um dos par ´ ametros para verificar ˆ o quao significativas s ˜ ao essas regress ˜ oes. ˜ 
Tabela 2. Resultado das regressoes lineares. ˜ 
Informac¸ao com HDFS sem HDFS ˜ Parametro ˆ a 0,0113 0,0287 
Parametro ˆ b 16,4363 -40,1454 Intervalo de Confianc¸a de a (95%) (13,5497, 19,3229) (-67,4188, -12,8720) Intervalo de Confianc¸a de b (95%) (0,0113, 0,0113) (0,0287, 0,0287) R2 0,9939 0,9545
Tempo de execuçãoo (segundos) 
 500  400  300  200  100  0 


	

	f1(x) = 0.0113*x + 16.4363f2(x) = 0.0287*x − 40.1454
	versão sem HDFSversão com HDFS
	

	

	

	

	

	

	

	R2
	= 0.9545
	

	

	

	

	

	

	

	

	

	

	

	

	

	R2
	= 0.9966
	

	

	

	

	

	

	

	



2000400060008000100001200014000 
0
Tamanho do arquivo (MB) 
Figura 4. Tempo de execuc¸ao do WordCount em func¸ ˜ ao do tamanho do arquivo. ˜ 
Assim, para a aplicac¸ao WordCount, a API de integrac¸ ˜ ao com o HDFS n ˜ ao s ˜ o se ´ mostrou mais rapida que o sistema tradicional, como tamb ´ em mais est ´ avel em relac¸ ´ ao ao ˜ aumento dos dados. Podemos ver pela Tabela 2 que o seu coeficiente de determinac¸ao˜ e´ 0,9939 e seus dois parametros s ˆ ao significativos. Tais resultados indicam que a regress ˜ ao˜ representa bem o cenario tratado. Seu par ´ ametro ˆ a (coeficiente angular) e o menor das ´ duas regressoes criadas, combinando com as outras observac¸ ˜ oes de que o HDFS possui ˜ melhor desempenho. Com base apenas nas regressoes geradas, podemos imaginar que ˜ para dados pequenos, utilizar ou nao o HDFS n ˜ ao tem tanto impacto ou talvez seja at ˜ e´ pior; entretanto, ainda assim, a diferenc¸a de tempo e pequena. Por outro lado, o HDFS ´ fornece ganhos significativos nos cenarios com dados massivos. ´ 
5.3. Impacto da integracao com o Lemonade ˜ 
Por fim, o ultimo experimento analisa o impacto do Lemonade sobre o COMPSs. Como j ´ a´ citado, as aplicac¸oes COMPSs criadas a partir do Lemonade utilizam func¸ ˜ oes fornecidas ˜ em uma biblioteca estatica. Dessa forma, em casos espec ´ ´ıficos, uma aplicac¸ao codifi- ˜ cada diretamente, sem utilizar o Lemonade, pode ser criada usando menos sequencias ˆ de tarefas COMPSs (i.e., com maior otimizac¸ao). Para avaliar esse impacto, realizamos ˜ novamente um teste pareado Z, com duas aplicac¸oes equivalentes, s ˜ o que uma delas cons- ´ tru´ıda utilizando o Lemonade e a outra implementada manualmente, de uma forma mais otimizada. Ambas utilizaram o HDFS e cada uma foi executada 40 vezes. A aplicac¸ao˜ escolhida para esse experimento foi o Workflow que, como ja visto, possui uma s ´ erie ´ de tarefas em sequencia, sendo assim, um bom exemplo de situac¸ ˆ ao em que pode haver ˜ alguma otimizac¸ao n ˜ ao identificada pelo Lemonade. ˜ 
A Tabela 3 mostra os resultados obtidos nesse experimento. Podemos ver que para essa configurac¸ao, as duas aplicac¸ ˜ oes possuem tempos de execuc¸ ˜ ao significativamente ˜ diferentes. O intervalo de confianc¸a de um lado (-∞, −19, 1) nos diz que uma aplicac¸ao˜ codificada diretamente, com atenc¸ao para as otimizac¸ ˜ oes poss ˜ ´ıveis, pode ser por volta de vinte segundos mais rapida que uma aplicac¸ ´ ao semelhante constru ˜ ´ıda com o Lemonade. No entanto, em termos de percentuais, esse tempo corresponde a apenas 6% de uma execuc¸ao m ˜ edia (considerando uma execuc¸ ´ ao sem o Lemonade). Logo, essa diferenc¸a ˜
nao˜ e t ´ ao impactante ao levarmos em conta os benef ˜ ´ıcios e praticidades de se utilizar o Lemonade, que libera o usuario da tarefa de desenvolver o programa COMPSs otimizado. ´ 
Tabela 3. Impacto do Lemonade: teste pareado Z (tempos em segundos). Informac¸ao Workflow ˜ 
Numero de amostras 40 ´ 
Media das diferenc¸as -23,2 ´ 
Desvio padrao das diferenc¸as 15,9 ˜ 
Intervalo de Confianc¸a de um-lado (95%) (-∞, −19, 1) 
Tempo de execuc¸ao m ˜ edio (com Lemonade) 317 ´ 
Tempo de execuc¸ao m ˜ edio (sem Lemonade) 292 ´ 
6. Conclusao e Trabalhos Futuros ˜ 
Os avanc¸os nas areas de HPC e ´ Big Data tem levado ˆ a aproximac¸ ` ao das ferramentas e ˜ tecnicas usadas em ambas. Neste trabalho propomos duas novas extens ´ oes ao ambiente ˜ COMPSs, desenvolvido originalmente para a area de HPC, para aumentar seu desem- ´ penho e facilitar sua utilizac¸ao e aplicac¸ ˜ oes ˜ Big Data. Essa extensoes permitem sua ˜ integrac¸ao com o sistema de arquivos distribu ˜ ´ıdos HDFS e com uma ferramenta visual para Data Analytics, ambas open-source. Acreditamos que tais contribuic¸oes ajudar ˜ ao a ˜ popularizar o COMPSs e contribuirao na sua converg ˜ encia com o mundo ˆ Big Data. 
Os resultados obtidos claramente demonstram os benef´ıcios introduzidos pela utilizac¸ao do HDFS. Ela n ˜ ao s ˜ o´ e recomendada pela abstrac¸ ´ ao dos dados, que faz com ˜ que o usuario n ´ ao precise se preocupar com tal divis ˜ ao, como tamb ˜ em pelo ganho de ´ desempenho promovido. Por sua vez, apesar de gerar codigo menos eficiente que um ´ programa cuidadosamente otimizado, o Lemonade possui aspectos como facilidade de utilizac¸ao, interface de usu ˜ ario para criac¸ ´ ao e execuc¸ ˜ ao de fluxos utilizando a funcionali- ˜ dade de arrastar e soltar elementos em uma interface visual que justificam seu uso. Alem´ disso, programadores iniciantes ou avanc¸ados em COMPSs podem utilizar as operac¸oes ˜ e algoritmos implementados para o Lemonade como uma biblioteca externa para suas aplicac¸oes caso n ˜ ao desejem utilizar o Lemonade diretamente. ˜ 
Trabalhos futuros incluem extensoes para novos conjuntos de dados e outros ˜ cenarios com aplicac¸ ´ oes reais. Al ˜ em disso, com relac¸ ´ ao ao HDFS, pretendemos estender ˜ o estudo feito tanto para operac¸oes com escrita de dados sobre o HDFS quanto para uma ˜ nova integrac¸ao, j ˜ a em andamento, usando a linguagem Java. Sobre o Lemonade, preten- ´ demos estudar a possibilidade de gerac¸ao de c ˜ odigo mais din ´ amico, com mais recursos de ˆ otimizac¸ao, de forma que conjuntos de tarefas possam ser agrupados em uma ˜ unica tarefa ´ paralelizada e, com isso, seja poss´ıvel gerar codigo mais otimizado automaticamente. 
Agradecimentos 
Este trabalho foi parcialmente financiado por Fapemig, CAPES, CNPq, e pelos projetos MCT/CNPq-InWeb (573871/2008-6), FAPEMIG-PRONEX-MASWeb (APQ-01400-14), H2020-EUBR-2015 EUBra-BIGSEA e H2020-EUBR-2017 Atmosphere.
Referencias ˆ 
Conejero, J., Corella, S., Badia, R. M., and Labarta, J. (2017). Task-based programming in COMPSs to converge from HPC to Big Data. The International Journal of High Performance Computing Applications, 17. 
Fox, G. et al. (2015). Big data, simulations and HPC convergence. In Workshop on Big Data Benchmarks, pages 3–17. Springer. 
Gonzales, S. D. (2016). PyWebHDFS: a python wrapper for the Hadoop WebHDFS REST API. Dispon´ıvel em: https://pypi.python.org/pypi/pywebhdfs. Acessado em 14/12/2017. 
Kamburugamuve, S., Govindarajan, K., Wickramasinghe, P., Abeykoon, V., and Fox, G. (2017). Twister2: Design of a big data toolkit. In EXAMPI 2017 workshop SC17 Conference, Denver CO. 
Leo, S. and Zanetti, G. (2010). Pydoop: a Python MapReduce and HDFS API for Ha doop. In Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing, pages 819–825. ACM. 
Lezzi, D., Rafanell, R., Lordan, F., Tejedor, E., and Badia, R. M. (2011). COMPSs in the VENUS-C platform: enabling e-science applications on the cloud. In 4th Iberian Grid Infrastructure Conference, volume 1, Braga, Portugal. Universidade do Minho. 
Lordan, F., Ejarque, J., Sirvent, R., and Badia, R. M. (2016). Energy-aware program ming model for distributed infrastructures. In 24th Euromicro Int’l Conf. on Parallel, Distributed, and Network-Based Processing (PDP), volume 24, pages 413–417. 
Reed, D. A. and Dongarra, J. (2015). Exascale computing and big data. Communications of the ACM, 58(7):56–68. 
Rocha, R. C., Hott, B., dos Santos Dias, V. V., Ferreira, R., Jr., W. M., and Guedes, D. (2016). Watershed-ng: an extensible distributed stream processing framework. Con currency and Computation: Practice and Experience, 28(8):2487–2502. 
Rosen, J. (2016). Pyspark internals. Dispon´ıvel em: https://cwiki.apache.org/ confluence/display/SPARK/PySpark+Internals. Acessado em 14/12/2017. 
Santos, W., Carvalho, L. F. M., d. P. Avelar, G., Silva, A., Ponce, L. M., Guedes, D., and Meira, W. (2017). Lemonade: A scalable and efficient spark-based platform for data analytics. In 2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID), pages 745–748. 
Tejedor, E., Becerra, Y., Alomar, G., Queralt, A., Badia, R. M., Torres, J., Cortes, T., and Labarta, J. (2017). PyCOMPSs: Parallel computational workflows in Python. The International Journal of High Performance Computing Applications, 31(1):66–82. 
Wu, X., Kumar, V., Quinlan, J. R., Ghosh, J., Yang, Q., Motoda, H., McLachlan, G. J., Ng, A., Liu, B., Philip, S. Y., et al. (2008). Top 10 algorithms in data mining. Knowledge and information systems, 14(1):1–37. 
Zaharia, M. et al. (2012). Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing. In Proc. of the 9th USENIX Symposium on Networked Systems Design and Implementation (NSDI 12), pages 15–28, San Jose, CA. USENIX.