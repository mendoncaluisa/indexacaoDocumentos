
Combinatorial Optimization
New Frontiers in Theory and Practice
NATO ASI Series
Advanced Science Institutes Series
A series presenting the results of activities sponsored by the NATO Science
Committee, which aims at the dissemination of advanced scientific and
technological knowledge, with a view to strengthening links between scientific
communities.
The Series is published by an international board of publishers in conjunction with
the NATO Scientific Affairs Division
A Life Sciences
B Physics
C Mathematical and
Physical Sciences
o Behavioural and
Social Sciences
E Applied Sciences
F Computer and
Systems Sciences
G Ecological Sciences
H Cell Biology
I Global Environmental
Change
NATo-pea DATABASE
Plenum Publishing Corporation
London and New York
Kluwer Academic Publishers
Dordrecht, Boston and London
Springer-Verlag
Berlin Heidelberg New York
London Paris Tokyo Hong Kong
Barcelona Budapest
The electronic index to the NATO ASI Series provides full bibliographical
references (with keywords and/or abstracts) to more than 30000 contributions
from international scientists published in all sections of the NATO ASI Series.
Access to the NATO-PCO DATABASE compiled by the NATO Publication
Coordination Office is possible in two ways:
- via online FILE 128 (NATO-PCO DATABASE) hosted by ESRIN,
Via Galileo Galilei, 1-00044 Frascati, Italy.
-via CD-ROM "NATO-PCO DATABASE" with user-friendly retrieval software
in English, French and German (© WTV GmbH and DATAWARE Technologies
Inc. 1989).
The CD-ROM can be ordered through any member of the Board of Publishers
or through NATO-PCO, Overijse, Belgium.
Series F: Computer and Systems Sciences Vol. 82
Combinatorial Optimization
New Frontiers in Theory and Practice
Edited by
Mustafa AkgOI
Department of Industrial Engineering
Bilkent University
Ankara 06533, Turkey
Horst W. Hamacher
Fachbereich Mathematik
Universitat Kaiserslautern
Erwin-Schroedinger-StraBe
W-6750 Kaiserslautern, FRG
SOleyman TOfekc;i
Department of Industrial and Systems Engineerina
University of Florida
303 Weil Hall
Gainesville, FL 32611-2083, USA
Springer-Verlag
Berlin Heidelberg New York London Paris Tokyo
Hong Kong Barcelona Budapest
Published in cooperation with NATO Scientific Affairs Division
Proceedings of the NATO Advanced Study Institute on New Frontiers in the Theory
and Practice of Combinatorial Optimization, held in Ankara, Turkey, July 16-29,
1990.
ISBN-13: 978-3-642-77491-1 e-ISBN-13: 978-3-642-77489-8
001: 10.1007/978-3-642-77489-8
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is concerned.
specifically the rights of translation. reprinting, re-use of illustrations, recitation, broadcasting, reproduction on
microfilm or in any other way, and storage in data banks. Duplication of this publication or parts thereof is per-
mitted only under the provisions of the German Copyright Law of September 9, 1965, in its current version, and
permission for use must always be obtained from Springer-Verlag. Violations are liable for prosecution under
the German Copyright Law.
© Springer-Verlag Berlin Heidelberg 1992
Softcover reprint of the hardcover 1st edition 1992
Typesetting: camera ready by authors
45/3140-54321 0 - Printed on acid-free-paper
Preface
There have been significant developments in the theory and practice of combinatorial
optimization in the last 15 years. This progress has been evidenced by a continuously
increasing number of international and local conferences, books and papers in this area.
This book is also another contribution to this burgeoning area of operations research and
optimization.
This volume contains the contributions of the participants of the recent NATO Ad-
vanced Study Institute, New Frontiers in the Theory and Practice of Combinatorial Op-
timization, which was held at the campus of Bilkent University, in Ankara, Turkey, July
16-29, 1990. In this conference, we brought many prominent researchers and young and
promising scientists together to discuss current and future trends in the theory and prac-
tice of combinatorial optimization. The Bilkent campus was an excellent environment for
such an undertaking. Being outside of Ankara, the capital of Turkey, Bilkent University
gave the participants a great opportunity for exchanging ideas and discussing new theories
and applications without much distraction.
One of the primary goals of NATO ASIs is to bring together a group of scientists
and research scientists primarily from the NATO countries for the dissemination of ad-
vanced scientific knowledge and the promotion of international contacts among scientists.
We believe that we accomplished this mission very successfully by bringing together 15
prominent lecturers and 45 promising young scientists from 12 countries, in a university
environment for 14 days of intense lectures, presentations and discussions.
The subjects covered in this book illustrate the importance and diversity of the area of
combinatorial optimization. In the theoretical area of combinatorial optimization, we have
papers and extended abstracts dealing with facet lifting cuts, polyhedral combinatorics,
general decomposition in mathematical programming, scheduling theory, graph theory,
maximization of submodular functions and traveling salesman problems. In the area of
application of combinatorial optimization methodology to decision problems, we have
papers and extended abstracts dealing with computerized tomography, emergency area
VI
evacuations, school timetabling, cost allocation in oil industry, robotic assembly problems
in electronics industry, telecommunications network design, vehicle routing, traffic control
at intersections, cutting stock problems, machine loading in FMS environment, and VLSI
layout problems.
We would like to thank the Scientific Affairs Division of the North Atlantic Treaty Or-
ganization for their generous support which made this Advanced Study Institute possible.
We also would like to extend our sincere thanks and gratitude to The Honorable Professor
Dr. ihsan DogramaCl, the chairman of the Council of Higher Education of Turkey and the
founder and the chairman of the board of directors of Bilkent University, and to Professor
Dr. Mithat CJoruh, the Rector of Bilkent University for allowing us to use all required
resources at the Bilkent University at a moment's notice for the entire duration of the
Institute. Their contribution to the success of this ASI has been fundamental and most
generous. We would like to thank the Department of Industrial and Systems Engineering
at the University of Florida, in Gainesville, Florida and to the Fachbereich fur Mathematik
at the University of Kaiserslautern, Germany, for their contributions before, during and
after this AS!. Finally we would like to thank two graduate students, Bassam E1Khoury
and George Vairaktarakis of the Department of Industrial and Systems Engineering at
the University of Florida for their help in writing this proceedings volume in LaTeX. It
was a learning experience for all of us.
December 1991 Mustafa Akgul
Horst W. Hamacher
Suleyman Tufek<;i
Table of Contents
Variable Decomposition, Constraint Decomposition and
Cross Decomposition in General Mathematical Programming.
Olaf E. Flippo, Alexander H. G. Rinnooy Kan
Surrogate Constraint Methods for Linear Inequalities .
Kai Yang, Katta G. Murty
An Evaluation of Algorithmic Refinements and Proper Data Structures
1
19
for the Preflow-Push Approach for Maximum Flow .................... 39
U. Derigs, W. Meier
A Cutting Plane Algorithm for the
Single Machine Scheduling Problem with Release Times ................. 63
G. L. Nemhauser, M. W. P. Savelsbergh
The Linear Assignment Problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
Mustafa Akgiil
Cost Allocation in the Oil Industry: An Example .................... 123
Kurt O. Jyfrnsten
On Preference Orders for Sequencing Problems
01', What Hath Smith Wrought? .............................. 133
E. L. Lawler
VIII
Dynamic Basis Partitioning for Network Flows
with Side Constraints ...................................... 161
Wonjoon Choi, Siileyman Tiifekr,;i
Combinatorial Optimization Models Motivated by
Robotic Assembly Problems.
Horst W. Hamacher
Job Shop Scheduling.
J. K. Lenstra
. ................... 187
199
On the Construction of the Set of K-best Matchings
and Their Use in Solving Constrained Matching Problems ................ 209
U. Derigs, A. Metz
Solving Large Scale Multicommodity Networks Using
Linear-Quadratic Penalty Functions . .
Mustafa r;. Pmar, Stavros A. Zenios
225
An Analysis of the Minimal Spanning Tree Structure ................... 231
Elzbieta Trybus
Genetic Algorithms: A New Approach to
the Timetable Problem. . . . . . . . . ..
Alberto Colomi, Marco Dorigo, Vittorio Maniezzo
A New Approximation Technique for
. ................... 235
Hypergraph Partitioning Problem. . ........................... 241
Scott W. Hadley
Optimal Location of Concentrators in a
Centralized Teleprocessing Network.
M. G. de Oliveira
IX
A Column Generation Algorithm for the
Vehicle Routing Problem with Time Windows.
Martin Desrochers, Jacques Desrosiers, Marius Solomon
The Linear Complementarity Problem,
245
249
Sufficient Matrices and the Criss-Cross Method. . . . . . . . . . . . . . . . . . . . . . . 253
D. den Hertog, C. Roos, T. Terlaky
A Characterization of Lifted Cover Facets of
Knapsack Polytope with GUB Constraints ..
George L. Nemhauser, Gabriele Sigismondi, Pamela Vance
On Pleasant Knapsack Problems.
Bela Vizvari
Extensions of Efficient Exact Solution Procedures to
Bicriterion Optimization .
Yasemin Aksoy
Combinatorial Aspects in Single Junction
259
263
269
Control Optimization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
Giuseppe Bruno, Gennaro Improta
Approximation Algorithms for Constrained Scheduling .................. 275
Leslie A. Hall, David B. Shmoys
x
An Analogue of Hoffman's Circulation Conditions
for Max-Balanced Flows ..................................... 279
Mark Hartmann, Michael H. Schneider
Some Telecommunications Network Design Problems and
the Bi-Steiner Problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283
Geir Dahl
Parallel Machine Scheduling to Minimize Costs for
Earliness and Number of Tardy Jobs.
H. G. Kahlbacher, T. C. E. Cheng
. ................... 287
Exact Solution of Multiple Traveling Salesman Problems. . . . . . . . . . . . . . . . . 291
J. Gromicho, J. Paixiio, 1. Bronco
A Nonlinear Two-Stage Cutting Stock Problem ....................... 293
J. M. Valerio de Carvalho, A. J. Guimariies Rodrigues
The Probabilistic Behavior of the Generalized HARMONIC
Algorithm for the On-Line, Multi-Dimensional Bin Packing ............... 295
J. Csirik, E. Mati
Efficient Labelling Algorithms for
the Maximum Noncrossing Matching Problem ....................... 299
Federico Malucelli, Daniele Pretolani
A Phase I That Solves Transportation Problems. . . . . . . . . . . . . . . . . . . . . . 303
Konstantinos Paparrizos
XI
A Polynomially Bounded Dual Simplex Algorithm
for Capacitated Minimum Cost Flow Problem.
Canan A. Sepil, Ay§egiil Altaban
. ................... 307
Formulation and a Lagrangean Relaxation Procedure, for
Solving Part Scheduling and Tool Loading Problems, in FMS ..
[(annan Sethuraman. Marshall L. Fisher, Alexander H. G. Rinnooy [(an
Euclidean Steiner Minimal Trees with Obstacles and
. ..... 309
Steiner Visibility Graphs .
Pawel Winter
. .................. 313
A Set Covering Formulation of the Matrix Equipartition Problem ............ 317
S. Nicoloso, P. Nobili
Maximizing a Submodular Function by
Integer Programming: A Polyhedral Approach ....................... 321
Heesang Lee, George L. Nemhauser
New Bounds for the Asymmetric Traveling Salesman Problem .............. 323
A. I. Barros, P. Barcia
A Lagrangean Heuristic for Set Covering Problems. . . . . . . . . . ........... 325
J. E. Beasley
Subject Index ........................................... 327
Variable Decomposition, Constraint
Decomposition and Cross Decomposition in
General Mathematical Programming
Olaf E. Flippo*
Alexander H. G. Rinnooy Kant
1 Introduction
Decomposition has been recognized as a fundamental technique in optimization ever since
the seminal papers of Benders (1962) and Dantzig & Wolfe (1960). The literature abounds
with theoretical extensions of these two basic approaches, as well as with reports of suc-
cessful applications (for references, see Flippo (1991)). In this paper, Variable and Con-
straint Decomposition will be introduced as proper generalizations of Benders Decompo-
sition and Dantzig-Wolfe Decomposition respectively. Under fairly mild conditions, these
methods can be prevented from exhibiting cyclic behavior, and under much stronger con-
ditions, they can be proven to converge asymptotically or even finitely. Special attention
will be paid to Cross Decomposition, which is intermediate between the two decomposition
methods mentioned above. A number of convergence criteria will be shown to prevent
the procedure from cycling, or even to enforce asymptotic or finite convergence.
2 Duality Theory
In this section, we briefly recall the main concepts and results from the Tind-Wolsey
duality theory (1981) that will be required below.
'currently with Royal/Shell-Laboratory, Amsterdam, P.O.Box 3003, 1003 AA Amsterdam, The
Netherlands.
tEconometric Institute, Erasmus University Rotterdam, P.O.Box 1738, 3000 DR Rotterdam, The
Netherlands.
This study was supported by the Netherlands Foundation for Mathematics (SMC) with financial aid from
the Netherlands Organization for Scientific Research (NWO).
NATO ASI Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgiil et aJ.
© Springer·Verlag Berlin Heidelberg 1992
2
Consider the following primal program
P: maxx f(x)
s.t. G(x) 00
xEX
(1)
(2)
(3)
where X s;;: W, 0 E {~,=}m, f: X -t IR U {±oo} and G
(G1, ... ,Gm)T.
X -t IR m with G =
The value function (perturbation function) v: IR m -t IR U {±oo} is defined as
{ suPx{j(x) I G(x) or, x E X} if r E RHS
v(r) =
-00 otherwise
with RHS = {r E IRm 13x EX: G(x) or} denoting the set of feasible right-hand-sides.
If P is regular, i.e. if its value cp(P) = v(O) is not equal to ±oo, and if for t 2 0,
f(x) 2 v(O) - t for some feasible solution x, then x is called an t-optimal solution to P.
To describe the dual program D, we introduce the set of functions
:F = {g: IRm -t IR U {±oo} I g(r) ~ g(r') Vr,r' E RHS: r or'} (4)
and define
D: ming g(O)
s.t. g(G(x)) 2 f(x) "Ix E X
9 E:F
(5)
(6)
(7)
It is easily verified that g.E :F satisfies (6) if and only if g(r) 2 v(r) Vr E IRm. Taking
r = 0, one obtains weak duality, which states that every feasible dual solution yields an
upper bound to the optimal value of the primal program. By observing that v itself is
feasible for D, one obtains strong duality, which says that the optimal primal and dual
objective values meet. Finally, by defining po as the primal problem with f(x) in (1)
being replaced by 0, and applying weak and strong duality to po and its dual DO, one
obtains that
3x EX: G(x) 00 {::} h(O) 20 Vh E:F: [h(G(x)) 2 0 "Ix E Xl
which is a proper generalization of the Farkas Lemma.
Clearly, the major drawback of this general duality theory is the asymmetry between P
and D, in that the primal solution space X is finite dimensional, whereas the dual solution
space :F is not. If one restricts :F to the (finitely parametrizable) set of affine functions
for instance, one recovers the Lagrangean Dual which properly generalizes Linear and
Quadratic Programming Duality.
3
3 Variable Decomposition
We are now ready to introduce Variable Decomposition, one of the two main decomposi-
tion approaches and a direct generalization of techniques that have been proposed under
names such as (Generalized) Benders Decomposition, Primal Decomposition and Resource
Directive Decomposition.
Let us assume that P can be written as:
P: maxx,y f(x,y)
s.t. G(x,y) 00
(x,y)EUn(XxY)
(x, y) is feasible for P =?
3y' E Y: [(x, y') is feasible for P /\ f(x, y') > -00]
(8)
(9)
(10)
(11)
By setting x = x E X, i.e. by projecting on the x-variables, we obtain a family of
parametrized primal subprograms
P(x): maxx,y f(x,y)
s.t. G(x,y)oO
x=x
(x,y)EUn(XxY)
(12)
(13)
(14)
(15)
where X is any superset of X that is independent of x. The original program P may then
be written as
max;; cp(P(x))
s.t. cp(pO(x)) ~ 0
x EX
(16)
(17)
(18)
where PO(x) and cp(.) are defined as in Section 2, and (17) ensures restriction to those
x E X that can be extended to a feasible primal solution (x, j)).
Dualization of P(x) now yields
1)(x): ming g(O, x) (19)
s.t. g(G(x,y),x) ~ f(x,y) V(x,y) E Un (X x Y) (20)
g E:F (21)
4
The dual solution space :F is defined similarly as in Section 2:
:F = {g : IRm+n, --+ IR U {±oo} I
g(r,x) :::; g(r',x) V(r,x), (r',x) E RHS: r 0 r'}
where RHS is the set of feasible right-hand-sides, here defined as
RH S = {(r, x) E IRm+nl I
3(x,y) E un (X x Y): [G(x,y) Or 1\ x = xl}
Thanks to strong duality, (16)-(18) can now be replaced by
max", cp(V(x))
s.t. cp(VO(x)) ~ 0
xEX
(22)
(23)
(24)
where VO(x), the dual of PO(x), is obtained from (19)-(21) by replacing f(x,y) by O. A
trivial reformulation of (22)-(24) renders
max""e ()
s.t. -cp(V(x)) + () :::; 0
-cp(VO(x)) :::; 0
(x,()) E X x IR
Now, if we let b. denote the feasible set of V(x) defined by (20)-(21), and similarly, if
we let b.° denote the feasible set of va (x), then outer app1'Oximation yields the master
program
s.t. -g(O,x)+():::;O VgEb.
-h(O,x) :::;0 VhEb.°
(x,()) E X x IR
(25)
(26)
(27)
(28)
which, under (11), is clearly still equivalent to P. Here, the constraints in b. can be
thought of as value cuts and the constraints in b. ° as feasibility cuts.
5
In (25)-(28), the y-variables have been eliminated at the expense of a great increase
in the number of constraints. Thus, one would typically solve V'D(.6., .6.0) by relaxation,
i.e. by considering a relaxed master program V'D(.6.,~) for appropriately chosen subsets
.6. ~ .6., ~ ~ .6.0. It is easily verified that for suitable choices of .6. and ~, solutions
to V'D(.6.,~) can provide arbitrarily tight upper bounds on 'P(P), whereas for suitably
chosen x, optimal solutions to P(x) can provide arbitrarily tight lower bounds. Thus,
one naturally arrives at the notion of an iterative procedure in which the relaxed master
programs generate candidate x-values, which, through 'D(x) or 'D°(x), yield value or
feasibility cuts that can be used to extend .6. or ~. The following lemma summarizes
the essential ingredients for such a procedure; for proofs of this and subsequent results in
this section we refer toFlippo (1991).
Lemma 3.1 Assume that 'P (V'D(.6.,~)) < +00.
3.1.1 If'P (V'D(.6.,~)) is infeasible, then so is P.
- -...,...0
3.1.2 If (x, ()) is feasible for V'D(.6.,.6. ), then
- ...,...0-
• P(x) is infeasible {:} 3h E .6.0 \.6. : -h(a, x) > a
3.1.3 If (x, 8) is t1-optimal for V'D(.6., ~), Y is t2-optimal for P(x), and 9 E .6. is t3-
optimal for 'D(x), then (x, y) is (t1 + t2 + (3)-optimal for P.
We are now in a position to describe the Variable Decomposition Procedure. Let the
superscript k denote an iteration index, and let U Bk, LBk and (xinc,k, yinc,k) be the
values of the best upper bound on 'P(P), best lower bound on 'P(P), and best feasible
solution to P found so far.
IVARIABLE DECOMPOSITION PROCEDURE - START I
10. INITIALIZATION PHASE I
LET k:= 1;
LET .6. k :~ .6., .6.O,k :~ .6.° be such that 'P (V'D(.6. k,.6. O,k)) < +00;
LET U BO := +00, LBO := -00;
6
11. MASTER PROGRAM PHASE I
IF r.p (VD(6.k, 6. 0 ,k)) = -00 THEN STOP [P is infeasible]
ELSE BEGIN
LET (xk, Ok) be an t~-optimal solution to V"D(6. k, 6. 0 ,k);
LET UB k :=min{UBk-l,Ok+tn
END;
1'-2-.-S-U-B-P-R-O-GR-A-M-P-H-A"""-SE-'1
IF r.p(P(xk )) = -00 THEN BEGIN
END
LET t~ := t~ := 0, LBk := LB k-\
LET hk E 6.° be such that _hk(O, xk) > 0;
LET 6. k ~: 6. k+1 :~ 6.;
LET 6. 0 ,k U {hk} ~: 6. 0,k+l :~ 6.0
ELSE BEGIN
LET yk be an t~-optimal solution to P(xk );
LET gk be an t~-optimal solution to D(xk);
LET 6. 0 ,k ~: 6. 0 ,k+1 :~ 6.0;
LET LBk := max{LBk-l ,J(xk, yk)};
IF LBk > LB k- 1 THEN LET (xinc,k, yinc,k) := (xk, yk)
ELSE LET (xinc,k, yinc,k) := (xinc,k-l, yinc,k-l )
END;
r-13-.-OP-T-'I-M-A-L-IT-Y-V-ER-I-F-IC-A-T-I-ON--'I
LET t~ :2 t~ + t~ + t~;
IF UBk - LBk :s t~ THEN STOP [(xinc,k,yinc,k) is t~-optimal for P]
ELSE LET k:= k + 1;
RETURN TO 1;
I VARIABLE DECOMPOSITION PROCEDURE - ENDI
7
The following comments may be helpful.
• Finding initial sets 6.1 and 6.0,1 with r.p (V'D(6.t, 6.0,1)) < +00 may be a non-trivial
task. If such sets do not exist, then clearly r.p(P) = +00.
• In the subprogram phase, hk exists, since it is implied by (11) that
r.p(P(x k)) = -00 {:> r.p(pO(xk)) = -00.
• In the optimality verification phase, (xinc,k, yinc,k) is (U Bk - LBk)-optimal for P,
because 0 :::; r.p(P) - f(xinc,k,yinc,k) :::; UB k - LBk. This solution is considered
accurate enough, if it meets the bound E~ on the overall inaccuracy.
• This framework allows for inaccuracies during the iterative process as well as for
duality gaps between the primal and dual subprograms, since these gaps are incor-
porated in the values for E;.
We now establish a few of the crucial properties of the procedure.
Theorem 3.1 (Non-cyclicity of master program solutions)
3.1.1 In the master program phase, no solution (x\ Ok) will be generated more than once.
3.1.2 In the subprogram phase, no solution hk will be generated more than once, and as
soon as a solution gk is generated for a second time, the algorithm will terminate.
Theorem 3.2 (Non-cyclicity of subprograms) If for all k satisfying
r.p(P(x k)) > -00, E~ :::: E~ + max1::;j::;d~ + ~}, then the procedure terminates as soon
as a solution xk is generated for the second time.
Under much stronger conditions, the procedure can even be shown to terminate after
a finite number of steps.
Theorem 3.3 (Finite convergence) If for all k with r.p(P(x k)) > -00, t~ :::: E~ +
max1::;i::;d ~ + ~}, then the procedure terminates in a finite number of steps if at least
one of the following two conditions is met:
3.3.1 every xk generated belongs to a finite subset of X;
3.3.2 every gk and hk generated belong to a finite subset of 6. and 6.° respectively.
Thus, the general procedure has the properties typically associated with its special
cases. As we shall see below, its asymptotic convergence can also be guaranteed, once
certain conditions are verified. Let us first focus on the sequence of feasible solutions
xk E FS x = {x E X I r.p(P(x)) > -oo} as generated by the algorithm, and let us
8
view the generation of this feasible sequence {X k IkE I ~ IN} as the result of repeated
application of the composed point-to-set map 00 (3, where
o:~-+-tXxIR
with o(g) = {(x, 0) E X X IR I -g(O, x) + 0:::; O}
and
(3: FSx x IR+ -t-t ~
with (3(x, (3) = {g E ~ I 9 is f3-optimalfor D(x)}
Of course, if feasibility rather than value cuts have to be added to a relaxed master
program (a process that can be described similarly in terms of point-to-set maps), then
the generation of the sequence of feasible solutions is (temporarily) interrupted.
Let us assume that III = +00, i.e. that the procedure continues to generate feasible
solutions (xk,yk), and that P is regular. To ensure asymptotic convergence, we shall
require 00(3 to be a closed point-to-set map (d. Zangwill (1969)), and we shall have to
impose certain continuity and compactness conditions. In the following theorem, these
conditions are made precise.
Theorem 3.4 (Asymptotic convergence) Assume that P is regular and that III
+00. Furthermore, assume that
• Un (X x Y) is compact,
• f is upper semi-continuous on Un (X x Y),
• Gi is lower semi-continuous on Un(X xY) in the case that 0i E {:::;}, and continuous
on Un (X x Y) in the case that 0i E {=},
• cp(P(·)) is lower semi-continuous on FS x ,
• Vk E I: ff E Ei ~ IR+ with Ei compact (i = 1,2,3), and
• 0 0 (3 is a closed point-to-set map on F S x X E 3 •
Then the Variable Decomposition Procedure converges asymptotically, in the sense that:
3.4.1 every accumulation point of (xkh is a lim sup( f~+f~)-optimal solution to (16)-(18),
3.4.2 every accumulation point of (x\ykh is a limsup(f~ + f~ + f~)-optimal solution to
P,
9
3.4.3 every accumulation point of (xinc,kh is a limsup(f~ + f~ + f~)-optimal solution to
(16)-(18), and every accumulation point of (xinc,k, yinc,kh is a lim sup ( f~ + f~ + f~)
optimal solution to P, and
3.4.4 0::; lim(UBk - LBk) ::; limsup(f~ + f~ + f~).
We note that Theorem 3.4.4 implies that if f~ is chosen to exceed f~ + f~ + f; by a
fixed, strictly positive amount independent of k, convergence to an €~-optimal solution
can be guaranteed in a finite number of steps as soon as the procedure will no longer be
interrupted by the generation of feasibility cuts.
It is natural to explore next what can be said about convergence if a finitely parame-
terizable representation of the dual space is available, the more so since without such an
assumption there is little hope of computing optimal dual solutions in the first place.
Theorem 3.5 (Closedness of a 0 (3) Suppose that X is closed and that cp(P(·)) is up-
per semi-continuous on F Sx. In addition, let there be a non-empty and compact set
T ~ IRT (7 E IN) and a function w : T x IRm+nl ~ IR U {±oo} which is continuous on
T x {(r, x)} \I(r, x) E RHS and on T x {O} x FS x , which is upper semi-continuous on
T x {O} xX, and which satisfies
\I(x, (03) E FS x x E3: [(z,(}) E (ao,B)(x'€3) '*
:It E T: w (t, . , .) E ,B ( x, (03) /\ (z, (}) E a( w (t, . , .)) 1
Then a 0 ,B is closed on F Sx x E3.
We can use a similar approach to investigate the case in which the algorithm is only
able to generate a finite number of feasible solutions from FS x (i.e. III < +00). If
the feasible set of VO can be restricted to finitely parametrizable solutions, then, under
appropriate assumptions, one can show that every accumulation point of (Xk)IN\I belongs
to F Sx, so that at least asymptotic feasibility is assured.
4 Constraint Decomposition
The Constraint Decomposition approach that we are about to introduce next, generalizes
Dantzig-Wolfe Decomposition in much the same way in which Variable Decomposition
generalizes Benders's original work. The idea underlying the approach is also known
as Column Generation, Generalized Linear Programming, (Generalized) Dantzig- Wolfe
Decomposition, Dual Decomposition and Price Directive Decomposition.
Let us consider the original primal program (1)-(3) under the additional assumption
that
\Ix EX: f(x) < +00 (29)
10
Consider, for all X ~ X, the following dual pair of programs:
1'(X) : max", f(x) (30)
s.t. G(x) 00 (31)
xEX (32)
1>(X) : ming g(O) (33)
s.t. g(G(x)) ~ f(x) \Ix E X (34)
gEr (35)
where r consists of all functions in :F with the exception of those that can take on the
value -00 (d. (4)); this does not affect strong duality between 1'(X) and V(X), provided
the value of the former is strictly greater than -00 (seeFlippo (1991)).
1'(X) and V(X) are clearly equivalent to l' and 1>, and are called the primal and dual
master program respectively; 1'(X) and V(X) are the restricted primal and relaxed dual
master programs respectively. For suitably chosen X ~ X, the latter programs provide
arbitrarily tight lower bounds on c.p(1'). For given 9 E r, an upper bound will be provided
through a generalized version of Lagrangean Relaxation:
C1>(g): max", f(x) + g(O) - g(G(x))
s.t. x E X
Indeed, if 9 E rand g(O) E IR, then
c.p(C1>(g)) = sup",{f(x) + g(O) - g(G(x)) I x E X} ~
sup",{f(x) + g(O) - g(G(x)) I G(x) 00, x E X} ~
sup",{f(x) I G(x)oO, x EX} = c.p(1')
If, moreover, 9 is f-optimal for 1>, then (d. (34))
c.p (CV(g)) = g(O) + sup",{f(x) - g( G(x)) I x E X} ~
g(O) ~ c.p(1)) + f = c.p(1') + f,
so these upper bounds can also be as tight as desired. Note that (29) and 9 E r ensure
that f(x) - g(G(x)) is always well defined.
It is now natural to consider an iterative procedure, in which the relaxed dual master
programs 1>(X) generate candidate g's, which, through CV(g), yield improvements to X.
We describe this Constraint Decomposition Procedure now, using the same notation as
in Section 3.
11
ICONSTRAINT DECOMPOSITION PROCEDURE - START I
10. INITIALIZATION PHASE I
LET k:= Ij
LET X k :~ X be such that c,o(p(Xk)) > -OOj
LET U BO := +00, LBo := -OOj
11. MASTER PROGRAM PHASE \
IF c,o(p(Xk)) = +00 THEN STOP [c,o(P) = +00]
ELSE BEGIN
LET xk be an E~-optimal solution to p(Xk)j
LET gk be an E~-optimal solution to V(Xk)j
LET LBk:= max{LBk-\f(xk)}j
IF LBk > LBk-l THEN LET xinc,k := xk
ELSE LET xinc,k := xinc,k-l
ENDj
r-12-.-SU-B-P-R-O-G-R-A-M-P-H-A-SE-'1
IF c,o (CV(l)) = +00 THEN BEGIN
END
LET zk E X be such that f(zk) + gk(O) - gk(G(zk)) > gk(O)j
LET X k u {zk} ~: Xk+l :~ X
ELSE BEGIN
LET zk be an E~-optimal solution to CV(l)j
LET Xk u {zk} ~: Xk+l :~ Xj
LET UB k := min{UBk-\f(zk) + l(O) - gk(G(zk)) + En
END;
13. OPTIMALITY VERIFICATIONI
LET f~ :2:: ft + f~ + f~;
12
IF UB k - LBk:::; f~ THEN STOP [(xinc'\yinc,k) is f~-optimal for P]
ELSE LET k:= k + 1;
RETURN TO 1;
1 CONSTRAINT DECOMPOSITION PROCEDURE - END 1
A few comments are in order.
• Finding an initial Xl ~ X for which cp(p(Xl)) > -00, may be a non-trivial task.
If no such set exists, then clearly cp(P) = -00.
• Since f(zk) + gk(O) - gk(G(zk)) + f~ is an upper bound on cp (cv(l)) , it is also one
on cp(P).
• In the optimality verification phase, xinc,k is (U Bk - LBk)-optimal for P, because
o :::; cp(P) - f(xinc,k) :::; UB k - LBk. Termination occurs if the bound f~ on the
overall inaccuracy is met.
• Optimality verification in this procedure is essentially verification of dual optimality.
To see why, consider the most recent improvement in the upper bound up until
iteration k (say, in iteration j :::; k), and define 9 by
Note that UBi-I> UBi = UB k = g(O). Now, ~-optimality of zj can easily be seen
to imply that f(x) - g(G(x)) :::; 0 \Ix E X, and it follows readily that 9 is feasible
for V. Furthermore, g(O) - cp(V) :::; UB k - LBk, so that if UB k - LBk :::; f~, then
9 clearly must be f~-optimal for V .
• As in the previous section, this framework allows for inaccuracies during the iterative
process, as wdl for duality gaps between the primal and dual master programs that
are encountered during the solution procedure, since these gaps are incorporated in
the values for f~.
As in the case of Variable Decomposition, we easily obtain non-cyclicity and finite
convergence under appropriate assumptions. Proofs of these statements can be found
inFlippo (1991).
Theorem 4.1 (Non-cyclicity of master program solutions)
4.1.1 In the master program phase, no solution gk will be generated more than once.
13
4.1.2 As soon as in the subprogram phase a solution zk is generated for a second time,
the algorithm will terminate.
To establish the equivalent of Theorem 3.2, let us call two functions g and g' essentially
identical, if their difference is a constant. Clearly, CV(g) is then equivalent to CV(g').
Theorem 4.2 (Non-cyclicity of subprograms) Assume that a real-valued upper bound
has been obtained at iteration ko, and that from that iteration onwards
• zk is chosen so as to ensure that f(zk)+gk(O)-gk( G(zk)) ;:::: U Bk in case cp (CV(gk)) =
+00, and
Then the procedure terminates as soon as a solution gk is generated which is essentially
identical to a previously generated solution gi (k > j ;:::: ko).
Theorem 4.3 (Finite convergence) The procedure terminates m a finite number of
steps if at least one of the following two conditions is met:
4.3.1 every zk generated in the subprogram phase belongs to a finite subset of X j
4.3.2 the assumptions from Theorem 4.2 are satisfied and, in addition, every gk generated
in the master program phase is essentially equivalent to a member of a finite subset
ofr.
Theorem 4.4 (Asymptotic convergence) Assume that
• P is regular,
• X is compact,
• f and G are continuous on X,
• Vk: E7 E Ei ~IR+ with Ei compact (i = 1,2,3), and
• there is a non-empty and compact set T ~ IW (7 E IN) and a function w : T x
IRm -+ IR u {±oo} which is continuous and real-valued on T X RHS, such that
w( t, .) E r Vt E T and such that if gk is generated in the master program phase,
then::ltkET: w(tk,.)=gk.
Then the Constraint Decomposition Procedure converges asymptotically, in the sense that
14
4.4.1 for every accumulation point tOO of (tk)IN, w(too ,.) + E~ is an (/::2' + E~)-optimal
solution for 1)(X) for some accumulation point (E2', E~) of (E~, E~)IN;
4.4.2 every accumulation point of (Xk)IN is a lim sup( E~ + E~ + E~) -optimal solution to P;
4.4.3 every accumulation point of (xinc,k)IN is a limsup( E~ + E~ + E~)-optimal solution to
P;
As in Section 3, finite convergence to an E~-optimal solution can be assured if under the
above conditions, E~ exceeds E~ + E~ + E~ by a strictly positive constant indepent of k.
Under these conditions one can also establish the equivalent of a well known property
of Lagrangean Relaxation, in that every accumulation point ZOO of (zk)IN is a lim sup E~
optimal solution to the program P in which the right-hand-side has been changed from 0
to G(Zoo) (for details, see Flippo (1991)).
5 Cross Decomposition
From the outline presented in Section 3 and 4 the similarity between the variable and
the constraint decomposition approach is obvious. In fact, the methods can be proven to
be dual to each other, in the sense that the latter applied to a mathematical program is
equivalent to the former applied to a well chosen dual (d. Flippo, (1991)).
This dual relation suggests that mixtures of the two approaches ought to be feasible;
and indeed they are. A particularly interesting example is a generalized version of Cross
Decomposition (Van Roy, 1980, 1983), which applies whenever the primal program would
allow both types of decomposition:
P: maxx,y f(x,y)
s.t. G(x,y)oO
(x, y) E U n (X x Y)
Here, Variable Decomposition would produce a subprogram
P(x): maxx,y f(x,y)
s.t. G(x,y)oO
x=x
(x, y) E U n (X x Y)
15
with, as its dual,
'D(x): mina 0"(0, x)
s.t. O"(G(x,y),x) ~ f(x,y)
V(x,y) E un (X x Y)
0" E r.
Constraint Decomposition on the other hand, would produce subprograms of the form:
C'D(g): maxx,y f(x,y) + g(O) - g(G(x,y))
s.t. (x,y)EUn(XxY).
In the regular implementations, the subprograms would supply the input for their respec-
tive master programs. But since the latter are usually much harder to solve, it is tempting
to iterate between the two subprograms instead, by using the solution a- for 'D(x) to define
a g, and using the solution of C'D(g) to set up the next version of'D(x). This is exactly
what Cross Decomposition does. As before, P(x) provides lower bounds on c.p(P), C'D(g)
provides upper bounds, and thus the usual termination criterion applies.
This attractive simplification, however, comes at a price; non-cyclicity, let alone con-
vergence, cannot be guaranteed. The intuitive reason for this is that the subprograms
rely only on recently obtained information, whereas the master programs accumulate all
information over time. Thus, at least an occasional call on a full master program is re-
quired to guarantee convergence for Cross Decomposition. Below some strategies will
be presented under which the (modified) Cross Decomposition procedure indeed has the
same desirable properties as the pure Variable or Constraint Decomposition procedures.
Strategy 1 Suppose the Cross Decomposition phase IS entered for the
k-th time. As soon as Sk Subprograms (Sk E IN) have been solved, at least one
full iteration of the Variable (c.q. Constraint) Decomposition Procedure will be
performed.
Under the appropriate conditions on t7, this algorithm will obviously not cycle. Moreover,
if the Variable (c.q. Constraint) Decomposition Procedure is guaranteed to terminate
after a finite number of steps, this hybrid algorithm will do so too. Since mild conditions
exist that prevent the Cross Decomposition procedure from iterating between two distinct
subprograms only, it is plausible to choose Sk ~ 4 (cf. Flippo et al. (1987)).
Strategy 2 Suppose the Cross Decomposition phase is entered for the k-th time. As
soon as tk subprograms (tk E IN) have been solved during which neither the best
16
upper nor the best lower bound found so far, have been subject to any change, at
least one full iteration of the Variable (c.q. Constraint) Decomposition Procedure
is performed.
This algorithm too, will not cycle under the appropriate conditions on tf. For similar
reasons as in Strategy 1, it is plausible to choose tk ~ 4. However, contrary to the first
strategy, finite convergence cannot always be assured, since one may be trapped in the
Cross Decomposition phase forever.
Strategy 3a Let the following conditions be simultaneously satisfied.
(i) The Cross Decomposition phase is not entered before a real-valued lower bound
LB is known.
(ii) Each time a regular subprogram P(x) is under consideration, an El-optimal
dual solution (f for Vex) is generated, and the lower bound is updated according to
LB := max{LB, (f(0, x) - Ed.
(iii) At least one full iteration of the Variable Decomposition Procedure is carried
out as soon as a solution x is under consideration which satisfies
cp(P(x)) = -00, or
cp(P(x)) E IR, and 0-(0, x) - £1 ::; LB
for some 0- and £1 that have been obtained from one of the regular Subprograms
P(X) which has been considered earlier in the current Cross Decomposition phase.
The logic behind this strategy is the fact that a solution x which satisfies the conditions
mentioned under (iii) cannot improve on the lower bound, at least not more than £1' In
case cp(P(x)) = -00 the statement is obviously true; in case cp(P(x)) E IR, any lower
bound obtained from any t-optimal dual solution (J' of Vex), will satisfy (J'(O, x) - t ::;
cp(P(x)) ::; 0-(0, x) ::; LB + £1'
Employing this strategy implies that as soon as a solution x is considered for the
second time during the current Cross Decomposition phase, this phase has come to an
end. This is not difficult to prove. Suppose first x and later x are generated during
the same Cross Decomposition phase. In addition, assume that 0- and £1 were generated
when solving P(x). Now, if x = x then obviously, cp(P(x)) = cp(P(x)) E IR. In that case,
LB ~ 0-(0, x) - £1 = 0-(0, x) - £1, so one skips to the Variable Decomposition Procedure.
Note that only finite time will be spent in any Cross Decomposition phase if only a
finite number of x-values need to be considered. Therefore, this strategy enforces finite
convergence in case the original program is a Linear Program.
17
A major drawback is that this strategy may turn out to be extremely time-consuming,
especially when the individual Cross Decomposition phases require a large amount of time.
In that case many previously generated dual solutions and their accuracies must be kept
in storage.
Strategy 3b Let the following conditions be simultaneously satisfied.
(i) The Cross Decomposition phase is not entered before a real-valued upper bound
UB is known.
(ii) Each time a regular Subprogram CD(g) is under consideration, an E2-optimal
solution (x, y) for CD(g) is generated, and the upper bound is updated according to
UB := min{UB, f(x, y) + g(O) - g(G(x, 'if)) + E2}'
(iii) At least one full iteration of the Constraint Decomposition Procedure is carried
out as soon as a solution 9 is under consideration which satisfies
'P(CD(g)) = +00, or
'P(CD(g)) E IR, and f(x,fj) + g(O) - g(G(x,fj)) + (2 ~ UB
for some (x, fj) and (2 that have been obtained from one of the regular Subprograms
CD(g) which has been considered earlier in the current Cross Decomposition phase.
The logic behind this strategy is the fact that a function 9 which satisfies the conditions
mentioned under (iii) cannot improve on the upper bound, at least not more than (2. In
case 'P(CD(g)) = +00 the statement is obviously true; in case 'P(CD(g)) E IR, any upper
bound obtained from any E-optimal solution (x, y) of CD(g) will satisfy:
f(x, y) + g(O) - g( G(x, y)) + E ~ 'P(CD(g)) ~
f(x, fj) + g(O) - g( G(x, iJ)) ~ UB - (2.
Adopting this strategy means that as soon as a function 9 is considered which is essen-
tially identical to another function 9 which has been considered earlier during the current
Cross Decomposition phase, this phase has corne to an end. This is easily proven. If 9 and
9 are both considered in the same Cross Decomposition phase, and if 9 is essentially iden-
tical to g, then 'P(CD(g)) = 'P(CD(g)) E IR. Suppose (x,fj) and (2 were generated when
solving CD(g), then UB :::; f(x, fj) +g(O) - g( G(x, iJ))+(2 = f(x, fj) +g(O) - g( G(x, iJ)) +(2.
As a result, one skips to Constraint Decomposition.
Note that only finite time will be spent in any Cross Decomposition phase if only a
finite number of essentially different functions 9 need to be considered. Therefore, this
strategy compels finite convergence in case the original program is a Linear Program.
18
6 Concluding Remarks
The framework introduced above is powerful enough to capture the relevant properties of
various decomposition procedures in an abstract setting. As such, it provides the proper
background for further research into the techniques that continue to play such an essential
role in mathematical programming.
References
[1] Benders, J.F. (1962). Partitioning procedures for solving mixed variables program-
ming problems. Numerische Mathematik 4: 238-252.
[2] Dantzig, G.B., and P. Wolfe. (1960). Decomposition principle for linear program-
ming. Operations Research 8: 101-111.
[3] Flippo, O. E. (1991). Stability, duality and decomposition in general mathematical
programming. Stability, duality and decomposition in general mathematical, pro-
gramming. CWI Tract 76, Centre for Mathematics and Computer Science (CWI),
Amsterdam, The Netherlands.
[4] Flippo, O. E., A. H. G. Rinnooy Kan, and G. Van der Hoek. (1987). Duality and
decomposition in general mathematical programming. Report 8747 jB, Econometric
Institute, Erasmus University Rotterdam, Rotterdam, The Netherlands.
[5] Tind, J., and L. A. Wolsey. (1981). An elementary survey of general duality theory
in mathematical programming. Mathematical Programming 21: 241-261.
[6] Van Roy, T. J. (1980). Cross decomposition for large-scale mixed integer linear pro-
gramming with applications to facility location on distribution networks. Doctoral
Dissertation, Applied Sciences, Katholieke Universiteit Leuven, Leuven, Belgium.
[7] Van Roy, T. J. (1983). Cross decomposition for mixed integer programming. Math-
ematical PrQgramming 25: 46-63.
[8] Zangwill, W. I. (1969). Nonlinear programming: a unified approach. Prentice-Hall
Inc., Englewood Cliffs, New Jersey.
Surrogate Constraint Methods for Linear
Inequalities
Kai Yang and Katta G. Murty*
Abstract
Systems of linear inequalities and equations are very important in optimization.
Recent applications of mathematical programming in areas such as computerized
tomography (CAT scan) lead to very large and sparse systems of linear equations
and inequalities which need to be solved approximately within reasonable time.
Traditional Phase I linear programming approaches are not appropriate for these
problems because of the very large size of the systems and irregular sparsity struc-
ture. Iterative relaxation methods can be used to solve these problems, but they
tend to be too slow.
We developed new iterative methods based on the generation of a surrogate
constraint from the violated inequalities in each step. These methods have nice
convergence properties and are amenable to both sequential and highly parallel im-
plementations. Computational experience with these methods is very encouraging.
1 Introduction
We consider the problem of finding a feasible solution to a system of linear inequalities:
Ax:S b (1)
where A E IRmxn and b E IRm. This is a very fundamental problem in optimization. A
major recent application of this problem arises from image reconstruction that is becoming
an important activity in many scientific fields. The aim of image reconstruction
'Department of Industrial and Operations Engineering, University of Michigan, Ann Arbor, Michigan
48109, U.S.A. Partially supported by NSF grant no. ECS-8521183.
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgiil et al.
© Springer-Verlag Berlin Heidelberg 1992
20
is to display images of internal structure of objects. For example, in medical imaging,
computerized tomography reconstructs the images of interior structures of the human
body by processing data obtained from measuring the attenuation of X-rays along a large
number of lines through the body. Other image reconstruction problems arise in remote
sensing [11], geogr~phical imaging (seismic tomography) [2] and industrial imaging for
nondestructive testing.
The problem of solving linear inequalities was the subject of intensive research activity
over the last two centuries. Already, there are two well developed approaches for this
problem. One of them transforms (1) into a Phase I linear programming (LP) problem,
which is then solved by well-established methods such as the simplex method or the
recently developed interior point methods. These methods employ matrix operations and
are, therefore, impractical in applications such as image reconstruction because of the
very large size of the systems that arise in them, and their irregular sparsity structure
which is hard to exploit in these methods.
The second approach is based on iterative methods. These methods always work
with the original data and most of them do not need matrix manipulations. The basic
computational step in iterative methods is extremely simple and easy to program. Because
of these advantages, all linear inequality solvers used in image reconstruction are iterative
methods. One class of iterative methods are derived from the relaxation method for linear
inequalities [1, 15, 14]. The methods are called 'relaxation' methods because they consider
one constraint at a time, so in each iteration, all but one constraint are 'relaxed'. At each
iteration, a violated constraint is identified and an orthogonal projection is made onto
the hyperplane corresponding to it from the current point. So, they are also called the
'successive orthogonal projection' methods. Making an orthogonal projection onto a single
linear constraint is computationally inexpensive. However, when solving a huge system
which may have thousands of constraints, considering only one constraint at a time leads
to slow convergence. Instead, it is better to process a group of constraints at a time.
But, making an orthogonal projection onto the affine space corresponding to a group of
constraints is computationally expensive. The amount of work for this projection grows
as a cube of the number of constraints in the group.
In this paper, we study a new class of iterative algorithms first proposed in [19]. They
are able to process a group of violated constraints at a time but retain the same computa-
tional simplicity as the relaxation method. These methods are called' surrogate constraint
methods' since they identify a group of violated constraints and derive a 'surrogate con-
straint' from them in each iteration. The current point is then orthogonally projected
onto this surrogate constraint treated as an equation. Three different versions of this
method have been designed to tackle large scale systems.
21
Section 2 discusses the application of linear inequality models in image reconstruction
problems. Section 3 defines notation. Section 4 describes the three versions of the surro-
gate constraint methods. Section 5 summarizes the convergence results and the geometric
interpretation of these methods. Some computational results are presented in Section 6.
2 Linear Inequality Models in Computerized To-
mography
The word 'tomography' is derived from the Greek words 'tomos', meaning 'slice' or 'sec-
tion', and' graphia' meaning 'recording'. So, in medical terms, tomography is a recording
of a slice or section of the body. Computerized tomography (CT) is a process in which
the image or picture of a cross section of the body is reconstructed with the aid of a com-
puter. Computerized tomography has revolutionized diagnostic radiology. CT is able to
detect some conditions that conventional X-ray pictures cannot detect, for example, CT
can show a three-dimensional view of the interior structures of the human body, such as
the brain, lungs and heart. So, CT can often replace certain traditional diagnostic tech-
niques, such as exploratory surgery and other procedures, to detect and locate tumors,
blood clots and other malignant items.
Figure 1 shows how computerized tomography works. An X-ray tube focuses a narrow
beam of X-rays across one layer or 'slice' of the body. As the beam passes through the
body, some of its energy is absorbed by the internal structures in the body. The amount
of energy absorbed at any point on the path can be assumed to be roughly proportional
to the density of the structure at that point, that is, the local density. The detectors,
located opposite the X-ray tube, record the intensity of the beam as it emerges after
passing through the body. The X-ray tube rotates around the body 'scanning' it, and
thousands of such readings are taken. Then, this information is relayed to a computer
and stored there. The computer analyzes the detectors' readings and calculates the X-
ray absorption and from it the local density at thousands of different points. These
calculations are converted into an image on a video screen. The image will be studied by
radiologists for diagnosis.
Based on the same principle, CT like techniques have been used to develop different
types of medical image procedures ( for example, PET (positron emission tomography),
NMRI (nuclear magnetic resonance imaging), SPECT (single photon emission comput-
erized tomography), etc. ) and, in general, procedures for computing and displaying
the internal structure of objects that absorb or scatter or diffuse some form of radiation,
22
Ima ge
o~
t
oataDOetector ~ ~
Computer -..... I I
Figure 1: Illustration of Computerized Tomography.
utilizing a large collection of shadow graphs. In these procedures, the local density at any
point inside the object represents whatever material property one wishes to reconstruct,
and image refers to a display of the density distribution within the object. By the ab-
breviation 'CT' we will refer to any procedure that reconstructs an image of the internal
structure of an object from measurements on emanations ( any type of radiation, X-rays,
infrared radiation, ultrasonic radiation, neutrons, etc. ) that have passed through the
region of the object being imaged.
There are two general classes of CT. The 'remote-probing CT'refers to the class of
procedures in which the emanations come from a source outside the object being imaged.
This is illustrated in Figure 1. A 'remote sensing CT'refers to a class of procedures in
which the source of emanations is within the interior of the object being imaged. An
example of this type of CT is PET illustrated in Figure 2. PET uses radioactive isotopes
inserted at known locations within the body as radiation sources. The attenuation of the
gamma rays emitted by these isotopes is measured by detectors as they emerge from the
body. These recordings are then used to reconstruct the image of the body's interior.
Table 1 and Table 2 summarize information on some commonly used remote-probing and
remote-sensing CTS.
23
Intesity
~___-\-._measllrements
\
\ x -- Image
\
\
.......
....... .......
Biological object with radioactive isotopes
(whose radioactive intensity distribution
de ends on biolo .cal functions )
Measurement data sent to
computer to reconstruct
the image
Lead plate
with holes
Radioactive rays
Close-up of the detector
Figure 2: Positron Emission Tomography.
Sensor
24
Applications Emanations Densities Sources Detectors Techniques
X-ray attenuation X-ray tube Scintillation Transmitted
X-mys coefficients detectors X-rays
Diagnostic Heavy panicals; Scattering Linear Stacked Transmitted
Radiology pions, alphas, etc absorption Accelerators detectors heavy particals
cross section
Ultrasond Attenuation; oscillators Mechanical Transmillcd
refractive index. transduction Ultrasound
device
Nondestructive X-mys X-ray attenuation X-ray tube Scintillation Transmitted
Testing coefficients detectors X-rays
Heavy particals; :.cattermg Linear Stacked Transmitted
pions, alphas, etc absorption Accelerators detectors heavy particals
cross section
Ultrasond Attenuation; oscillators Mechanical Transmitted
refractive index. transduction Ultrasound
device
Microscopy of Electron (wave) Schrodinger Photomultiplier; Electrical
'weak' specimens potential Electron guns Film. Microscopy
beams distribution
Crude imaging of low frequency Electrical Eleclrical
blood vessels electric conductivity Electrodes Electrodes Impedance
current distribution
Table 1: Applications of the Remote-Probing CT.
25
Applications Emanations Densities Sources Detectors Techniques
Concentralion of KadlO-labled Gamma Single Pholon
Gamma radio-labled suslancc inlroduced Emission
inlo body cameras (SPECT)
Diagnostic -rays substance
Concentration of Posilronium Ring of PositronRadiology Photons posilronium labeled isolopes counters and Emission
labled introduced coincidence (PET)
subslance inlO body circuits
Seismic Attenuation;
refractive index Earch Mechanical Seismicor Changes in transduction
Acoustic acoustic quake device Tomography
Geological waves impedance.
Prospecting low frequency Electrical Electricalelectric conductivity Electrodes Electrodes
current distribution Impedance
Thermal
Satellite infrared or Temperatue radiation emitted Radio, infrared,
Remote microwave profile of because inside ultraviolet, Radiometric
Sensing radiation aunosphere of atmosphere X-ray telescope
is hot
Table 2: Applications of the Remote-Sensing CT.
Linear Inequality Models for Image Reconstruction
Image reconstruction is the problem of determining the distribution of density in the
interior of the object using measurements on emanations that have passed through the
object. There are several approaches for image reconstruction. We discuss the finite series
expansion approach which leads to large sparse linear inequality models.
To reconstruct the interior structure of the object, we divide the object into volume
elements (voxels) whose size is set to the desired resolution. The general reconstruc-
tion problem is three dimensional, but for ease of explanation, we will discuss a two-
dimensional system ( the generalization of the approach to the three dimensional case
is straight forward). In the two-dimensional case, volume elements are termed pixels.
Consider an X-ray beam, beam 1, penetrating the two-dimensional object, entering it at
the source with an intensity of S, and emerging at the detector at the end of its path
through the object with an intensity of D. Then, the total attenuation, the absorption
along the path, is bl = S - D. Put an P x N grid of P N pixels over the object as in
Figure 3. Assume that the local density is constant over each pixel. Let Xj denote the
unknown local density of the ph pixel, j = 1 to P N. Let alj denote the constant of
proportionality of the ph pixel per unit density, towards the absorption of intensity of
X-ray
Source
Reference
Gride of
PxN
Pixels
26
Path of beam #1
with initial
intensity S
Terminal
intensity D
Cross Section of
Object to be
Reconstructed
X-ray
Detector
Figure 3: Digitized Image and X-ray Source and Detector.
beam number 1. For example, alj can be taken to be proportional to the length of the
intersection of the path of beam number 1 with the ph pixel since the longer the length
of the path of the beam through this pixel, the larger the absorption of its intensity by
the pixel. Figure 4 sho~s an example of how to estimate alj values.
As the absorption increases both with the coefficient alj and the density xj, aljXj is
assumed to represent the reduction in the intensity of beam 1 as it travels through the
ph pixel, j = 1 to P N = n. Thus, we have the equation, 2::5'=1 aljXj = bl , for the total
attenuation of this beam. If data is collected from M different beams, this would lead to
the following system of linear equation:
n
2:= aijXj = bi, i = 1 to M
j=l
(2)
where bi = Si - Di is the total attenuation of the ith beam by its passage through the
object, and A = (aij) is the coefficient matrix corresponding to all the beams, obtained
as described above.
If the number of pixels in the grid is large and the area of each pixel is small, the
lengths of the intersections of the rays with the crossing pixels will be approximately the
same and can be assumed to be equal to 1. In this case, we can define aij = 1 if the ith
X-ray
(a) X-ray penetrates a grid
of9 pixels
27
0.3 0.2 0.0
0.0 0.9 0.0
0.0 0.1 0.4
(b) The lengths of the intersections
of the path with the pixels, 1 unit
= length of the diagonal of a pixel.
(alj ) = ( 0.3,0.2,0,0,0.9,0,0,0.1,0.4)
Figure 4: An Example of Estimating alj Coefficients.
beam intersects the ph pixel, 0 otherwise, and A = (aij) becomes a 0-1 matrix.
The number of unknowns in system (2), n = PN, grows very quickly as the pixel
size is decreased for good resolution. The bi's are based on physical measurements which
are usually noisy. Also, our assumption that the local density is a constant within each
pixel is unlikely to be valid perfectly. Furthermore, the equality of the two sides in (2)
is unlikely to hold exactly because of multiple scattering. For all these reasons, system
(2) is only an idealization, and it does not make any sense to require an exact solution
for it, which may not even exist. Hence, it is reasonable to replace (2) by the following
system of linear inequalities (3),where Ei'S are error tolerances based on the knowledge of
the expected characteristics of the interior of the object and the path of the ith beam.
Since the densities are nonnegative, we have x j ::::: 0 for all j = 1 to n. And, u in (4) is a
known upper bound for density.
n
bi - Ei ~ L aijXj ~ bi + Ei
j=l
o ~ Xj ~ U
(3)
(4)
This is the linear inequality model for image reconstruction. Once a density vector x
is computed, the image is generated from it. For instance, the image is drawn by making
the darkness of color of each pixel proportional to its density value.
In a sophisticated equipment, there are usually separate detectors for measuring not
only attenuation, but also sideways, forwards and backwards scattering. Using a similar
approach, image based on sideways, backwards and forwards scattering data are also
generated to depict different characteristics of the object.
28
3 Notation and Assumptions
We consider system (1). A = (aij), b = (b i ), and A; denotes the ith row vector of A. We
assume that all this data is integer and Ai #- 0 for all i. We let JC denote the set of feasible
solutions of (1) and we assume that JC #- 0.
I C {I, ... , m} denotes an index set identifying a subset of the constraints.
JC i = {x I A;x :::; b;}, is the half space corresponding to the ith constraint.
Hi = {x I A;x = b;}, is the boundary hyperplane of the ith constraint.
JC I = {nJCi};EI, JC I #- 0 since JC I :J JC.
lSI = Cardinality of the set S.
AI = the III X n matrix with rows Ai, i E I.
h = (bi , i E I), a column vector.
Ilxll: the Euclidean norm of a vector x, Ilxll = +V"£j x;
d(x, Hi) = minimum Euclidean distance from x to Hi.
d( x, JC i) = minimum Euclidean distance from x to JC i . Note that d( x, JCi) = 0 if x E JC i;
otherwise, d(X,JCi) = d(x, Hi).
cfJ( x) = sUPiE{l, ...m} d( x, JCi )
d(x, JC) = minimum Euclidean distance from x to JC.
Here we define the length of the binary encoding of all problem data in (1) as:
L = 2: I)1 + log(laijl -tel)] + 2:[1 + log(lb;l + 1)] + [1 + log nm] + 2 (5)
j
Clearly, a point x E JC iff cfJ( x) = O. In practice, we are interested in getting an
approximate solution within some tolerance. A solution x is said to be feasible to (1)
within tolerance E if cfJ(x) = maxi(Aix - bi) :::; E, or in other words, Aix - bi :::; E for all
i = 1 to m. Clearly, if E = 0, x is an exact feasible solution.
4 The Surrogate Constraint Methods
In each iteration, these methods generate a surrogate constraint by taking a positive
combination of a set of original constraints violated by the current point. We describe
three different methods developed in [19].
29
Algorithm 1: The Basic Surrogate Constraint Method
Let E be the small positive user specified tolerance for each constraint. In each iteration
of this method, all the constraints violated by more than the tolerance at the current point
are identified and a surrogate constraint is generated from them. ). satisfying 0 < ). < 2
is the relaxation parameter that the user can select.
Initialization: Let XO E IRn be some initial point ( it could be 0 or some known near
feasible point). Select a tolerance E and go to Step 1.
General Step k+1: Let xk be the point obtained at the end of the previous step.
Identify Ik = I(x k) = {i : Aix k - bi > fl. If Ik = 0, xk is feasible to (1) within the spec-
ified tolerance E, terminate. Otherwise, select a row vector 'Irk = ('lrf : i E I k ) of positive
weights and generate the surrogate constraint 'Irk Ark x :::; 'lrkbrk. The surrogate hyperplane
in this iteration is 1{k = {x : 'Irk Ark x = 'lrkbr k}. Here, we are assuming 'Irk Ark i=- 0 ( if
'Irk Ark = 0 , a different weight vector is selected to generate the surrogate constraint ).
The new point is:
Go to the next step.
Remark 1. If). = 1, xk+l will be the orthogonal projection of the current point xk
on the surrogate hyperplane 1{k. ( See Figure 5 )
Remark 2. Recommended choices of the weight vector 'Ir : The following are some of the
rules that can be used to select the vector of weights in each iteration.
(i) Weight by error: The quantity ri = A;xk - bi denotes the Euclidean distance
from the current point xk to Ki for each i E I k. Since larger ri corresponds to greater
infeasibility with respect to Ki , it may be desirable to make 'lri proportional to ri for all
i E I k , that is take
(ii) Weigh equally: in this rule, 'lrf = rAI ,for all i E Ik .
(iii) Convex combination of the two weights given above:
k
'lri ari 1- a
'" + -IIkl ' for i E I k , and for some 0 < a < 1.
UiEIk ri
30
Surrogate
Hyperplane
--......
k
X
Figure 5: Illustration of a Step in the Surrogate Constraint Method with A = 1.
Algorithm 2: The Sequential Surrogate Constraint Method
In many applications, m and n are often very large, and the A matrix is very sparse.
The computation may have to be carried out on site, which may make it very difficult to
work on all constraints at the same time. In such situations, it is preferable to work on
one small subset of constraints in (1) at a time. So we have to partition system (1) into
p subsystems:
(6)
where At and bt are mt * nand mt * 1, respectively, for t = 1 to p, with I:~=1 mt = m.
This algorithm operates on subsystems t = 1 to p in (6) successively in cyclic order. It
goes through major cycles. In every major cycle, each of the p subsystems is operated on
once, in serial order t = 1 to p.
Initialization is the same as in Algorithm 1. Consider a major cycle. In this major
cycle, operate on subsystems in the order t = 1 to p.
Let xk be the current point, and let the tth subsystem be the one to be operated next.
Find V(xk) = {i: ith constraint is in tth subsystem and it is violated by more than the
tolerance at xk}.
If V(xk) = 0, define xk+l = xk.
31
k
X Xk+l Xk+2 X k+p-l Xk+p
~
Subsystem Subsystem .. - Subsystem ~
j 1 2 p
Figure 6: Diagram of the Sequential Surrogate Constraint Method.
If t < p, go to the next subsystem with xk+l.
If t = p, this completes the major cycle. If there is no change in the current point
throughout this major cycle, then the current point is feasible to (1) within the specified
tolerance, terminate. Otherwise, go to the next major cycle with the current point.
If rt(x k) f- 0, select a row vector 1rtk of positive weights and dimension IIt(xk)l, and
define the surrogate constraint in this step to be
where Atk is the submatrix of At consisting of its rows i for i E It(xk), and btk the
subvector of bt consisting of its elements corresponding to i E It(xk). The surrogate
constraint hyperplane in this iteration is {x : 1rtkAtkx = 1r tk btk }. Here, again we are
assuming that 1rtk Atk f- 0 as discussed under Algorithm 1, otherwise 1rtk is changed so
that this property holds. The new point is: Xk+l = xk - )..gk where
k (1rtk Atkxk _ 1r tk btk )( 1rtk Atk)T
g = II1rtk A tk l\2 (7)
With xk+l, go to the next subsystem if t < p, or to the next major cycle if t = p.
Algorithm 3: The Parallel Surrogate Constraint Method
The surrogate constraint method can also be implemented to work on ALL of the
subsystems, Atx ~ bt , for t = 1 to p of (6) SIMULTANEOUSLY. This is particularly
suited for parallel cor:nputation. This algorithm generates one new point in each step and
an operation is carried out with the current point on each subsystem in a parallel manner.
Initialization is the same as in Algorithm 1.
General Step k + 1: Let xk be the point obtained at the end of the previous step. Do
the following for each subsystem, Atx ~ bt , for t = 1 to p in parallel. Find It(xk) as in
Algorithm 2. If rt( xk) = 0 for all t = 1 to p, xk is feasible to (1) within the specified
tolerance, terminate. Otherwise, continue.
32
k+
X
k Take convex
.--.. x
;7 combination
~ P"",lIei ~ro""ing
I Subsystem p
I
~ Sub,y'~m I ~
/,.---S-U-b-Sy-s-te-m-2-----.I~
Figure 7: Diagram of the Parallel Surrogate Constraint Method.
For each t, If :rt(xk) = 0, define Pt(x k) = xk. If It(xk) # 0, select the weight vector
7'itk as in Algorithm 2, and with Atk and btk having the same meaning as there, define
pte xk) = xk - gk, where
k (7'itk Atkxk _ 7'i tk btk )(7'itk Atk)T
9 = 117'itkAtkll2
Define P(x k) = 2:~=1 TtPt(Xk), where Tt'S are nonnegative numbers summing to 1 with
T; > p> 0 for all t such that It(xk) # 0, and p is some small positive number.
Define xk+1 = xk + A(P(Xk) - Xk) where 0 < A < 2.
5 Convergence Results and Geometric Interpreta-
tions
In this section, we provide some geometrical interpretations of aspects of the surrogate
constraint algorithms and summarize convergence results for them based on [19].
Given 0 # KeIRn, a sequence {x k : k = 1 to oo} in IR n is said to be strictly
Fejer-monotone with respect to the set K if Ilx k +1 - xII < Ilxk - xII for all k and for all
x E K. Every Fejer-monotone sequence is bounded if K # 0 since Ilxk - xII is always
positive and monotonically decreasing with k.
Clearly, Algorithm 1 can be treated as a special case of Algorithm 2 with p = 1. So,
we discuss convergence results only for Algorithm 2 and Algorithm 3.
Consider Algorithm 2. Let xk be the current point which is infeasible to (1) at some
stage, and let r s be the surrogate constraint half-space ( i.e., the set of feasible solutions
33
k k
X - 2g
Figure 8.
k k
X - g
for the surrogate constraint ) and tis the surrogate constraint hyperplane constructed
in this step. So K c rs. The equation for tis is a positive combination of a subset of
constraints from some subsystem in (6), each of which is violated at xk. As every point
in K satisfies all the constraints in (6), this implies that tis strictly separates xk from K.
From this and the results in [1, 10], it follows that I\y - (x k - >.gk)11 < Ily - xkl\ for all
°< >. < 2 and for all y E r. (since K c r., this holds afortiori for all y E K, where gk is
as defined in (7) in Algorithm 2 in this iteration. See Figure 8.
From this, itfollows directly that if Xk+l =I- xk in Algorithm 2, Ilx-xk+l11 < Ilx-xkl\, for
all x E K, i.e., the sequence of distinct points generated by Algorithm 2 ( and consequently
the sequence of points generated by Algorithm 1) possesses Fejer-monotone property.
From these, we have proved the following results in [19].
1. If K =I- 0, and Algorithm 1 or Algorithm 2 is operated with E = °resulting in the
sequence of points {x k : k = 1,2, ... }, then limk_oo <p(x k) = 0, and the sequence converge
to a point x E K.
2. If K =I- 0, the sequence {x k : k = 1,2, ... } generated in Algorithm 2 terminates with a
point in {x : AiXk ::; bi + E, i = 1 to m} after a finite number (::; ~:~~~~) of major cycles.
Similarly, the sequence generated by Algorithm 1 terminates in the same way after at
most the same number of iterations.
3. In Algorithm 3, if xk+l =I- x\ then I\x - xk+ll\ < I\x - xkll for all x E K.
4. If K =I- 0, and Algorithm 3 is operated with E = 0, the sequence of points generated
{x k : k = 1,2, ... } satisfies limk_oo<p(xk) = 0, and the sequence converge to a point
x EK.
5. If K :f:. 0, the sequence {x k : k = 1,2, ... } generated in Algorithm 3 terminates with a
. . { A k b . } f ( m 3 2 2L - 2 )
pomt m x: iX ::; i + E, ~ = 1 to mater at most .\(2_.\).,2 steps.
34
Figure 9: Illustration of One Step in Algorithm 3.
Figure 9 illustrates one step in Algorithm 3. Here, we consider a system divided into
three subsystems. With the current point xk, three surrogate constraint hyperplanes are
generated ( one for each subsystem) in parallel, and orthogonal projections are made
simultaneously onto these three surrogate hyperplanes. These three projection points are
denoted by the three vertices of the shaded triangle in Figure 9. The new point Xk+l will
be somewhere inside the triangle, depending on the choice of the t (convex combination)
vector selected in this step.
Figure 10 provides an illustration for comparing the surrogate constraint methods with
the classical relaxation methods. In each iteration of the relaxation method, an orthogonal
projection is made from the current point xk onto an individual half-space IC i . However,
IC i only contains the information from one constraint. Sometimes, the projection on IC i
offers little improvement in reducing the distance from the current point xk to set IC.
On the other hand, the surrogate hyperplane contains the information of more than one
violated constraint, so it could be expected to generate a better new point than that of
the relaxation method.
6 Computational Results
Algorithm 1 and Algorithm 2 have been tested on several randomly generated problems.
They are coded in FORTRAN and the tests are carried out on an IBM 3090-400/VM
main frame computer' at the University of Michigan.
Surrogate
, Hyperplane
"K', '--...>,
,
y><'",
,
Origin'a} " , ,
COnstrait'H:s
""
,
\.'\'.'
,
New
POint
by
Surrogate
Constraint
MethOd
35
New Point by
RelaxationMethOd
k
X
Pig"'e
10,
COmpaci'on of 'he Sun-oga'e
Con"'ain,
Me'hod wi'h 'he
Cl""k
al llel
axa
'io
n
Metbod.
36
Problem Size Number of CPU Time Sparsity
of the
Iterations Seconds Problem
Rows Columns %
40 40 4 0.006 25.0
100 100 9 0.073 25.0
200 200 12 0.133 25.0
400 400 15 0.363 15.0
800 800 15 0.430 4.0
Table 3: Computational Results for Algorithm 1.
Problem Size Number of CPU Time Sparsity Number Rows
of the of Sub- in each
Iterations Seconds Problem systems Sub-
Rows Columns % system
40 40 5 0.006 25.0 2 20
100 100 10 0.029 25.0 2 50
200 200 20 0.132 25.0 4 50
400 400 26 0.365 15.0 4 100
800 800 35 0.578 4.0 4 200
Table 4: Computational Results for Algorithm 2
The same test problems are used for both algorithms. In these problems, m and n
ranged from 40 to 800. The sparsities (which is defined to be the number of nonzero entries
in the A matrix divided by mn) ranged from 25% to 4%. Entries in the A matrix are
selected randomly between -5.0 to 5.0. The b vectors are generated so that the resulting
linear inequality systems are feasible. Table 3 and Table 4 list the performance summaries
of Algorithm 1 and Algorithm 2. In these tables, rows represent the number of inequalities
and columns represent the number of variables. In Algorithm 1, the step length Ais chosen
to be 1.2 for all test problems. A is chosen to be 1.5 in Algorithm 2. The 7r vector is
37
chosen according to the following formula in both algorithms: 7ri = ari + (1 - a) where
a is equal to 0.1 and ri is the residual for ith constraint.
Problem Size Number of CPU Time Sparsity Number Rows
of the of Sub- in each
Iterations Seconds Problem systems Sub-
Rows Columns % system
4000 4000 4 0.375 1.0 4 1000
4000 8000 4 0.787 1.0 4 1000
4000 2000 5 0.445 2.0 4 1000
5000 1000 11 0.667 2.0 5 1000
9000 1000 19 1.898 2.0 9 1000
9000 2000 18 1.816 1.0 9 1000
10000 10000 16 3.855 0.4 10 1000
16000 2000 18 1.841 0.4 8 2000
16000 4000 15 2.429 0.4 8 2000
18000 9000 17 3.881 0.2 9 2000
20000 20000 20 6.674 0.1 10 2000
Table 5: Computational Results for Algorithm 2 for Large Sparse Problems.
Some very large and sparse problems have been solved by Algorithm 2, and the CPU
time and number of iterations for those problems are summarized in Table 5.
These computational results with the surrogate constraint methods are clearly very
encouraging.
References
[1] S. Agmon. The relaxation method for linear inequalities. Canadian Journal of
Mathematics, 6: 382-392, 1954.
[2] Anderson, D. 1., and A. M. Dziewonski. Seismic tomography. Sci. Amer., 251:
58-66, 1984.
[3] 1. M. Bregman. The method of successive projection for finding a common point
of convex sets. Soviet Mathematics Doklady, 6: 688-692, 1965.
38
[4] 1. M. Bregman. The relaxation method of finding the common point of convex sets
and its application to the solution of problems in convex programming. U.S.S.R.
Computational Mathematics and Mathematical Physics, 3: 200-217, 1967.
[5] Censor, Y., and G. T. Herman. On some optimization techniques in image recon-
struction from projections. Applied Numerical Mathematics, 3: 365-391, 1987.
[6] Censor, Y., and T. Elfving. New method for linear inequalities. Linear Algebra and
Its Applications, 42: 199-211, 1982.
[7] Y. Censor. Row-action methods for huge and sparse systems and their applications.
SIAM Review, 23(4): 444-466, Oct. 1981.
[8] G. Cimmino. Calcolo approssimato per Ie soluzioni dei sistemi di equazioni lineari.
Ricerca Sci. (Roma), Ser. II, Anno IX, 1: 326-333, 1938.
[9] De Pierro, A. R., and A. N. Iusem. A simultaneous projections method for linear
inequalities. Linear Algebra and Its Applications, 64: 243-253, 1985.
[10] 1. 1. Eremin. The relaxation method of solving system of inequalities with convex
function on the left sides. Soviet Mathematics Doklady, 6: 219-222, 1965.
[11] H. E. Fleming. Satellite remote sensing by the technique of computerized tomogra-
phy. J. Appl. Meterology, 21: 1538-1549, 1982.
[12] Gacs, P., and 1. Lovasz. Khachiyan's algorithm for linear programming. Report
STAN-CS-79-750, Department of Computer Science, Stanford University, 1979.
[13] Gubin, 1. G., Polyak, B. T., and E. V. Raik. The method of projections for find-
ing the common point of convex sets. U.S.S.R. Computational Mathematics and
Mathematical Physics, 6: 1-24, 1967.
[14] S. Kaczmarz. Angenherte auflosung von systemn linearer gleichungen. Bull. Inter-
nat. Acad. Polon. Sci. Lett. A., 35: 355-357, 1937.
[15] Motzkin, T. S., and 1. T. Schoenberg. The relaxation method for linear inequalities.
Canad. J. Math., 6: 393-404, 1954.
[16] K. G. Murty. Linear complementarity. Linear and Nonlinear Programming, Helder-
mann Verlag Berlin, 1988.
[17] K. G. Murty. Linear Programming, John Wiley & Sons, 1983.
[18] J. Telgen. On relaxation methods for system of linear inequalities. European Journal
of Operational Research, 9: 184-189, 1982.
[19] Yang, K., and K. G. Murty. New iterative methods for linear inequalities. Technical
Report 90-9, Department of Industrial and Operations Engineering, The University
of Michigan, Ann Arbor, Michigan, U.S.A., Feb. 1990.
An Evaluation of Algorithmic Refinements and
Proper Data Structures for the Preflow-Push
Approach for Maximum Flow
U. Derigs and W. Meier*
Abstract
Following the milestone paper by Goldberg on the preflow-push algorithm for
solving maximum flow problems on networks, quite a number of refinements have
been suggested in literature which reduce the computational complexity of this
approach. One of these streams is based on "scaling".
In this paper we shortly review some recently published scaling approaches, and
we develop an appropriate data structure by which an efficient storage and use of
the additional information on feasible subnetworks etc. is possible. Finally, we
report extensive computational results.
1 Introduction and basic notation
Let G = (V, E) be a directed graph and c : E -+ IN a capacity function on the edges.
In G, we distinguish two nodes: the source s E V and the sink t E V, s =1= t. Then,
we call N = (V, E, c, s, t) a network, and w.l.o.g., we assume N to be asymmetric, i.e.
(v, w) E E:::} (w, v) ~ E. For an edge (v, w) E E, v is the tail-node and w the head-node.
For v E V, we define three sets of edges, 5+(v), the set of incoming edges, i.e. the
backward star, and 5- (v), the set of outgoing edges, i.e. the forward star of v, and
5( v) = 5+( v) U 5-( v). This definition is extended to sets W C V by 5+(W) := {(v, w)lv rt
W,w E W},5-(W):=;: {(v,w)lv E W,w ~ W} and 5(W):= 5+(W) U 5-(W) .
*Lehrstuhl fiir Wirtschaftsinformatik insbesondere Datenmanagement, Entscheidungsunterstiitzung
und Quantitative Methoden, Universitat zu Kaln, Albertus-Magnus-Platz, D-5000 Kaln 41.
NATO AS! Series. Vol. F 82
Combinatorial Optimization
Edited by M. AkgiH et al.
© Springer-Verlag Berlin Heidelberg 1992
40
For the following, we assume 8+(s) = 8-(t) = 0, and we set n :=1 V I and m :=1 E I.
Moreover, the set of neighbors of node v is given by N(v) := {w E VI(v,w) E 8-(v) or
(w, v) E 8+(v)}.
A function f : E ~ lRt is called a flow on N if the following properties hold:
(w,v)ES+(v)
f(v,w) < c(v,w)
f(w,v) = L
(v,w)ES-(v)
V(v,w) E E
f(v, w) Vv E V\{s, t}
The conditions (1.2) are called the flow conservation rules for v E V\ {s, t}, and
If I := L f(v,t) L f(s,v)
(v,t)ES+(t) (s,v)ES-(s)
(1.1)
(1.2)
(1.3)
is the flow value of f. A maximum flow is a flow the value of which is maximum among
all flows on N, and the maximum flow problem is to find such a flow. Note that with c
integer valued, there exists always a maximum flow which is integer valued.
The maximum flow problem is one of the fundamental problems on graphs and net-
works. It is of interest because of its theoretical properties and practical relevance. Ac-
cordingly, quite a number of different algorithms and implementations for this problem
have been published in the past starting with the classical paper and augmenting path
algorithm of Ford and Fulkerson [7].
Recently, Goldberg and Tarjan [9] presented a conceptually new and simple algorithm
which differs from all other known efficient maximum flow algorithms by the fact that a
(minimum) cut is found first and then this information is used to construct a maximum
flow. In contrast to the classical augmenting path method, flow is pushed along single
edges and not along entire paths. This implies that the algorithm cannot maintain flow
conservation at the nodes and therefore has to deal with a relaxation of flows:
A preflow on N is a function g : E ~ lRt which fulfills (1.1) and
L g(v,w):::; L g(w,v) Vv E V\{s,t} (1.4)
(v,w)ES-(v) (w,v)E6+(v)
Note that (1.4) is a relaxation of (1.2) and any flow is a preflow, too.
The function /:::"g( v) : V ~ lRt with
"" ~ g(v,w)/:::,.g(v):= L- g(w, v) - L- (1.5)
(w,v)ES+(v) (v,w)ES-(v)
is called the excess function with respect to a preflow g. For a node v E V, the value
/:::"g( v) is called the excess of the node.
41
With respect to a preflow 9 (and hence any flow) on N, we define Rg = (V, Eg, Cg, 8, t),
the so-called residual network, with
Eg := ((v,w) I (v,w) E E andg(v,w) < c(v,w)}
U{(v,w) I (w,v) E E andg(w,v) > O}
{ c(v,w)-g(v,w), (v,w) E EgnE
cg(v,w) := g(w,v), (v,w) E Eg\E
(1.6)
(1.7)
Given a (residual) network R g , we say that a node w is reachable from node v if there
exists a (directed) path from v to w in Rg. For any such path P, we define the length of
P as the number of edges in P. In the sequel, a shortest path is always the shortest with
respect to this measure, i.e. a path with the least number of edges, and the length of the
shortest path from node v to node w defines their distance denoted by dg ( v, w).
Let W C V with 8 E Wand t ~ W. Then, we call the partition (W, V\W) an (8-t-
separating) cut. Any edge in t5(W) is called a cut edge, and c(W) := E(V,W)ES-(W) c( v, w)
is called the capacity of the cut.
From the results of Ford and Fulkerson [7), we know that the value of a maximum flow
equals the capacity of a minimum cut, i.e.
max{1 fl I f flow on N} = min{c(W)I(W, V\W) cut in N}
In section 2, we shortly review the preflow-push algorithm for solving maximum flow
problems and some of its recently developed refinements. Then, in section 3, we present
a data structure for storing the network which is ideally suited to support the different
information needs of these implementations. Finally, in section 4, computational results
are presented.
2 A Short Review of the Preflow-Push Algorithm
and some of its Refinements
The preflow-push-algorithm of Goldberg and Tarjan [9] is conceptually very simple. At
the beginning, every edge leaving the source is saturated, and the algorithm tries to push
the flow along single edges to nodes which are estimated to be closer to the sink. This
process is repeated until no more flow can be pushed into the sink. At this time, there
might not (and in general will not) be a feasible flow at hand since at some nodes, the
flow conservation rule may be violated, i.e. there may be positive excess. Therefore, the
algorithm pushes the excess of those nodes back into the source to obtain a feasible flow.
42
From the first results on maximum flow algorithms, we know that the key to efficiency
is the proper choice of the paths along which the flow is augmented. A clever rule is to
always choose shortest augmenting paths. Therefore, the preflow-push algorithm must
be provided with a distance measure, which can easily be updated when the prefiow and
therefore the residual network is changed, for performing such a shortest path scheme.
Given a preflow g consider a labeling d : V ~ INo of the nodes with the following
properties:
d(t) = 0
d(v) > 0 "Iv E V
d(v) < d(w) + 1 V(v,w) E Eg
(2.1)
(2.2)
(2.3)
Obviously, by conditions (2.3) and (2.1), the label d(v) defines a lower bound for the
distance from node v to the sink t. Since the length of the shortest path from any node
to the sink cannot exceed n, this implies that the excess at a node v cannot contribute to
an increase of the flow value once its label exceeds the value n. In this case, the excess
of the node must be pushed backwards to the source and d( v) - n defines a lower bound
for the distance from v to s.
A labeling satisfying conditions (2.1), (2.2) and (2.3) is called a valid labeling with
respect to the preflow g. For instance, d(i) = 0 and d(v) = 1 for v #- t defines a (naive)
valid labeling.
With respect to a valid labeling d, we call an edge (v, w) E Eg with d(v) = d(w) + 1
an eligible edge and a path consisting of eligible edges, only, is called an eligible path.
Note that eligible paths from the source to the sink are shortest augmenting paths.
Prior to stating the preflow-push algorithm, two more conventions and definitions have
to be introduced. For any node v, we assume a certain arbitrary but fixed order on the
set 5( v) and hence on the set N( v) of neighbors of node v. Let LAST( v) and F I RST( v)
be the last and first neighbor of v, respectively, subject to this order. During the course
of the algorithm, the neighborhood of every node is examined according to this order and
CAND(v) always denotes the current candidate neighbor to be used next to process 5(v).
Since further computations have to be done on nodes with positive excess excluding
the sink only, a node with positive excess is called an active node and the set of active
nodes is denoted by A.
43
Algorithm 1: The generic prefiow-push algorithm
Start:
g(v, w) f- 0 for all (v, w) E EV-(s).
g(s,v) f- c(s,v) for all (s,v) E 5-(s).
d : V -> lNo a valid labeling with respect to g.
CAND(v) f- FIRST(v) for all v E V.
A f- {v E V\{i} I 6.g(v) > OJ.
while A =J 0 do
select v E A.
w f- CAND(v).
while (d(w) ~ d(v) or cg(v, w) = 0) and w =J LAST(v) do
w f- next element in N (v ).
enddo.
if d( w) < d( v) and Cg(v, w) > 0
then PUSH(v,w)
else RELABEL( v)
endif.
enddo.
PUSH(v,w):
t f- min{6.g(v),cg(v,w)}.
if(v,w) E E theng(v,w) f-g(v,w)+t.
if(w,v) E E then g(v,w) f- g(v,w) - t.
update A.
CAND(v) f- w.
return.
RELABEL(v):
d(v) f- min{d(w) + 11 (v,w) E Eg}.
CA-ND(v) f- FIRST(v).
return.
Please, note that there are several obvious choices for an (initial) labeling, and that
during RELABEL, the set over which the minimum is computed cannot be empty since
v has positive excess and hence there must be at least one path from v to the source in
the residual network.
Algorithm 1 is generic since no selection rule is specified in the SEARCH step, but
it can be shown that every instance of Algorithm 1 has a complexity of O(n2 m) if the
selection during the SEARCH step can be performed in constant time. Yet, this can easily
be achieved using standard data structures like stack, queue, etc. for storing the set of
active nodes.
44
A computational analysis of this algorithm was done by Derigs and Meier [5]. In
rough terms, we found that 5% of the computing time was spent to find a preflow and a
minimum cut, i.e. the value of a maximum flow, and the remaining 95% was spent in order
to convert the preflow into a maximum flow. A closer look into the computational logic
of the algorithm as well as empirical results revealed that the reason for this behavior
was the immense number of RELABEL steps which had to be performed to drive the
label of the nodes with positive excess to n, i.e. to signal that their excess may be pushed
back into the source. Therefore, we proposed a modified relabeling strategy, the so- called
RELABEL-GLOBAL step.
Let 9 be a preflow and d a valid labeling with respect to g. Then, a number zEIN is
called a gap, iff the following properties hold:
z E {1, ... ,n-l}
d(v) #- z Vv E V
3w E V with d( w) > z
(2.4)
(2.5)
(2.6)
Let z be a gap, then it can easily be shown that setting d( v) = n for all nodes v with
current labels that are greater than z and less than n yields a valid labeling, again. This
procedure can be viewed as a global update of the valid labeling and is furtheron referred
to as global relabeling.
Consider a RELABEL of a node v, with d(v) = z and d(w) #- d(v) for w #- v. Now,
the new label of node v is strictly greater than its old value z. Therefore, conditions
(2.5) and (2.6) hold for z. If condition (2.4) is satisfied, too, then z is a gap. Hence, the
RELABEL step can be modified as follows:
Algorithm 2: The modified RELABEL procedure
RELABEL-GLOBAL(v)
ifl{w E V I d(w) = d(v)} 1= 1 and d(v) < n then
z ~ d(v).
forw Eo V do
if z ::; d( w) < n then
d(w) ~ n.
CAND(w) ~ FIRST(w).
endif.
enddo.
else
d(v) ~ min{d(w) + 11 (v,w) E Eg}.
CAND(v) ~ FIRST(v).
endif.
return.
45
To enable an efficient detection of gaps we maintain a list which for every value z E
{I, ... ,n} stores the number of nodes v which have a label d( v) = z.
Our computational results showed that using this modified RELABEL step saves be-
tween 40 and 90 percent computing time, depending on the graph structure, on every
single example. Yet, no improvement in theoretical complexity could be proved.
Cheriyan and Maheshwari [3] observed that it is essentially the number of non-saturating
pushes, i.e. PUSH steps after which the PUSH edge is not saturated, that slows down
the pre:flow-push-algorithm. They analyzed the so-called highest-label-selection rule, i.e.
always to push :flow from a node with highest label, first, and showed that this rule reduces
the number of nonsaturating pushes to O(n 2 fo), establishing a time bound of O(n 2 fo)
for the pre:flow-push algorithm with highest-label selection.
Ahuja and Orlin [1] bounded the number of nonsaturating pushes by O(kn 2 (logk U +
1)), where U is an upper bound for the capacities and k is an appropriate constant, i.e.
k = 2 for instance. This is a reduction when U = O(nl) holds for some constant I. They
applied" excess scaling" which can be interpreted as an extension of the maximum capacity
augmenting path method of Edmonds and Karp [6] and the capacity scaling algorithm of
Gabow [8] to the pre:flow-push algorithm. The entire algorithm runs in O(nm + n 2 10gzU)
time. The idea is to consider first the nodes whose excess has the highest potential to
increase the :flow value. Obviously, these are the nodes with the largest excess. Therefore,
the algorithm is organized into phases during each of which not the entire set of active
nodes is considered, but only those nodes with "large excess". Here, a node v is said
to have large excess if 6.g(v) 2: 6., where, starting from 6. = k[1ogk Ul-\ the value 6. is
successively divided by k from one phase to the next. The complete description of this
approach is given in the following Algorithm:
Algorithm 3: The excess scaling algorithm
Start:
g(v,w) f- 0 for: all (v,w) E E\8-(s).
g(s,v) f- c(s,v) for: all (s,v) E 8-(s).
d( v) f- min{ dg ( v, s) + n, dg ( v, i)} for: all v E v.
while 6. > 1 do
A f- {iJ E V\{i} I 6.g(v) 2: 6.}.
SEARCJI(A).
6. f- ~.
enddo.
46
SEARCH(A):
while A =I 0 do
select v E A with d(v) = min{d(w) I w E A}.
w _ CAND(v).
while (d(w) ~ d(v) or cg(v,w) = 0) and w =I LAST(v) do
w _ next element in N(v).
enddo.
ifd(w) < d(v) and cg(v,w) > 0
then b.-PUSH(v,w)
else RELABEL{-GLOBALJ(v)
endif·
enddo.
b.-PUSH(v, w):
E - min{b.g(v),cg(v,w)}.
if w =I t then E - min { E, kb. - b.g( w)}.
if(v,w) E E then g(v,w) - g(v,w) + L
if(w,v) E E then g(w,v) - g(w,v) - L
update A.
CAND(v) - w.
return.
(Note that the expression RELABEL[-GLOBAL]( v) stands for the alternative use of
RELABEL(v) or RELABEL-GLOBAL(v).)
Ahuja, Orlin and Tarjan [2] obtained a further reduction of the number of nonsaturat-
ing pushes to O(kn2 + n 2 (logk U + 1)) by replacing the PUSH and the RELABEL steps
by a combined STACK-PUSH/RELABEL step. This bound is smaller than the bound
given earlier for the excess scaling algorithm for an appropriate k, like k = r1 lO~2 U U 1 asog2 og2
proposed by Ahuja et al. [2]. Starting from the node v with large excess and maximal
label, the algorithm in a depth-first manner searches for a path toward the sink, such
that after pushing flow from the successors of v on this path, first, the excess at v can
be reduced more significantly. Since the depth-first search is implemented using a stack
data structure, this is called the STACK-PUSH/RELABEL step (SPR-step). The entire
algorithm can be shown to run in O( nm + n 2 l lOr2 U U) time.og2 og2
47
Algorithm 4: The SEARCH step of the excess scaling algorithm using STACK-PUSH/RELABEl
SEARCH(A):
while A#-0 do
select v E A with d(v) = max{d(w) I w E A}.
S~[v].
Stack-Push-Relabel:
while S #- 0 do
v ~ node on top of s.
w ~ CAND(v).
if d(w) ? d(v) and cg(v, w) = 0 then
if w = LAST( v) then
pop v from S.
RELABEL[-GLOBALJ( v).
else
CAND(v) ~ next element in N(v).
endif
else
if .6.g(w) > %and w #- t then
push w onto S.
else
.6.-PUSH(v, w).
if .6.g(v) = 0 then pop v from S.
endif.
endif·
enddo.
enddo.
Recently, Cheriyan, Hagerup and Mehlhorn [4] observed the following drawback in
the excess scaling algorithm. During a SCALE phase, the flow might be pushed along
edges the capacity of which is small compared to .6. and therefore the excess of the active
node under consideration cannot be reduced very much. Hence, it could be of advantage
not to consider the entire network during a SCALE phase, but a subnetwork containing
only those edges (v, w) with a sufficiently large capacity c( v, w) ? a := %' where fJ is
a constant. This network N'" = (V, E"', c, s, t) is called the surface network and has to
be constructed at the beginning of each SCALE phase. By this construction scheme, the
sets E'" are monotonically enlarged during the procedure with the final surface network
being the entire network. When adding edges to the surface network, we have to saturate
all edges (v, w) with d( v) > d( w) to preserve the validity of the labeling d. Moreover,
to reduce the number of PUSH steps during a specific SCALE phase, for every node v,
the excess to be processed in the current phase is reduced to its necessary minimum, the
so-called visible excess .6.g'" (v), where
.6.g"'(v):= max{.6.g(v) - I: c(v,w),O}
(u,w)EE\E"
48
Note that for any edge (v,w) E E\EC< we have cg(v,w) = c(v,w).
Now the incremental excess scaling algorithm can be formulated as follows.
Algorithm 5: The incremental excess scaling algorithm
Start:
g(v,w) f- 0 for all (v, w) E E\5-(s).
g(s,v) f- c(s,v) for all (s,v) E 5-(s).
d : V -t 0 for all v E V\ {s }.
des) f- n.
CAND(v) f- FIRST(v) for all v E V.
Ec< f- 5-(s) U 5+(s) .
.6. f- kflogk Ul-l.
j3 f- L~J
while .6. > 1 do
for alllv,w) E E\EC< do
if c( v, w) ~ %then
Ec< f- Ec< U {(v, wH.
if d(v) > dew) then g(v, w) f- c(v, w).
endif·
enddo.
A f- {v E V\{s,t} l.6.gc«v) ~ .6.}.
SEARCH(A, EC<) .
.6. f- ~.
enddo.
SEARCH(A, ECX):
while A i=- 0 do
select v E A with d(v) = min{d(w) I w E A}.
w f- CAND(v).
while (d(w) ~ d(v) or cg(v, w) = 0 or (v, w) ¢ E;) and w i=- LAST(v) do
w f- next element in N (v ).
enddo.
ifd(w)::; d(v) and cg(v,w) > 0 and (v,w) E E;
the'fl- .6.-PUSH( v, w)
else RELABEL[-GLOBALJ( v)
endif·
enddo.
The incremental excess scaling algorithm can be shown to run in O( v'nS-Jm+n 2 log 2 U+
nm) time.
Like in the excess scaling approach the SPR Step can be applied for the incremen-
tal excess scaling algorithm, too. Since this extension is straight forward, we omit the
49
presentation of this procedure, which we will refer to as Algorithm 6 in the section on
computational results.
3 Appropriate Data Structures for Implementing
Preflow-Push Variants
When implementing the preflow-push algorithms described in section 2, the efficiency of
the algorithms heavily depends on which data structure is used for storing the (surface)
network and for storing the set of active nodes.
3.1 Storing the Network
A common representation of (flow) networks is the forward-star representation and/or the
backward-star representation where the sets 8- (v) and 8+ (v) together with the capacity
and flow values of the edges contained are stored for each node v. Figure 1 gives a
standard way of efficiently organizing the forward star and the backward star without the
redundancy of storing every edge twice.
Assuming the set of edges and the set of nodes being numbered, i.e. E = {I, ... , m}
and V = {I, ... ,n}, the graph is stored using the m-arrays TAIL, HEAD, PREF LOW
and CAPACITY with T AIL(i), HEAD(i), PREFLOW(i) and CAPACITY(i) being
the appropriate values for edge i E E. The m-array INDOUT (INDIN) consecutively
contains the sequences of edges contained in the forward star (backward star) of the nodes
v E V and the (n + I)-array OUT (IN) gives the index of the first entry in INDOUT
(IN DIN) for every node v E V, i.e. the set of edges in the forward star of node v is
{(v,HEAD(INDOUT(i))) I OUT(v) S i < OUT(v + I)} with OUT(n + 1) = m + l.
For every node v E V we store the candidate edge to be tested for eligibility next in
the following way. For every node v E V there is an entry in an n-array CAN DID AT E
which gives either the index of the candidate edge in INDOUT with positive sign if the
edge is from the forward star of v or the index of the candidate edge in IN DIN with
negative sign if the edge is from the backward star.
All algorithms outlined before have to search for eligible edges. Using the above data
structure for storing the network, testing the eligibility of the candidate edge of node v
involves the evaluation of CANDIDATE(v), and assuming CANDIDATE(v) = j > 0,
the determination of
w ~ HEAD(INDOUT(j)).
50
Then we have to test for
d(v) > dew) and (3.1)
PREFLOW(INDOUT(j)) < CAPACITY(INDOUT(j)). (3.2)
For CANDIDATE(v) = j < 0, we determine
w .- TAIL(INDIN( -j))
and test for (3.1) and
PREFLOW(INDIN(-j)) > O. (3.3)
I I ~ I I 8
:(v,W ~(v,w v w
--!r~='
I
~--47 0Wl041l!l1t1
I iE
-r-~~.
Fig. 1 Standard data-structure for storing a network
51
The above formulas demonstrate the relatively large number of accesses necessary to
test eligibility due to the rather general data structure. Moreover, many tests have to be
performed before a PUSH or a RELABEL step can be done. Hence, there is a need for
modifying the data structure to simplify the special testing procedure to be performed in
preflow-push implementations. Obviously, the determination of the head node or the tail
node, respectively, of the candidate edge cannot be avoided, but by using a more involved
data structure, the testing of the preflow value of the edge and the index computation
can be reduced significantly.
Now we consider a forward star representation using the m-arrays HEAD, P REF LOW
and CAPACITY, where the information on the edges in a forward star of a node v E V
is stored in the entries indexed by OUT( v), ... ,OUT( v + 1) -1, where OUT is an (n + 1)-
array and OUT( n + 1) = m + 1. The backward star is represented in a similar way using
an m-array TAIL and an (n + I)-array IN. Here, 5-(v) = {(v, HEAD(i)) I OUT(v) :=:;
i < OUT(v + I)} and 5+(v) = ((TAIL(i),v) I IN(v) :=:; i < IN(v + I)}. Moreover, an
m-array INDIN is introduced to link the backward star, i.e. the array TAIL, to the
forward star, i.e. the arrays HEAD, PREFLOW and CAPACITY as follows.
For any j with IN (w) :=:; j < IN (w + 1), the following properties hold with v = T AIL(j):
HEAD(INDIN(j)) = w
PREFLOW(INDIN(j)) = g(v, w) and
CAPACITY(INDIN(j)) = c(v,w).
Moreover, flags are introduced to mark those edges which are not contained in the residual
network, thereby avoiding the testing steps (3.2) and (3.3). By construction, for every
edge (v, w) E E, there exists an index i and an index j such that
HEAD(i)
TAIL(j)
w with OUT(v) :=:; i < OUT(v + 1) and
v with IN(w) :=:; j < IN(w + 1).
Now, the flags are set in the following way:
• if g(v,w) = c(v,w) then HEAD(i) is set to -w instead of w
• if g(v, w) = 0 then TAIL(j) is set to -v instead of v.
To enable an efficient maintenance of the integrity of this data structure we introduce
another m-array IN DOUT with
INDOUT(INDIN(k)) = Hor k = 1, ... ,m.
52
The complete data structure is shown in Figure 2 (where for reasons of clarity we have
omitted the display of the CANDIDATE array).
I I § ~ ~l!I
- -
~~='v,w v,w w
Fig. 2 New data-structure for storing the network
Using this data structure, testing an edge for eligibility reduces to test the label condition
(3.1) for those neighbors, which are not flagged. No more computation of indices is
necessary. This saving has to be paid for by a more complex updating of the data structure
after a PUSH step, where the flags have to be set appropriately. This procedure is
demonstrated in an example, where we assume the situation as in the above figure and a
saturating push along edge (v, w), With k = CAN DID AT E (v) we have to perform the
following steps:
HEAD(k) +- -HEAD(k)(= -w) and
TAIL(INDOUT(k)) +- -TAIL(INDOUT(k))(= v)
For an implementation of the incremental excess scaling algorithm, the data structure used
should provide additional information on the inclusion of an edge in the current surface
network No<. This is achieved by a straightforward modification of the data structure
described above.
So far, for every edge (v, w), three different states out of four representable states are
53
possible. The following table gives an overview of these stages with their representation
and meaning:
Value in Value in
State FROM TO Meaning
a) v w The edge (v,w) has positive flow,
but is not saturated.
b) v -w The edge (v,w) is saturated.
c) -v w The flow on edge (v,w) is zero.
d) -v -w Not used.
State d) is so far not active and is now being used to mark those edges, which are not
members of E"'. Since with the above data structure the test for eligibility of an edge
is performed on unflagged neighbors only and for state d) the head and the tail node
are flagged, edges being in state d) will never be considered during a PUSH. Therefore,
we have a criterion at hand which provides a natural and efficient means to eliminate a
particular edge from a (residual) network.
3.2 Storing the Active Nodes
When implementing the most basic variants of the preflow-push algorithm standard list
structures can be used for storing the set of active nodes and for supporting different
selection rules. The computational study of Derigs and Meier [5] extensively describes
the impact of using
- a stack for implementing the last-in-first-Qut selection rule,
- a queue for implementing the first-in-first-out selection rule,
- a dequeue ("double-ended queue") for implementing a hybrid FIFO/LIFO scheme first
used in shortest path algorithms and
- a linked list for implementing the highest-first selection rule.
When implementing the (incremental) excess scaling algorithm, a data structure is
needed which enables identification of nodes with large excess as well as an efficient
update at the beginning of each SCALE phase. For example, keeping all active nodes in
an unordered list is a very easily updatable data structure. Yet, the selection of an active
54
node is very inefficient since every time the whole list has to be scanned. Therefore, a
more complex data structure is needed for an easy selection.
In a first set of computational tests, we have implemented a bucket structure for storing
the set of active nodes. Recently, Ahuja and Orlin [1] proposed a special list structure
which, as our experience has shown, is more efficient than our bucket approach. Using this
list structure, the set of active nodes with large excess to be scanned during the current
phase is kept as the union of sets of nodes with identical label. Then, the set of nodes
with highest or lowest label, respectively, is processed first. Each set is stored as a doubly
linked list and for each possible d-value we maintain a pointer to the list of nodes bearing
this label. Moreover, a special pointer addresses the nonempty set containing the nodes
with highest or lowest label, respectively. Figure 3 illustrates this data structure.
i
1IM1tNI ~0
~ ~0
~0 0
rv~ + I tNEXf
IIII IIII III II
::::: ~ Hl ~. t j~0 00
fit r tPREY
IIII IIII III II
• t ~ J J0
Fig. 3 Data-structure for storing the nodes with large excess for the (incremental) excess
scaling algorithm
The time bounds established by Ahuja and Orlin [1] are valid if the underlying data
structure can be maintained in O(n2 + n2 (logk U + 1)). We now show that this bound
remains valid also when applying global relabeling strategy. Assume a global relabeling
with gap z. Then, all sets of nodes v with z < d( v) ~ n have to be joined into one set. This
55
operation requires O( n) time. Since there are at most n global relabeling steps, the total
amount spent on global relabeling is O(n2 ) using the above data structure. Therefore,
applying RELABEL-GLOBAL does not violate the time bound of the entire algorithm.
4 Computational results
The computational results presented below are based on a series of networks with different
sizes and densities randomly generated using the RMFGEN generator. The special struc-
ture of the networks generated by RMFGEN is described in Goldfarb and Grigoriadis [10]
as follows:
"This program accepts values for four parameters a, b, CI and C2(> CI) and generates
networks with the structure depicted in Figure 4:"
e e e
e • •
Fig. 4 Structure of RMFGEN-generated networks
The graph can be visualized as having b frames, each of which has a 2 nodes at the lattice
points of the square of side a. Each frame is a symmetric subgraph: each node in a frame
is connected to its neighbors on the lattice by a pair of oppositely directed arcs. There
are 4ab( a-I) such 'in-frame' arcs with capacity equal to c2a 2. There are a2 arcs from
the nodes of one frame to a pseudorandom permutation of the nodes of the next (one
to one), with pseudorandom integer capacities in the range [CI' C2]' Similarly, there are
a 2 arcs from a frame to its preceding one. This portion of the graph is not symmetric.
The generated graphs have n = a2b nodes and m = 6a 2b - 4ab - 2a2 arcs, source node
s = 1 (at the lower left of the first frame in figure 4) and sink t = a 2 b (at the upper right
of the last frame in figure 4). Nodes have indegree and out degree of 3, 4, 5 or 6. Since
min = 6 - 4/a - 2/b ~ 6 for sufficiently large a and b, these form a relatively dense set of
56
'sparse' instances. Their solution involves flow augmenting paths whose length is at least
b. The minimum cut is located between two consecutive frames and it contains a 2 arcs."
For each problem defined by fixing the parameters a and b, ten networks, i.e. flow
problems, were generated with different seeds for the pseudorandom number generator.
However, the capacity range was fixed leI, C2] = [1,1000] for all problems. Each problem
was solved by all methods before the next example was generated. In the following,
we report the average running times in CPU-seconds on a IBM RISC 6000/320. All
codes are written in ANSI-FORTRAN and compiled using AIX-FORTRAN with option
OPTIMIZE.
Note that for all tables presented in the sequel of this study, we have prepared and run
a dedicated series of randomly generated problems. Under this condition, entries for the
same algorithm and problem dimension in two different tables are based on two different
test runs.
In a first step, we tested the influence of the new data structure for storing the network
on the behavior of the algorithm. We considered the implementation GOLDRMF of the
preflow-push algorithm which was found to be best on RMFGEN-generated graphs (ef.
[5]). This implementation uses the old data structure for storing the network depicted in
Figure 1 and highest-Iabel-selection. The program was then modified to handle the new
data structure depicted in Figure 2. The (average) running times of the old and the new
implementation of the preflow-push algorithm are given in Table l.
From these results, we see that the new data structure for storing the network saves
already roughly 30% of the computing time for the basic preflow-push implementation.
Thus, for the remainder of this study, we are only considering the new data structure.
In Table 2, we present our computational results for the excess scaling algorithms,
Algorithm 3 and Algorithm 4 from section 2. It is evident that the RELABEL-GLOBAL
is a time saver here too, and that the STACK-PUSH/RELABEL variant is slightly better
than the basic excess scaling approach, when RELABEL-GLOBAL is used.
In Table 3, we display the computational results for the incremental excess scaling
algorithm - algorithm 5 and algorithm 6 from section 2. Here, the message is that the
RELABEL-GLOBAL is outperforming the other variants, again, and applying the SPR-
strategy to the incremental excess scaling algorithm saves more computing time than
applying it to the excess scaling algorithm. Yet, the pure excess-scaling algorithm is
superior to the incremental excess-scaling algorithm.
The above results seem to indicate that the new variants of the preflow-push algorithm
are inferior to the basic implementation GOLDRMF. To get a better basis for such an
evaluation, we compared the best implementations out of their respective class:
57
I GOLDRMf GOLDRMF
A B New data structure Old data structure
4 2 .003 .001
4 .002 .008
6 .009 .010
8 .014 .016
6 2 .008 .010
4 .017 .024
6 .030 .045
8 .044 .059
8 2 .023 .026
4 .039 .055
6 .070 .090
8 .112 .153
TOTAL 11 .371 .497
Table 1: Running times with different data structures for storing the network.
- GOLDRMF
- EXSCAL the excess scaling algorithm with STACK- PUSH/RELABEL
- INEXSCAL the incremental excess scaling algorithm with STACK-PUSH/RELABEL
on a set of problems with larger dimension. Needless to say that we used RELABEL-
GLOBAL in all these implementations. The results are depicted in Table 4.
The following two tables report our investigation on the influence of certain conditions
on the candidate implementations:
- Table 5 gives the results for the implementations without RELABEL-GLOBAL,
- Table 6 compares the results for different capacity ranges.
5 Concluding Remarks
Our computational tests indicate that the basic implementation GOLDRMF with highest
first selection and RELABEL-GLOBAL outperforms its competitors, and in general, the
basic preflow-push implementation GOLDRMf is superior to the theoretically "faster"
refinements. An interesting result is that the capacity range seems to be of minor impact
58
IMPLEMENTATIONS OF EXCESS
SCALING ALGORITHM
NO SPR-STEP SPR-STEP
NORMAL RELABEL- NORMAL RELABEL-
A B RELABEL GLOBAL RELABEL GLOBAL
4 2 .008 .003 .010 .003
4 .024 .010 .027 .007
6 .080 .016 .090 .013
8 .161 .021 .176 .021
6 2 .043 .010 .050 .010
4 .186 .023 .200 .026
6 .561 .051 .585 .049
8 .894 .086 .931 .069
8 2 .132 .020 .149 .022
4 .521 .060 .553 .056
6 1.742 .127 1.808 .116
8 2.700 .192 2.789 .169
TOTAL II 7.052 I .619 I 7.368 I .561
Table 2: Running times of the different variants of the excess scaling algorithm.
on the computing time in contrary to what one would expect due to the theoretical
complexity. Thus, the preflow-push algorithms allow robust and stable implementations.
The RELABEL-GLOBAL strategy, although not leading to a reduced theoretical com-
plexity, empirically seems to reduce the computational effort significantly, since it reduces
the number of relabelings without the need for substantial overhead, while the practical
success for the theoretically superior refinemeuts is questionable.
For nearly all variants of the preflow push algorithm, there are dynamic tree imple-
mentations yielding better theoretical bounds.
Ahuja et al. {2] gave another excess scaling algorithm, the so-called wave-scaling
algorithm. This method runs in O( nm + n 2 jlogk U) time, and based on this result,
Cheriyan et al. [4] have given another version of their incremental excess scaling algorithm
running in O( -{/nyrn + n 2 jlogk U + mn) time.
59
IMPLEMENTATIONS OF THE INCREMENTAL
EXCESS SCALING ALGORITHM
NO SPR-STEP SPR-STEP
NORMAL RELABEL- NORMAL RELABEL-
A B RELABEL GLOBAL RELABEL GLOBAL
4 2 .014 .006 .010 .005
4 .035 .019 .030 .010
6 .077 .033 .086 .021
8 .156 .048 .172 .032
6 2 .060 .020 .051 .012
4 .186 .063 .190 .034
6 .467 .113 .535 .062
8 .766 .172 .876 .090
8 2 .123 .056 .162 .023
4 .434 .156 .542 .076
6 1.286 .269 1.650 .141
8 1.994 0400 2.477 .200
TOTAL \\ 5.598\ 1.355 \ 6.781 \ .706
Table 3: Running times of the different variants of the incremental excess scaling algo-
rithm.
60
A I B II GOLRMF I EXSCAL I INEXSCAL
4 2 .003 .004 .004
4 .002 .010 .010
8 .009 .018 .032
16 .025 .050 .081
32 .046 .092 .199
64 .081 .229 .564
6 2 .005 .011 .011
4 .016 .032 .032
8 .034 .077 .092
16 .105 .184 .247
32 .206 .319 .676
64 .386 .740 1.760
8 2 .015 .025 .025
4 .038 .067 .080
8 .107 .170 .206
16 .249 .450 .549
32 .685 1.202 1.971
64 1.481 2.093 5.494
12 2 .068 .073 .071
4 .140 .196 .254
8 .349 .533 .652
16 .748 1.280 1.790
32 2.296 3.899 5.439
64 5.684 7.440 15.493
16 2 .162 .157 .142
4 .401 .438 .543
8 1.147 1.515 1.888
16 2.585 4.088 5.125
32 5.139 9.091 15.636
64 21.618 42.469 76.134
TOTAL II 43.830 I 76.952 I 135.200
Table 4: Running times of the best representatives of each class.
61
A I B II GOLDRMF I EXSCAL I INEXSCAL
4 2 .004 .010 .011
4 .018 .029 .031
6 .060 .089 .084
8 .116 .177 .163
6 2 .029 .051 .050
4 .129 .199 .184
6 .392 .588 .510
8 .633 .932 .818
8 2 .097 .163 .152
4 .368 .556 .520
6 1.204 1.795 1.563
8 1.924 2.815 2.395
TOTAL II 4.974 I 7.404 I 6.481
Table 5: Running times without using RELABEL-GLOBAL.
CAPACITY-RANGE: [1,1000J CAPACITY-RANGE: [1,1000000J
AI B GOLDRMF I EXSCAL I INEXSCAL GOLDRMF I EXSCAL I INEXSCAL
4 2 .002 .003 .003 .001 .007 .006
4 .003 .006 .012 .007 .010 .014
6 .003 .016 .017 .009 .016 .024
8 .011 .019 .033 .009 .017 .031
6 2 .009 .012 .012 .009 .010 .014
4 .017 .025 .034 .019 .035 .039
6 .030 .061 .056 .032 .053 .064
8 .038 .070 .082 .046 .061 .087
8 2 .014 .023 .024 .023 .024 .027
4 .041 .065 .074 .045 .070 .078
6 .066 .125 .132 .065 .112 .135
8 .110 .159 .203 .105 .172 .224
TOTAL II .344 1 .584 1 .682 I .370 I .587 1 .743
Table 6: Inpact of the capacity-range on the running time.
62
References
[IJ Ahuja, R. K., and J. B. Orlin. A fast and simple algorithm for the Maximum Flow
Problem. Operations Research 37: 748-759, 1989.
[2J Ahuja, R. K., J. B. Orlin, and R. E. Tarjan. Improved time bounds for the Maximum
Flow Problem. SIAM Journal of Computing, 18: 939-954, 1989.
[3J Cheriyan, J., and S. N Maheswari. Analysis of preflow push algorithms for maximum
network flow. SIAM Journal of Computing, 18: 1057-1086, 1989.
[4J Cheriyan, J., T. Hagerup, and K. Mehlhorn. Can a maximum flow be computed
in O(nm) time? To appear in Proceedings of the 17th International Colloquium on
Automata, Languages and Programming, London, 1990.
[5] Derigs, D., and W. Meier. Implementing Goldberg's max-flow algorithm - A com-
putational investigation. ZOR - Methods and Models of Operations Research, 33:
383-403, 1989.
[6] Edmonds, J., and R. M. Karp. Theoretical improvements in algorithmic efficiency
for network flow problems. Journal of the ACM, 19: 248-264, 1972.
[7] Ford, 1. R., and D. R. Fulkerson. Maximal flow through a network. Canadian Journal
of Mathematics, 8: 399-404, 1956.
[8] H. N. Gabow. Scaling algorithms for network problems. Journal of Computer and
System Sciences, 31: 148-168, 1985.
[9] Goldberg, A. V., and R. E. Tarjan. A new approach to the Maximum Flow Problem.
Journal of the ACM, 35: 921-904, 1986.
[10] Goldfarb, D., and M. D. Grigoriadis. A computational comparison of the Dinic and
Network Simplex Methods for maximum flow. Annals of Operations Research, 13:
83-123, 1988:
[11] W. Meier. Neue Ansatze zur Bestimmung maximaler Fliisse in Netzwerken. Diplo-
marbeit, Dniversitat Bayreuth, 1987.
A Cutting Plane Algorithm for the Single
Machine Scheduling Problem with Release Times
G.L. Nemhauser*
M.W.P. Savelsberght
Abstract
We propose a mixed integer programming formulation for the single machine
scheduling problem with release times and the objective of minimizing the weighted
sum of the start times. The basic formulation involves start time and sequence
determining variables, and lower bounds on the start times. Its linear programming
relaxation solves problems in which all release times are equal. For the general
problem, good lower bounds are obtained by adding additional valid inequalities
that are violated by the solution to the linear programming relaxation. We report
computational results and suggest some modifications based on including additional
variables that are likely to give even better results.
1 Introduction
Recently developed polyhedral methods have yielded substantial progress in solving many
important NP-hard combinatorial optimization problems. Some well-known examples are
the traveling salesman problem [Grotschel and Padberg 1985a, Grotschel and Padberg
1985b], the acyclic subgraph problem [Junger 1985], and large scale 0-1 integer program-
ming problems [Crowder, Johnson and Padberg 1983]. See Hoffman and Padberg [1985]
and Nemhauser and Wolsey [1988] for general descriptions of this approach.
However, for mixed-integer problems, in particular machine scheduling, polyhedral
methods have not been nearly so successful. Investigation and development of polyhedral
'Georgia Institute of Technology, Atlanta. Supported by NATO Collaborative Research Grant No.
901057 and by NSF Research Grant No. ISI-8761183.
tEindhoven University of Technology. Supported by NATO Collaborative Research Grant No. 901057
and by the Netherlands Organization for Scientific Research through NATO Science Fellowship Grant
No. N62-316.89.
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgfil et al.
© Springer-Verlag Berlin Heidelberg 1992
64
algorithms do not perform well on certain problem types in this class, for instance job
shop scheduling. The major difficulty is obtaining tight lower bounds which are needed
to prove optimality or even optimality within a specified tolerance.
Relatively few papers and reports have been written in this area. Balas [1985] pi-
oneered the study of scheduling polyhedra with his work on the facial structure of the
job shop scheduling problem. Queyranne [1986] completely characterized the polyhe-
dron associated with the nonpreemptive single machine scheduling problem. Dyer and
Wolsey [1990] examined several formulations for the single machine scheduling problem
with release times. Queyranne and Wang [1991] generalized Queyranne's results to the
nonpreemptive single machine scheduling problem with precedence constraints. Sousa
and Wolsey [1989] investigated time indexed formulations for several variants of the non-
preemptive single machine scheduling problem. Finally, Wolsey [1989] compared different
formulations for the single machine scheduling problem with precedence constraints.
In this paper, we propose a formulation that involves start time and sequence deter-
mining variables for the nonpreemptive single machine scheduling problem with release
times and we develop a cutting plane algorithm based on this formulation and several
classes of valid inequalities. The paper is organized as follows. In the next section, we
formally introduce the single machine scheduling problem with release times and propose
a mixed integer programming formulation. In the subsequent sections, we discuss a lin-
ear relaxation, various classes of valid inequalities, separation heuristics, and the cutting
plane algorithm we have implemented. In the final sections, we present computational re-
sults and possible enhancements that are based on using additional variables and column
generation.
2 The single machine scheduling problem with re-
lease times
A set J of n jobs has to be processed without interruption on a single machine that can
handle at most one job at a time. Each job j E J becomes available at its release time
rj and requires a processing time Pj. The problem is to find a feasible schedule that
minimizes the weighted sum of the completion times. In the sequel, we assume that both
rj and Pj are nonnegative integers and the jobs are numbered in order of nondecreasing
release time, i.e., 0 :S r1 :S r2 :S ... :S T'n.
For any ordering 7r of the jobs, there exists one feasible schedule that dominates all
others. In this schedule, called an active schedule, each job is processed as early as possible,
given the processing order. If t,,(j) denotes the start time of job 7r(j), the active schedule
65
for 1C' is
t".(l) = r"'(l);
t".(j) = max(r".(j), t 1f (j-l) + P"'U-l») for j = 2, ... , n.
The above observation shows that we can restate the nonpreemptive single machine prob-
lem with release times as: find a permutation and associated active schedule for which
the objective function is minimum. Therefore, to obtain a valid formulation it suffices to
find a linear inequality description of the set of permutations and of the active schedule
associated with a given permutation.
Let Dij be equal to 1 if job i precedes j and 0 otherwise. Then D E Bn(n-l) is a
permutation if and only if it satisfies
(2.1)
The inequalities in (2.1) are called triangle inequalities. Grotschel, Junger, and Reinelt
[1984, 1985] study the convex hull of solutions to these inequalities and present a cutting
plane branch and bound algorithm for finding a minimum weight permutation.
A linear description of the active schedule associated with a given permutation is
provided by the following theorem. Note that bik + Dkj - 1 :5 DikDkj, which equals one if
and only if Dik = Dkj = 1.
Theorem 1 The following linear program determines the active schedule associated with
a permutation given in terms of b-variables. (For convenience let Djj = 1.)
min L tj
l:$;j:$;n
subject to (2.2)
tj 2: riDij + L Pk( Dik + bkj - 1) + L Pkbkj 1:5 i, j :5 n.
k<i,kfj k?i,kfj
Proof. Clearly the active schedule associated with b is a solution. To show that the
active schedule associated with b is an optimal solution, consider the dual linear program
given by
subject to
2:: Uij = 1
l~i~n
Rn2
U E +.
66
1 ::; j ::; n;
Observe that any feasible schedule consists of a number of blocks, each consisting of a set
of jobs that are processed continuously. The value of the dual solution given by
u .. _ {I if job i is the first job in the block that contains job j
'3 - 0 otherwise
is equal to the sum of the start times of the active schedule. 0
Combining (2.1) and (2.2), we obtain the following mixed integer programming formula-
tion for the single machine scheduling problem with release times.
min'" w·t·L..t J J
l~j~n
subject to
k<i,kjc.j
Sij + Sji = 1 1::; i < j ::; n;
S E Bn(n-l).
k~i,kfj
(2.3)
Note that it is better to put a job k with k < i and rk = ri in the second sum of the
constraints defining the start time tj of job j. However, for simplicity of presentation, we
will not do this explicitly. Note also that in the absence of degeneracy, exactly one of the
n constraints defining the start time tj of job j is satisfied with equality in an optimal
solution to (2.3) ..
3 A linear programming relaxation
The first step in the development of a cutting plane algorithm is the definition of an initial
linear programming relaxation. After relaxing the integrality condition on the S-variables
in (2.3) to obtain a linear program, we made three additional modifications, one to reduce
its size, the others to strengthen it.
67
To keep the size of the linear programming relaxation reasonable, the triangle in-
equalities are dropped. As a consequence, an integral solution to the linear programming
relaxation may not be feasible.
An examination of the time constraints reveals that they all have the following struc-
ture. A first part that establishes a 'base' release time for the constraint, a second part
that deals with all the jobs that have a release time that falls before the base release time
and a third part that deals with all the jobs that have a release time that falls after the
base release time.
Observe that if job k has a release time before the base release time, but has an earliest
possible completion time after the base release time, i.e., 'rk < 'ri and 'rk + Pk > 'ri, then
Pk(Oik + Okj -1) is dominated by ('ri - 'rk)(Oik + Okj -1) + ('rk + Pk - 'ri)Okj. In fact, we
no longer distinguish between jobs that have a release time before the base release time
and jobs that have a release time after the base release time. Instead, we distinguish
between processing time that may fall before the base release time and processing time
that must fall after the base release time. Furthermore, the base release time part 'riOij
can be strengthened by observing that for i < j, it is dominated by 'ri and that for i > j,
it is dominated by 'rj + ('ri - 'rj)Oij.
Consequently, we take the initial linear programming formulation to be the following.
min L: Wjtj
l~j~n
subject to
+
+ L: PkOkj
k?i,ki-j
t· > 'r. + ('r. - 'r·)o· +3 - 3 , 3. '3
+
+ L: PkOkj
k?i,k"l-j
Oij + Oji = 1
oE R~(n-l).
1 :::; i < j :::; n;
1 :::; i :::; j :::; n;
(3.1 )
1 :::; j < i :::; n;
68
4 Equal release times
When all release times are equal (without loss of generality rk = 0 for k E J), (3.1)
reduces to
min L Wjtj
l~j~n
subject to
tj;::: L PkOkj 1:::; j :::; n;
l~k#-j~n
Oij + Oji = 1 1 :::; i < j :::; n;
oE Rn(n-l)
+ ,
and we have the following result.
(4.1)
Theorem 2 When rk = 0 for all k E J, the optimal objective value of (4.1) equals the
value of a minimum weight schedule.
Proof. Smith's rule [Smith 19.56] says that if the jobs are in order of decreasing ratio ~,
PJ
then (1,2, ... ,n) is an optimal sequence giving an objective value El~j~n Wj(El~i<j Pi) ..
The dual of (4.1) is
max L Vij
l~i<j~n
subject to
Uj = Wj 1:::; j :::; n;
PiUj - Vij ;::: 0 1:::; i < j :::; n;
pjUi - Vij ;::: 0 1:::; i < j :::; n;
n(n-l)
U E R~,v E R 2
Taking Vij = Pi'Uij produces a dual feasible solution, since i < j =? ~ ;::: ~ =? pjWi -
PiWj ;::: 0 =? pjUi - Vij ;::: 0, with objective value l:l~j~n Wj(El~i<j Pi). By weak duality,
the optimal value of (4.1) is at least as large. 0
Since (4.1) does not include the triangle inequalities, it contains infeasible integer solu-
tions. If we add the triangle inequalities, then all solutions with integral 0 are feasible.
The proof of Theorem 2 shows that there is an integral optimal solution for any W ;::: 0,
and the problem is unbounded if any component of W is negative. Hence, we have the
following corollary.
69
Corollary 1 When rk = 0 for all k E J, the constraints of (4.1) and the triangle inequal-
ities give the convex hull of feasible solutions.
Queyranne [1986] shows that in t-space, the convex hull of the set of feasible schedules is
defined by following system of linear inequalities
~:::>jtj~2::Pj 2:: Pi VSr;J,S:f.0; (4.2)
jES jES iES,i<j
Proposition 1 The linear inequalities (4.2) are nonnegative linear combinations of the
inequalities (4.1).
Proof.
tj ~ 2:: Pk 8kj ~ 2:: Pk 8kj.
kEJ\{i} kES\{j}
Hence
2::Pjtj ~ 2::Pi 2:: Pk 8 kj
jES jES kES\{i}
= 2::Pj 2:: Pk(8 kj + 8 jk )
jES kES,k<j
= 2::Pj 2:: Pk· 0
jES kES,k<j
5 Valid inequalities
In this section, we derive several classes of valid inequalities. The first class of valid
inequalities establishes lower bounds on the position of jobs in an optimal schedule based
on a well-known dominance criterion that says that if ri + Pi S rj, then j will not be the
first job in any optimal schedule.
Dominance inequalities. Let Qj = minkEJ:Tk+Pk::;TJ (rk + Pk) if it is defined. Then,
8·· > 1'J -
is a valid inequality.
If, as we have assumed throughout, the jobs are given In order of increasing release
times, it is very simple to generate all possible dominance inequalities in advance. Since
70
the number of dominance inequalities is probably small, we can add all of them to the
original formulation.
The second class of valid inequalities is derived directly from the mixed integer pro-
gramming formulation. Observe that Oik + Okj - 1 can be negative based on the values
of the sequence determining variables. Therefore, we consider time constraints that only
use a subset of the terms that involve two sequence determining variables.
Subset inequalities. Let rij denote the base 7'elease time, i. e., ri if i ::; j an d rj + (r; -
l' j )Oij if i > j, and let S ~ {1, ... , i - 1} \ {j}, then
+
+ L PkOkj
i$k$n,k¥-j
is a valid inequality for all i, j E J.
The next four classes of valid inequalities establish different base release times. After
presenting them, we will indicate how they can be strengthened.
Summation inequalities I. Lei 8 ~ J\ {j} and let rs = minkEs{rd be such that
l' = rs + ~kESPk > rj, then
tj 2:: rj + (1' - rj)(L Okj -181 + 1)
kES
is a valid inequality.
Proof. The validity follows from the observation that rs ::; rk for k E S and that for any
feasible sequence ~kES Okj > 181- 1 if and only if all jobs in S precede j. 0
Sequence inequalities I. Let j E 8 ~ J and let 71' be a permutation of S such that
11"(ISI) = j and l' = rrr(1) + ~kES\{j}Pk > rj, then
tj 2:: rj + (1' - rj)( L O".(k)7r(k+1) - lSI + 2)
l$k<lsl
is a valid inequality.
Proof. The validity follows from the observation that any feasible schedule for which
~l$k<lsl 8rr (k)rr(k+1) > lSI - 2 contains (71'( 1),71'(2), ... ,71'( lSI)) as a subsequence. 0
71
Example 1 rj pj Wj
1 1 4 1
2 2 3 2
3 6 2 3
4 7 2
Table 1: Problem instance
Oij 1 2 3 4
1 0.00 0.25 0.25
t· 1 2 3 4J 2 1.00 1.00 1.00
8.00 2.00 6.00 8.00 3 0.75 0.00 1.00
4 0.75 0.00 0.00
Table 2: Solution to the initial linear programming relaxation
Consider the problem instance given in Table 1. The optimal sequence is (2,3,4,1) with
associated start times tl = 10, t2 = 2, t3 = 6 and t4 = 8. The solution to the initial
linear programming relaxation is given in Table 2. The solution violates the summation
I inequality (withj = 3 and S = {1,2})
t3 ;::: '1'3 + ('1'1 + PI + P2 - '1'3)(013 + 023 - 1) = 4 + 2(013 + 023)
and the sequence I inequality (with j = 3,S = {1,2} and 1r = (2,1))
The two inequalities above replace ri in the base release time rj + (ri - rj) by '1' to
get rj + ('1' - rj). The two inequalities below replace rj in the base release time to get
r+(ri- r ).
Summation inequalities II. Let S ~ J\ {j} and let rs = minkEs{rd be such that
rj < '1' = rs + EkESPk :::; ri, then
tj ;::: r(L Okj -·ISI + 1) + (ri - r')(L Okj + Oij -lSI)
kES kES
is a valid inequality.
Proof. Same as for Summation inequalities I and tj ;::: ri if Oij = 1. 0
Sequence inequalities II. Lei j E S ~ J and let 1r be a permutation of S such that
1r(ISI) = j andrj < '1' = '1'11'(1) + I::"ES\{j} p" :::; ri, then
tj ;::: '1'( L 011'(k)11'(k+1) -lSI + 2) + (ri - '1')( L 011'(k)11'(k+l) + Oij -lSI + 1)
l$k<lsl l$k<lsl
72
is a valid inequality.
Proof. Same as for sequence inequalities I and tj ;:::: 7'; if Oij = 1. 0
As mentioned before, the four families of valid inequalities establish different base
release times. Therefore, the inequalities can be strengthened by considering the jobs
k E J \ S that have not been used in the definition of the base release time and adding
the term {7'k + Pk - 7')8 k j if 7'k < 7' and 7'k + Pk > 7' or the term Pk8kj if 7'k ;:::: 7'.
Example 2
Again, consider the problem instance given in Table 1 and the solution to the initial
linear programming relaxation given in Table 2. The summation II inequality {with
S = {2},j = 1 and i = 3) and sequence II inequality (with S = {I, 2}, 11' = (2, l),j = 1,
and i = 3) are the same, namely
This inequality is not violated, but it can be strengthened by adding P3831 + P4841 to the
right hand side. The strengthened inequality is violated.
Note that all the valid inequalities we have derived so far relate start times to release
times and processing times, i.e., the data that specify the problem instance. However,
if at some point it is established that some job i precedes another job j, for instance if
in the context of a branch and bound algorithm branching is done by variable fixing, we
have the following result.
Precedence inequalities I. Ifjob i precedes job j, then fo7' any S 5:;; J\ {i, j}
tj ;:::: ti + Pi + I>doik + Okj - 1)
ke8
is a valid inequality.
A natural nonlinear inequality that relates start times is tj ;:::: (ti+Pi)8 ij . This inequal-
ity can be linearized by replacing ti by any lower bound Ii to obtain tj ;:::: (Ii +Pi)Oij. The
trivial lower bound Ii = 7'; is useless, since it results in an inequality that is dominated by
the inequalities in the original formulation. All other known lower bounds on ti involve
sequence determining variables, which again results in a nonlinear inequality. However,
in this case the nonlinear terms involve precisely two sequence determining variables and,
as in the formulation (2.2), can be linearized using 8ij Oki ;:::: Oij + 8kl - 1.
73
Precedence inequalities II. If job i precedes job j and t; ;:::: f(D), then a valid inequality
zs
where 7( D) is obtained from f( D)Oij by replacing all nonlinear terms D;jDkl by D;j + Dkl - 1.
Example 3
Again, consider the problem instance given in Table 1. The solution to the initial linear
programming relaxation plus the violated summation I and violated sequence I inequalities
of Example 1 is given in Table 3. The solution violates the precedence II inequality
Dij 1 2 3 4
1 0.07 0.21 0.21
t· 1 2 3 4J
2 0.93 1.00 1.00
7.86 2.21 6.42 8.00 3 0.79 0.00 1.00
4 0.79 0.00 0.00
Table 3: Solution to the extended linear programming relaxation
where the lower bound on t3 is given by the sequence I inequality of Example 1.
6 Separation
Any linear programming based algorithm that has to deal with an exponential number
af inequalities will start with a partial description of the set of feasible solutions and will
subsequently try to identify and add violated inequalities. The problem of identifying
violated inequalities is known as the separation problem. Formally, if we are given a
polyhedron P E Rn and a point cERn, the separation problem [Grotschel, Lovasz, and
Schrijver 1981] is the one of deciding whether c E P and, if not, to find a separating
hyperplane, i.e., an inequality that is satisfied by all points c' E P but violated by c.
In the remainder of this section, we will discuss the separation procedures that are
implemented in our cutting plane algorithm. The solution to the current linear program
is denoted by (t*, D*).
The triangle inequalities. The triangle inequalities of the linear ordering polytope are
74
handled by enumerating all n(n - l)(n - 2)/3 of them and identifying those that are
violated.
Incorporation of the following observations increase the efficiency of the enumeration.
First, any permutation of three elements has a representation in which the elements are
in increasing order or in decreasing order. Secondly, as soon as we detect that bij = 0 or
bij + bjk ~ 1, we know the inequality will not be violated.
The subset inequalities. For each of the n 2 time constraints in the original formulation,
we check whether it contains terms involving two sequence determining variables that
currently have a negative contribution, i.e., bik + bkj < 1, and, if so, whether the deletion
of these terms would lead to a violated inequality.
The summation inequalities I. For each job j, we try to find a violated inequality. The
separation heuristic is based on two properties of a set S· that, for a given job j, induces
a summation inequality, if one exists, for which the violation is maximum.
2. If b kj = 1, then job k will be in So, unless it causes a conflict with property (1).
In order to not have to worry about property (1), we construct sets Sk for each job k
that contain, besides job k itself, only jobs with a release time larger than rk, and in the
end take S to be the best among the S'k'S we have constructed. The set Sk initially contains
job k and all jobs 1 (l > k) for which o/j = 1, giving a base release time rj + (r - rj){),
with r = rk + :EIESk PI and {} = O;;j. Next, we try to expand Sk by adding jobs I (l > k)
with 0 < bi; < 1. Observe that any job 1 (l > k) with 0 < bi; < 1, if added, will increase
r by PI and decrease {} by 1 - 0ij' Note that this approach does not necessarily find an
optimal S·.
The sequence inequalities l. Since Oij = 1 - Oji, in a fractional solution, it is always
possible to concentrate on a sequence determining variable bij with 0 < bij ~ 0.5 and try
to identify a violated inequality that, if added to the current formulation, will force that
variable to go down. The other main idea embedded in the separation heuristic for the
sequence inequalities is that of trying to prove a sequence is locally optimal by disproving
optimality for sequences obtained from this sequence by relocating one job.
Both ideas are illustrated by the following example. Suppose that we believe that the
sequence (1l'(1), 1l'(2), .... , 1l'(k), 1l'(k + 1), ... , 1l'(n)) is optimal, but b;(k)1l"(k+1) is fractional.
Then, by considering the subsequences (1l'( i), 1l'( i +1), ... ,1l'( k+ 1), 1l'( k)) for i = 1, ... , k-l,
we try to identify a violated sequence inequality for the sequence (1l'(1), 1l'(2) , ... ,1l'( k +
1),1l'(k), ... ,1l'(n)) that will force O,,(k+l)1l"(k) to go down.
75
We consider three candidates for an optimal sequence: (1) the sequence associated
with the best feasible schedule, (2) the sequence suggested by the current values of the
start time variables, (3) the sequence suggested by the current values of the sequence
determining variables, i.e., 7r(i) < 7r(k) if 2:j {iij < 2:j {ikj. (Note that the last two
sequences are not necessarily the same.)
The summation inequalities II. The separation heuristic is similar to
the one described for the summation inequalities I.
The sequence inequalities II. Based on the sequence associated with the best feasible
schedule found so far, we enumerate all possible sets S
that generate a release time r that satisfies the restrictions rj < r :::; rio
The precedence inequalities I. If job i precedes job j, we try to identify a violated inequality
by taking the sum of all terms Pk ({iik +(ikj - 1) for k < i and Pk{ikj for k > i and comparing
it to tj - (ti + Pi).
The precedence inequalities II. For each of the three sequence defined above, we establish
whether it contains a pair of consecutive jobs i and j, with 7r( i) < 7r(j) and such that
ti + Pi > tj. If so, we linearize the precedence constraint tj ~ (ti + Pi){iij using one of the
inequalities in the current formulation that defines ti and that is tight with respect to the
current LP solution. Then we see whether the resulting inequality is violated.
7 The algorithm
Since even for moderately sized problem instances, the number of variables and the number
of constraints in the initial linear programming relaxation is rather large, we reduce both
by replacing all occurrences of {iji with j > i by 1 - (iij and delete all equality constraints.
This reduces the number of variables from n 2 to n( n + 1) /2 and the number of constraints
from n(n -1)/2 + n 2 to n 2 •
The algorithm uses a combimltion of cutting planes, primal heuristics and branch and
bound. In each node of the branch and bound tree the following steps are performed.
1. Solve the current linear program. If its solution is integral and satisfies the triangle
inequalities, then, if necessary, modify the best primal solution found so far, fix
variables based on their reduced costs and try to fathom nodes of the branch and
bound tree. If all nodes are fathomed, then stop else select another node and go to
step 1.
2. Calculate the active schedule associated with the sequence suggested by the cur-
76
rent values of the start time variables and the active schedule associated with the
sequence suggested by the current values of the sequence determining variables. If
necessary, modify the best primal solution found so far, fix variables based on their
reduced costs and try to fathom nodes of the branch and bound tree. If all nodes
are fathomed, then stop. If the current node was fathomed, then select another
node and go to step 1.
3. Call the separation heuristic for the triangle inequalities to check if the current
solution violates any of them. If any violated triangle inequalities are found, add
them to the current linear program and go to step 1.
4. Call the separation heuristics for the subset inequalities, the summation inequalities
(I and II), the sequence inequalities (I and II) and the precedence inequalities (only
I) to identify if the current solution violates any of them. If any violated inequalities
are found, add them to the current linear relaxation and go to step 1.
5. Branch by selecting the fractional variable Dij that is closest to one-half. On one
branch Dij = 0 and on the other Dij = 1.
An important consequence of fixing sequence determining variables when branching,
besides being able to look for violated precedence inequalities, is that we can modify a
release time. If Dij = 1, then T'j = min{Tj+I, max{Tj, 7'i + Pi). The min operation is used
to ensure that the jobs remain in order of increasing release times.
8 Computational results
The purpose of the computational study is to investigate the feasibility of using mixed-
integer programming, in particular, a formulation with sequence determining as well as
start time variables and an exponential number of constraints, to solve the single machine
scheduling problem with release times.
The algorithm is implemented using MINTO, a tool for solving mixed integer program-
ming problems. The heart of MINTO is a linear programming based branch and bound
algorithm. Although MINTO can be llsed as a general purpose mixed integer optimizer,
it also facilitates the development of a special purpose mixed integer optimizer since it
provides mechanisms to incorporate problem specific functions. For further information
on MINTO, we refer to Nemhauser, Savelsbergh, and Sigismondi [1991].
Test problems are randomly generated by a commonly used scheme. The weights and
processing times are integers uniformly distributed in [1, ... ,10] and [1, ... ,5] respectively.
77
The release times are uniformly distributed in [0, ... ,0' 2:1< ·<n Pi], where n is the number_3_
of jobs and 0' a control parameter, which is usually taken between 0.3 and 0.7. We have
used 0' = 0.5 for all our experiments.
The actual computational study consisted of two parts. First, a general evaluation of
the proposed method. Second, an evaluation of the value of the scheduling inequalities in
proving optimality.
Tables 4 and 5 present the computational results for instances with 20 and 30 jobs.
Several observations can be made regarding these results. The number of evaluated nodes
is small and does not seem to increase very much when the number of jobs increases.
The integrality gap, i.e., the difference between the value of the optimal solution and
the value of the the solution to the initial linear programming relaxation, is also small
(less than 4 percent for n = 20 and less than 3 percent for n = 30) and does not
seem to increase with the problem size. The number of linear programs solved and
the number of cuts generated is relatively large and sharply increases with problem size.
Furthermore, the linear programs become harder and harder to solve when the number of
generated inequalities increases. The first two observations are positive, whereas the last
two observations are negative.
problem ZOPT ZLP gap #nodes #LPs #triangle cuts #scheduling cuts
1 2839 2738.95 3.52 2 16 141 85
2 3915 3891.26 0.60 3 22 93 66
3 4750 4708.16 0.88 4 38 190 68
4 4428 4364.09 1.44 2 15 120 41
5 3113 3029.90 2.66 17 72 164 147
6 3437 3412.10 0.72 32 108 185 282
7 3305 3254.42 1.53 2 11 96 24
8 3287 3213.00 2.25 27 161 364 519
9 3172 3130.81 1.29 5 75 21
10 3530 3498.20 0.90 56 247 343 853
Table 4: Computationals results for n = 20.
To evaluate the value of the scheduling inequalities in proving optimality, we have
solved the instances with 20 jobs with a bare-bone version of the algorithm. This bare-
bone version of the algorithm generates only triangle inequalities. The results shown
in Table 6 clearly demonstrate the value of the scheduling inequalities. In all cases the
number of evaluated nodes, and the number of linear programs that have to be solved,
increases sharply when no scheduling inequalities are generated.
78
problem ZOPT ZLP gap #nodes #LPs #triangle cuts #scheduling cuts
1 8352 8251.86 1.19 36 155 557 382
2 6653 6567.91 1.27 18 98 694 341
3 8359 8277.05 0.98 12 66 341 572
4 7116 7071.96 0.61 3 21 268 38
5 8859 8672.28 2.10 8 63 716 172
6 8408 8264.84 1.70 19 102 987 270
7 8156 8004.00 1.86 30 180 604 693
8 7653 7583.83 0.90 11 62 324 144
9 7235 7149.73 1.17 52 286 966 1972
10 7096 7005.68 1.27 94 426 848 1424
Table 5: Computationa.ls results for n = 30.
problem ZOPT Zu' #nodes #LPs solved
1 2839 2738.95 54 130
2 3915 3891.26 15 47
3 4750 4708.16 56 126
4 4428 4364.09 15 40
5 :3113 :3029.90 26 54
6 :34:37 :3412.10 75 162
7 :3305 :3254.42 28 71
8 :3287 321:3.00 208 426
9 :3172 :31:30.81 4 10
10 :35:30 3498.20 128 274
Table 6: Computationa.ls results for the bare-bone version for n = 20.
The results show that the mixed-integer programming formulation is strong but many
cuts are needed to prove optimality. Thus the approach is successful on relatively small
problems but further work is required to make it competitive with purely combinatorial
methods or to achieve the impressive computational results that have been obtained with
cutting plane branch and bound algorithms for problems such as the traveling salesman
problem [Padberg and Rinaldi 1987, Padberg and Rinaldi 1988, Grotschel and Holland
1988].
The efficiency of our cutting plane branch and bound approach can be improved in at
least three ways.
First, we can improve the current implementation. There are several possibilities
79
here. The most promising ones relate to solving the LP relaxation more efficiently. It
currently consumes 85 percent of the computation time because it is unnecessarily large. A
better approach to solving the LP relaxation would be to fix most sequence determining
variables and ignore most constraints temporarily using the best feasible solution as a
guide. In particular, only the sequence determining variables and constraints that are
locally relevant with respect to the current best solution would be active, the remaining
sequence determining variables would be fixed at their values in the current best solution
and the remaining constraints would be ignored. The temporarily fixed variables would
be activated if a better feasible solution or if their reduced costs indicated doing so. The
ignored constraints would be judiciously checked by an implicit enumeration separation
routine.
Second, we can make use of the objective function. Again, there are various possibil-
ities. We cap. use bounds and feasible solutions that arise from combinatorial methods
[Harari and Potts 1983], or we can usc additional dominance relations [Rinaldi and Sas-
sano 1977].
Finally, we can improve the mixed-integer formulation. Two possible ways of accom-
plishing this are given in the final sectiOll.
9 Extended formulations using additional variables
Improved linearization
In our basic model, see (2.2) or (3.1), we linearized the term OikOkj by the lower bound
Oik + Okj - 1 which may be negative. A better approximation is obtained by introducing
the 0-1 variables Oikj for all i i= j i= k and replacing the terms Pk( Oik + Okj - 1) by PkOikj
where in the linear programming relaxation we add the constraints
Now observe that because of the cost structure, there is an optimal solution with Oikj =
max(O,oik + Okj - 1). Thus, the upper bound constraints are superfluous.
The idea is to use the Oikj in place of the subset inequalities. In particular, suppose
Oik +Okj -1 < 0 and there is a violated subset inequality that contains this negative term.
80
Instead of adding it, we strengthen all of the original inequalities containing tiik + 6kj - 1
by replacing 6ik +ti kj - 1 by 6ikj . The weaker versions can be removed. Now with ti ikj = 0,
the strengthened version of the inequality will be violated.
The advantage of this approach is that we can accomplish with at most O(n3 ) addi-
tional variables what could require an exponential number of subset constraints. More-
over, the additional variables can be generated as we need them. The disadvantage is
that a much more complicated implementation is required.
Block variables
We have already noted that in the absence of degeneracy, exactly one of the constraints
of the original formulation defining the start time tj of job j is satisfied with equality in
an optimal solution to (2.3), namely the one associated with the first job of the block that
contains job j. These first jobs of the blocks, called block-headers, have another, maybe
even more important, property: their start times are exactly equal to their release times.
Thus, if we could identify a block-header, we can fix its start time by providing an upper
bound as well as a lower bound. More generally, as seen in the formulation below, the
block variables can be used to get upper bound constraints on start times which may be
violated by fractional solutions in the formulation (2.3).
To identify block-headers we introduce variables Uij equal to 1 if job i is the header of
the block that contains job j and 0 otherwise and we let Tj be an upper bound on tj over
all solutions that are candidates for optimality. This leads to the following formulation
min L Wjtj
l~j~n
subject to
tj ~ riUij + L Pk( Uik + ti kj - 1) + L Pk 6kj
k<i,kij k~i,kij
t· < r' + (1 - U· ·)(T· - r)J - J JJ J J 1 ::; j ::; n;
L Uij = 1 1 ::; j ::; n;
l~i~n
Uij ::; Uii 1 ::; i,j ::; n;
1 ::; i,j ::; n;
1 ::; i =1= j ::; n;
81
1:'5 i =I j :'5 n;
Observe that if we replace Uij by Oij in the inequalities that provide lower bounds on the
start times and delete all other inequalities involving block variables, we obtain formula-
tion (2.3).
The advantage of the block oriented formulation over the original formulation is the
presence of upper bounds. However, the linear relaxation of the upper bound inequalities
for the block-headers is relatively weak. Several classes of valid inequalities have been
derived to force the block variables to their proper values. One class follows from the
observation that the constraints 2:1:$i:$n Uij = 1, Uij :'5 Uii, and U E Bn2 are precisely those
that appear in the well-known uncapacitated facility location problem (UFL), so that
polyhedral results for UFL can be immediately applied (see, e.g., Cornuejols, Nemhauser,
and Wolsey [1990]). In addition summation inequalities and sequence inequalities similar
to those derived for the o-variables also can be derived here. The efficacy of these ideas
as well as those mentioned at the end of the last section is being explored and will be
reported in a sequel to this paper.
References
[1] Balas, E. 1985. On the facial structure of scheduling polyhedra. Mathematical Pro-
gramming Study 24: 179-218.
[2] Cornuejols, G., G. 1. Nemhauser, and L. A. Wolsey. 1990. The uncapacitated facility
location problem. In Discrete Location Theory, P. B. Mirchandani and R. 1. Francis
(Eds.), Wiley, Chichester.
[3} Crowder, H., E. L. Johnson, and M. W. Padberg. 1983. Solving large-scale zero-one
linear programming problems. Opel'. Res., 31: 803-834.
[4} Dyer, M. E., and.1. A. Wolsey. 1990. Formulating the single machine sequencing
problem with release dates as a mixed integer program. Discrete Applied Mathemat-
ics, 26: 255-270.
[5} Grotschel, M., and O. Holland. 1988. Solution of Large-Scale Symmetric Travelling
Salesman Problems. Report No.73, Universitiit Augsburg.
[6} Grotschel, M., M. Junger, and G. Reinelt. 1984. A cutting plane algorithm for the
linear ordering problem. Opel'. Res., 32: 1195-1220.
82
[7] Grotschel, M., M. Junger, and G. Reinelt. 1985. Facets of the linear ordering poly-
tope. Math. Programming, 33: 43-61.
[8] Grotschel, M., 1. Lovasz, and A. Schrijver. 1981. The ellipsoid method and its
consequences in combinatorial optimization. Combinatorica, 1: 161-197.
[9] Grotschel, M., and M. W. Padberg. 1985a. Polyhedral theory. In The Traveling
Salesman Problem: A Guided Tour of Combinatorial Optimization, E. 1. Lawler,
J. K. Lenstra, A. H. G. Rinnooy Kan and D. B. Shmoys (Eds.), Wiley, Chichester.
[10] Grotschel, M., and M. W. Padberg. 1985b. Polyhedral computations. In The Travel-
ing Salesman Problem: A Guided Tour of Combinatorial Optimization, E. 1. Lawler,
J. K. Lenstra, A. H. G. Rinnooy Kan and D. B. Shmoys (Eds.), Wiley, Chichester.
[11] Harari, A. M. A., and C. N. Potts. 1983. An algorithm for single machine sequencing
with release time dates to minimize total weighted completion time. Discrete Appl.
Math.,5: 99-109.
[12] Hoffman, K., and M. Padbcrg. 1985. LP-based combinatorial problem solving. Annals
of Opel'. Res., 4: 145-194.
[13] Junger, M. 1985. Polyhcdml CombinalO1'ics and the Acyclic Subdigraph Problem,
Heldermann Verlag, Berlin.
[14] Nemhauser, G. 1., M. W. P. Savelsbcrgh, and G. C. Sigismondi. 1991. Functional
description of MINTO, a M'ixed INTegel' Optimizer.
[15] Nemhauser, G. L., and L. A. Wolsey. 1988. Integer and Combinatorial Optimization,
Wiley, Chichester.
[16] Padberg, M., and G. Rinaldi. 1987. Optimization of a 532-city symmetric traveling
salesman problem. Opel'. Res. Letters, 6: 1-7.
[17] Padberg, M., and G. Rinaldi. 1988. A Branch-and-Cut algorithm for the resolution
of Large-Scale Symmetric Traveling Salesman Problems. Research Report R.247,
IASI-CNR, Rome.
[18] Queyranne, M. 1986. Structure of a simple scheduling polyhedron. Working paper,
University of British Columbia, Vancouver.
[19] Queyranne, M., and Y. Wang. 1991. Single machine scheduling polyhedra with
precedence constraints. lHathematics of Operations Research, 16: 1-20.
83
[20] Rinaldi, G., and A. Sassano. 1977. On a job scheduling problem with different ready
times: Some properties and a new algorithm to determine the optimal solution.
Report R.77-24, Instituto di Automatica, Universita di Roma.
[21] Smith, W. E. 1965. Various optimizers for single-stage production. Naval Res. Logist.
Quart., 3: 59-66.
[22] Sousa, G., and L. A. Wolsey. 1989. Time indexed formulations of non-preemptive
single-machine scheduling problems. CORE Discussion Paper 8904, Catholic Univer-
sity of Louvain, Louvain-Ia-Neuve.
[23] Wolsey, L. A. 1989. Formulating single machine scheduling problems with precedence
constraints. CORE Discussion Paper 8924, Catholic University of Louvain, Louvain-
la-Neuve.
The Linear Assignment Problem
Mustafa Akgiil*
Abstract
We present a broad survey of recent polynomial algorithms for the linear assign-
ment problem. They all use essentially alternating trees and/or strongly feasible
trees. Most ofthem employ Dijkstra's shortest path algorithm directly or indirectly.
When properly implemented, each has the same complexity: O( n 3 ) for dense graphs
with simple data structures and O( n 2 log n +nm) for sparse graphs using Fibonacci
Heaps.
Introduction
The assignment problem is one of the most-studied, well-solved and important problems in
combinatorial optimization. It has numerous applications in various scheduling problems,
vehicle routing etc. More importantly, it emerges as a subproblem in many NP hard
problems. In particular it occurs as a relaxation of the travelling salesman problem. It has
been generalized to bottleneck, quadratic and algebraic cases, see [24, 25] for references.
Solution procedures vary from primal-dual/successive shortest paths [19,58,59,81,43,
41,26,56] (see [35] for a survey), cost parametric [76], recursive [79], relaxation [39, 52],
signature based [15, 16, 48, 9, 66, 67,68] to primal methods [17,31,6] to name just a few.
Our aim is to give a rather informal survey of recent polynomial algorithms for the
linear assignment problem. Our treatment is a bit biased toward our research in the field.
Most of these polynomial algorithms have the same time complexity: O(n 3 ) for dense
graphs using simple d~ta structures, and O(n 2 logn + nm) for sparse graphs using Fi-
bonacci heaps [44]. These are currently the best available bounds. Unless otherwise
stated explicitly, each of the algorithms discussed has the above complexity.
*Bilkent University, Department of Industrial Engineering, Ankara, Turkey
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgiil et al.
© Springer-Verlag Berlin Heidelberg 1992
86
These algorithms share the following features:
i) They solve, in various ways, an increasing sequence of problems to optimality; the last
of which being the original problem,
ii) Either they can be implemented using Dijkstra's algorithm as a subroutine after some
transformation on the graph, or their behaviour can be better understood and im-
plemented in the terminology of Dijkstra's algorithm,
iii) They work with alternating trees and/or strongly feasible trees.
In section 1, we set up the notation and terminology used in the rest of the paper. We
tried to be uniform in our terminology and notation as far as possible. For this purpose,
we first translate some of the algorithms to our terminology. Then we discuss algorithms
according to our classification. We try to group the algorithms according to motivation
and basic algorithmic primitives. Since most of the algorithms are 'near-equivalent' to
each other, our classification may seem a bit arbitrary. Starting with section 2, we discuss
the Hungarian algorithm, followed by successive shortest path, primal simplex, signature,
dual simplex, signature guided, forest and other algorithms.
1 Preliminaries
We view the assignment problem (AP) as an instance of transshipment problem over a
directed (bipartite) graph, G = (U, V, E) = (N, E), where U is the set of source (row)
nodes, V is the set of sink (column) nodes, N = U U V and E is the set of edges. The
edge e = (i, j) E E with tail t( e) = i and head h( e) = j, is directed from its tail to its
head, has weight (cost) We = Wij and flow Xe' Thus the AP can be formulated compactly
as
min {wx : Ax = b, x ~ 0 } (1)
where x EnE, bE n N with bu = -1, u E U, b1J = +1, v E V, and A is the node-edge
incidence matrix of G.
The dual of (1) is
such that
Yj - Yi :s: Ce ,Ve = (i, j) E E (2)
87
Let us set up some notation: for S, X , YeN and X n Y 0,
,(S) = {e E E : t(e), h(e) E S}
5(X, Y) = {e E E : t(e) EX, h(e) E Y}
5+(X) = 5(X,N\X), 5-(X) = 5(N\X,X)
G[S] = (S, ,(S))
) (3)
For a subgraph H of G, we will represent edge set and node set of H by E(H) and N(H).
But, very often we will simply write H for its edge set and node set. For u E U, N+( u) will
denote nodes or edges incident with u, and for X C U, we have N+(X) = UUEXN+(U).
dH ( v), for v E N is the degree of node v in the (undirected) subgraph H. Degree 1
non-root nodes are called leaf.
The reduced cost of the edge e = (i,j) with respect to y is We = we(y) = Wij - Yj + Yi.
Given a dual feasible y, let E (y) be the equality set defined as
E(y) = {e E E : We = o} (4)
and equality subgraph
G(y) = (N, E(y)). (5)
A set of edges M C E is a matching if degree of every node in G[M] = (N, M) is either
zero or one. Degree one nodes are called matched and degree zero nodes are called free.
An edge e E E \ M will also be called free. A matching M will be called perfect if every
node is matched. We often assign flow values x E RE as Xe = 1 -{==? e E M and
Xe = 0 -{==? e t/. M. For a dual feasible y, a matching M in G(y) will automatically
satisfy complementary slackness conditions
Xe > 0 ==} We = O. (6)
Such a (y, M) pair is often called compatible. The importance of the compatible pair
concept comes from the observation
Theorem 1 Let y' be y restricted to N(M). For a compatible pair (y, M) we have:
i) (y', M) is compatible,
ii) y' and edges in M solve the assignment problem and its dual defined over G[N(M)].
To store the current matching we use an array named mate. If e = (i, j) E M, we will
say mate(i) = j and mateU) = i. Node i E N will be free if and only if mate(i) = O.
88
A path in G(y) is alternating (with respect to matching M) if the edges are alternately
free and matched edges. An augmenting path between i, j E N is an alternating path
with the additional condition that i and j are free. An augmentation is just the switching
of matched and free edges in an augmenting path, which increases the size of matched
edges.
In the context of primal simplex and dual simplex; it is well-known that any basis of
(1) corresponds to a tree T of G. Given any T, it is well-known that the flow values
X e , e E T are uniquely determined for each compatible (I: bv = 0) 'supply' vector b.
Moreover, the complementary dual basic solution is also uniquely determined once one of
the y's is fixed at an arbitrary level.
For every co-tree edge e E Tl. = E - T , T U e contains a unique cycle C(T, e), called
the fundamental cycle determined by T and e. We orient C(T, e) in the direction of
e. This will give us a partition of C(T, e) as
C(T, e) = C+(T, e) U C-(T, e), e E C+(T, e) (7)
where C+(T, e) contains all edges of C(T, e) having the same orientation as e.
For f E T, T - f will have exactly 2 components, say X and Xc = N - X, with
t(J) EX. Unconventionally, we will call f the cut-edge, and X the cut-set.
The set of edges with one end in X and the other in Xc is called the fundamental
co cycle of G determined by T and f, D(T,1). This can now be partitioned into
(8)
where D+(T, 1) contains all the edges in the cocycle having the same orientation as f,
i.e. {j E E : t(j) EX, h(j) E XC} .
The dual variable change in primal simplex, dual simplex and primal-dual algorithms
will be
{ Yv
Yv = Yv + E
v E Xc
v EX (9)
for some Xc N; where E is determined so that for some edge e E 8+(X) U 8-(X) we have
We = 0 with respect to new dual variables, i.e. E is the amount of dual (in)feasibility of
the edge e; E = ±We •
Thus, dual variable (potential) change defined by (9) will cause the following changes
in reduced costs:
{ w·-t:
Wj = ~: + t:
Wj
89
j E 8-(X)
j E 8+(X)
otherwise
(10)
Cunningham [29) and Barr et al. [18), introduced the concept of strongly feasible tree.
Given a specified node, say, r as a root, let distT( x) be the distance of the node x from r in
the (undirected) tree T, i.e., the number of edges in the unique path from r to x. We say
e E T is directed toward r or a reverse edge, if distT( t( e)) = distT( h( e)) + 1 , otherwise
it is directed away from r or a forward edge. A feasible rooted tree is strongly feasible
(SFT) if Vf E T, x f = 0 implies f is a forward edge.
Let T be the tree obtained from T by changing all reverse edges to forward edges.
Vv E N, v =J r, there is a unique edge e = (u,v) E T with h(e) = v. Through such an
edge the parent of v is defined as p( v) = u. T is called a branching rooted at r. T is
represented as a data structure using parent, first (child), left (sibling) and right (sibling)
pointers. A node of degree 2 is completely characterized with parent and first pointers
in a tree. Clearly, the root has no parent. For v =J r, when (p(v), v) is deleted from T,
the component containing v is called the subtree rooted at v and denoted by T( v). This
subtree contains all nodes that can be reached from v by a directed path in T. Clearly,
v E T( v) and r E T( v) {:=:? v = r. Thus in terms of original orientation of tree edges,
an edge is forward if and only if its tail is the parent of its head.
Relevant properties of S FTs can be summarized as follows:
Lemma 1 Let T be a spanning tree for the AP rooted at a sink node, r. Then the
following are equivalent.
(i) T is a SFT,
(ii) Every reverse tree edge has flow 1, and every forward tree edge has flow 0,
(iii) d(r)=l, d(v)=2, Vv =J r, v E V where d(.) denotes the degree of the specific node. 0
Clearly (ii) implies that T is primal feasible and (iii) implies that the column signature
of T, i.e., the degree sequence of the column (sink) nodes is (2,2,2, ... ,2,1). Moreover,
if any T has column signature such as above and rooted at a node of degree 1, then such
a T is SFT.
Clearly, a dual-feasible tree which is also SFT is an optimal tree. A signature-guided
90
method changes the tree by linking and cutting edges to obtain a tree having the desired
signature, i.e., (2,2,2, ... ,2,1) .
Alternating Tree
An alternating tree T is a tree rooted at a free source node r so that for each vET,
the path from r to v in T is an alternating path. Moreover it has the following properties:
• d( v) = 2, V v E V nT,
• r is the only free node,
• all leaf nodes are source nodes,
• when matched edges in T are reversed in orientation, the new tree is a branching.
Equivalently, all matched edges are reverse and all free edges in T are forward.
• for some natural number k we have the equalities: IN(T)I = 2k+ 1, IE(T) nMI = k,
IN(T) n UI = k + 1, IN(T) n VI = k.
2 The Hungarian Algorithm
The primal-dual algorithm of Kuhn [58] and Munkres [59] starts with a compatible pair
(y, M) (M = 0, y = 0 is acceptable for w 2:: 0), and maintains such a pair throughout
the algorithm. It searches for an augmenting path by building an alternating tree rooted,
say, at a free source node r. When the alternating tree reaches a free sink node, an
augmenting path is found. Then the matching is enlarged by augmenting along this path
and the process is repeated with a new alternating tree rooted at a free source node, if
any. Let us call the work involved between two successive augmentations a stage. Thus
the primal-dual algorithm needs at most n stages. The alternating tree and the current
matching are subgraphs of the current equality sub graph G(y).
When the alternating tree rooted at a free source node r is maximal, dual variables are
changed so that at least one new edge (whose tail lies in the alternating tree and whose
head is in the outside of the tree) is added to the equality subgraph to allow a larger
alternating tree. The alternating tree is maximal means S+(T) n E(y) = 0. Letting
E = min{w e : e E S+(T)} = We, (findmin)
91
if [j+(T) =1= 0, then at least one edge e is added to the alternating tree in the new equality
sub graph. In order to achieve this, dual variables are updated according to:
Yv"'- { Yv - I: if vET
Yv otherwise (dual- update)
Note that as a result of the dual variable change yb is increased by 1:. I: > 0, simply
because T was maximal; if there were e E [j+(T), with We = 0, the algorithm should have
used it. If however, [j+(T) = 0, then there is no perfect matching saturating T by Hall's
theorem: N+(UnT) = VnT and IUnTI = IVnTI + 1 by the definition of the alternating
tree. From now on, we will assume, for ease in presentation, that the graph under study
has a perfect matching.
It is worth pointing out that a theorem of alternatives comes into the picture at this
point. The current alternating tree is maximal means the system
A'x = b, x ~ 0
has no solution, where A' is the submatrix of A whose columns are indexed by edges in
E(y). Then the alternative system
11"A':::; 0, 11"b = 1,
has the solution 11" = -XN(T), where N(T) is the node set of T and X is the characteristic
vector. Thus the dual-update can be viewed as
y ...- y + 1:11" ,
and I: found by find-min is the largest value maintaining dual feasibility of y. Hall's
theorem corresponds to case where 11" satisfies 11" A :::; 0, 11"b = 1; implying that the
system Ax = b, x ~ 0 has no solution, hence G has no perfect matching.
Thus the Hungarian algorithm is an instance of primal-dual algorithm of the general
linear programming. Actually, the latter is a generalization of the former. The general
primal-dual algorithm is finite whereas the Hungarian algorithm for the assignment prob-
lem is polynomial. The polynomiality of the Hungarian algorithm for the assignment
problem comes from the continuation of the same alternating tree until an augmentation
occurs or a proof that there is no perfect matching available. When we continue working
with the same alternating tree, the tree grows at most n times, since we add at least 2
nodes at each 'grow _tree' step. If we change the algorithm so that after each dual-update
we start afresh with a free source node as the alternating tree, the algorithm may take
92
exponential time, since we are only relying on the increase in the objective function. This
is exactly what happens in Bertsekas' algorithms [19, 21]. Moreover,if the weights are ir-
rational numbers, then the method may fail to give the optimal solution, for the sequence
of objective function values may converge to a value lower than the optimal objective
function value as shown by Araoz and Edmonds [14].
We now give the pseudo code for the tree version of the Hungarian algorithm for a
stage. Nodes in T are labeled. L contains edges of the form e = (u, v) in the equality
subgtaph G(y) with u E T which are not processed yet, and it is automatically updated
after each dual-update and grow _tree operation.
The Algorithm Al 1* stage */
Input: Compatible pair (y, M), root r E U
Initialization: T +- r, L +- N+(r), done +- false
while not done do
if L = 0 then findmin, dual-update
else select eEL, e = (u,v), L +- L \ e
if v ~ T then 1* v is unlabeled */
if mate( v) = °then Augment, done +- true
else w = mate(v), T +- T+ (u,v)+ (v,w) 1* grow_tree */
endif
end if
endwhile
Now we would like to show that dual variable updates can be postponed until an
augmentation occurs and the amount of dual variable change for each update can be
calculated efficiently. Suppose we have k dual variable updates before an augmentation
and let Ti be the tree just before i + 1 'th dual update and yi be the corresponding dual
vector for i = 0"", k - 1, and Tk and yk be the tree and the dual vector when the
augmentation detected. Furthermore, let So = TO, and Si = Ti \ T i- 1 , i :::: k; and
Si = Si, for i = 0,,", k -1, Sk = N \ Tk-l. Si, 1 :::: i :::: k is the part of the tree grown
after i'th dual-update. Let Ei be the amount of dual variable change and Ei be the set of
edges e such that w(yi-l) = Ei. Let Eo = 0, Ei = 2:j=o Ej, I'. = Ek. Clearly,
i+1 _ { y~ - Ei V E Sj, j :::: i
yv - .
y~ otherwise
and consequently, k
k 0 '"yv = yv - ~ Ej, if v E Si, i < k .
j=i+1
93
Let us define a new dual vector y by
for v EN.
Clearly, ykb = yb, and the reduced costs with respect to yk and yare the same. Then
i
yv = y~ + L tj = y~ + Ei, for v E Si .
j=o
So the main work is the calculation of E;. Instead of updating yi, we will update f/ = y.
Y will be, just before the i'th dual-update or find-min as
_ {yVYv = y~
for v E Sj, j < i
otherwise (11)
Let e E Ei , e = (u, v), u E Sf, V E Si, £ < i. We need to show that we(y) = Ei <==?
we(yi) = ti, to prove the validity of the new update. Notice that,
i-1 i-1
we(yi) = We - y~ + y~ = We - y~ + (y~ - L tj) = we(yO) - L tj .
j=£ j=£
On the other hand,
£-1 £-1
we(y) = We - Yv + Yu = We - y~ + y~ + L tj = we(yO) + L Ej .
j=O j=O
From these, it follows that
Thus one can update dual variables as in (11), and compute E's accordingly. Then
one does not need to carry the list L, but the list Ei after each findmin. But then the
computation of Ei and update of y's are essentially the same with Dijkstra's algorithm
[38].
We now give a modification of the algorithm Al which implements the above dual-
update. Instead of the list L C E(y), we carry the node list Q = V \ T. For i E Q, 7ri
stores the temporary label min {Wu,i (y) : u E Un T} and nb( i) stores the tail of the edge
defining 7ri. The routine findmin returns E, u, v, where v = argmin{7ri : i E Q} and
E = 7r v , U = nb(v).
The Algorithm A2 /* stage */
Input: Compatible pair (y, M), (free) root r E U
94
Initialization: Q (- V, 7ri (- 00, nb(i) (- 0, for i E N, 7rr (- 0, scan(r), done (- false
while not done do
findmin
if mate( v) = 0 then Augment, done (- true
else
Q (- Q \ v, w (- mate(v), Yv (- Yv + c,
T(-T+(u,v)+(w,v), scan(w)
endwhile
Yw (- Yw + c, }
Yj (- Yj + c for j E v U N \ T /* Extend..Dual */
scan(i) /*iEU*/
for (i, j) E E and j E Q do
temp = Wij - Yj + Yi
if temp < 7rj then
7rj = temp, nb(j) = i
endif
endfor
endscan
3 Successive Shortest Path Algorithms
There is a strong relationship between assignment problem (AP) and shortest path prob-
lem (SP). It is known since Ford-Fulkerson [43] that one can solve AP by solving a min-
imum cost flow problem over an extended graph. Just add a supersource s and connect
to every source node via artificial arcs (s, i), i E U with zero cost and unit capacity, and
add a supersink t and arcs (j, t), j E V with zero cost and unit capacity. Original edges
retain their costs and get capacity 1 or 00. Given any graph G and 0 - 1 flow vector
x, the residual graph RG( x) is obtained from G by reversing the direction of edges with
flow 1, and multiplying their cost by -1. General primal-dual algorithm for the min-cost
problem when specialized to assignment problem (with w ~ 0) reduces to:
Algorithm A3
0) Pass from G to extended graph G' with (super)source sand (super)sink t
G (-- G' , y=o
95
1) fori=l,···,ndo
Solve SP over G ( using reduced costs Wij = Wij - Yj + Yi (t))
Send one unit of flow from s to t in G with dual variable vector 7r.
G<-RG,
endfor
2) Edges in G, except artificial edges, in the reverse direction give the optimum match-
ing and Y is an optimal dual vector.
Since in passing to residual graph the sign of edge costs change, (without using 0)),
one can not use Dijkstra's algorithm and hence complexity of SP becomes O(n3 ) resulting
O(n4) complexity for dense AP's. Edmonds-Karp[41] and Tomizawa[81] independently
observed that one can work with reduced costs. Since edges subject to reversing are on
the shortest path tree in the current graph, their reduced costs are zero, whence remain
zero. Thus, edge costs in SP calculations remain nonnegative and hence one can use
Dijkstra's algorithm resulting O(n3 ) or O(n 2 log n + nm) algorithm depending on density
of the graph and data structures used.
The classical Kuhn's algorithm grows only one alternating tree rooted at a source
node. To realize Kuhn's algorithm by the above algorithm one does not need to add a
supersourcei but choose a free source node as the root for SP.
The Shortest Path Problem in the above algorithm is the single source problem. In
other words, one needs to reach or label every node in the original graph. Since Dijkstra's
algorithm is a special case of general primal-dual algorithm[65], one does not need to form
full shortest path tree. In other words, one can stop SP algorithm after at least one free
sink node is reached. Then one can extend dual-variable vector to unlabeled nodes by
assigning the last label to all of these nodes.
One can start with any compatible pair (y, M) instead of M = 0, Y = o. For
random problems the classical row minimum/column minimum yields an initial matching
saturating %75 of nodes on the average[35]. Nawijn and Dorhout [60] studied the size of
maximum matchings in G(y), where y is obtained by the classical row reduction followed
by column reduction and G is the complete bipartite graph. Under non-degeneracy and
uniform distribution of cost coefficients assumptions, they showed that, asymptotically,
the expected size of a maximum matching in G(y) is equal to %80 n.
There are several successive shortest path algorithms e.g., [35, 26, 42, 47, 56, 27],
differing mainly in the way they solve the shortest path problems. For some recent
96
improvement in data structures to solve the shortest path problem see [44, 4] and for
earlier related works see, e.g., [33,34,36,37,40,45,49,50,53,61,62,70,80].
There is an alternative to residual graph. Given M one can shrink (contract) the
edges in M; i.e. e = (u, v) can be replaced by a pseudo node, say, e. Let G be the graph
resulting from shrinking, and t be the corresponding shortest path tree. Replacing each
pseudo node e E t with the edge e appropriately, we obtain an alternating tree. Succes-
sive shortest path algorithms are performing this shrinking and unshrinking operations
implicitly.
Relaxation Methods
We now discuss 1969 algorithm of Dinic-Kronrod [39] and 1980 algorithm of Hung-Rom
[52]. Even though Dinic-Kronrod algorithm is published a decade ago, it did not get
the attention it deserves. We believe this is partly due to the facts that: i) the paper
does not use LP terminology, ii) it contains significant typographical errors, and iii) the
translation is not very good. However, when properly implemented it should be faster
than Hung-Rom algorithm.
Both algorithms work with semi-assignments and utilize star graphs. A semi-assignment
is a many-to-one mapping from U to V ( or from V to U). A star is a complete bipartite
graph K1,k for some k; that is a source node is connected to k sink nodes or vice versa
for Kk,l. A semi-assignment M decomposes into a matching M and a collection of stars.
In a star, there can be one matched edge. In both algorithms, the selection of match-
ing edge within a star is postponed until the star reduces to a single edge. This 'equal
employement' behaviour saves a little work.
Let us now give an equivalent description of Dinic-Kronrod algorithm in our terminol-
ogy. Let us apply the classical column minimum, find a dual-feasible y, semi-assignment
Min G(y), with matching part M C M. Let U_ CUbe set of free nodes, and U+ be set
of nodes of degree ;::: 2, and Uo = U \ (U_ U U+ ). Nodes in Uo are matched by the edges
in M. Let 1\ be the neighbour set of U+ with respect to M. A stage of Dinic-Kronrod
algorithm starts with the selection of r E U_, and obtains a new semi-assignment for
which r 1. U_. Thus the number stages is bounded by the initiallU_l. The algorithm for
a stage amounts to finding a shortest path from r to V+ on the graph G[V U Uo + r] with
edges in M reversed in orientation. Let P be such a path and t E V+ be the end of P
and u E U+ be the node assigned to t by M. Then t is removed from star of u and M is
shifted along the path P.
97
Hung-Rom algorithm starts with row minimum and obtains a dual-feasible y, a semi-
assignment M. Let us define V_, Vo, V+ as the set of free nodes, matched sink nodes, and
nodes with degree ~ 2 with respect to M. In a stage: i) they choose an r E V+ as a root, ii)
form a shortest path tree spanning N == N\ V_ with the edges in M reversed in orientation,
iii) choose atE V_, extend T and y to t by an edge e via e = argminG : j E 8(N, t)}, and
iv) change the semi-assignment along the path in T from r to t. When demand vector b
is relaxed as bv = dM( v), for v E V+, T is 8FT for the resulting transshipment problem
over N.
In Dinic-Kronrod algorithm only one star is involved; whereas in Hung-Rom algorithm
one star is chosen as a root and all others are forced to be on the shortest path tree
T. Thus in Hung-Rom algorithm the shortest path problem is solved on a larger set of
nodes. Moreover 'relaxation' in Dinic-Kronrod is combinatorial whereas in Hung-Rom it
is algebraic.
Engquist [42] presented an algorithm which is essentially the same as the Dinic-Kronrod
algorithm. It is described in LP terminology, and involves shrinking and/or reorienting
semi-matched edges. He reports that his code is about six times faster than Hung-Rom
code.
4 Primal Simplex Methods
Dantzig specialized the simplex method to networks early in 1951. The network simplex
method in general is very efficient for network flow problems. This efficiency comes mainly
from the fact that the network simplex algorithm works combinatorially over trees rather
than algebraically over the matrices.
There are several efficient primal simplex algorithms for the assignment problem, either
especially designed for the assignment problem [18] or designed for the transshipment
problem [46, 75]. Naturally, they all work reasonably well in practice, but theoretically
they are exponential algorithms.
When the network simplex method is specialized to assignment problems, degeneracy
comes into picture. For an n X n assignment problem there are n - 1 degenerate and
n non-degenerate edges in any basis. It has been observed by several researchers that
about %90 percent of pivots are degenerate in an assignment problem. Roohy-Laleh [72]
exhibits a family of problems with exponentially long non-degenerate pivot sequences.
Cunningham [29, 30] while devising a network simplex method which does not cycle
98
introduced the concept of strongly feasible tree. Barr, Glover and Klingman, indepen-
dently and simultaneously introduced the alternating basis tree. It turns out that, a
strongly feasible tree for an assignment problem is exactly an alternating basis tree. An
alternating basis tree resembles the alternating tree of the primal-dual algorithm. In fact,
an alternating tree becomes a strongly feasible tree after an augmentation if rerooted at
the free sink node causing augmentation.
In a primal simplex algorithm, if We ~ 0, VeE E then T is optimal. Otherwise an
edge e E T.L = E \ T is chosen with We < 0 as the pivot edge. Then a flow of value () is
sent through C(T, e) in the direction of e. The cut edge, f, () and the flow update can be
described as: () = XI = min{xj : j E C-(T,e)}
T=T+e-f
j E C+(T, e)
j E C-(T, e)
otherwise
(12)
y is updated so that We = o. For more information on the simplex method see, e.g.
[28, 29, 54, 22, 71, 3]. Following the convention in [6, 18], we will choose the root of
the 8FT as a source node and use S FT' to differentiate from the previous one. Then
T is SFT' if Vf E T, XI = 0 implies f is a reverse edge. We need to classify co-tree
edges as forward, reverse and cross. e E T.L is a forward edge if t( e) lies on the
unique path from r to h( e) and a reverse edge if h( e) lies on the path from r to t( e) .
Otherwise a co-tree edge is called a cross edge. For nodes u and v, the nearest common
ancestor NCA(u,v) is the last node common to paths from r to u and v respectively.
Then, e = (u, v) E E is forward if u = NCA(u, v), reverse if v = NCA(u, v), otherwise
e is a cross edge.
When rooted at a source node, a S FT' has the following properties:
Lemma 2
i) Every forward edge has flow value I, and every reverse edge has flow value O.
ii) The root has degree 1, every other source node has degree 2.
iii) If e, f satisfy
e E T.L, f E C(T,e), tee) = t(f), (13)
then the selection of f as the departing variable is valid and maintains strong feasi-
bility.
99
iv) .For e E Tl. , the pivot determined by e and (13) is nondegenerate iff e E Tl. is forward
iff f E T is forward.
v) For e, f satisfying (13) the pivot is nondegenerate iff rEX.
vi) For e, f = (u, w) satisfying (13): the pivot is degenerate iff X = T(u), and the
pivot is nondegenerate iff X = N \ T( w). 0
Cunningham and Roohy-Laleh [72] developed a genuinely polynomial primal simplex
algorithm. The algorithm needs O(n 3 ) pivots and O(n 5 ) time in the worst case. The
algorithm uses strongly feasible trees.
Hung [51] gave a polynomial primal simplex method that requires O(n 3 log~) pivots,
where ~ = ~o and ~k = wx k - wx* is the difference in the objective function value
between the current solution xk and an optimal solution x* , and X O is the initial basic
solution. Let 13k = min{ We : e E E} < 0, be the most negative reduced cost at the
iteration k (Dantzig's rule). From the equation w x = W x + w X, one obtains
Suppose the pivot with reduced cost 13k is non-degenerate, then
~k 1
~k+1 = ~k + 13k :S ~k - - = ~k(l - -)
n n
Thus after k non-degenerate pivots with Dantzig's rule, we have
~k :S ~o(1 - ~f .n
Assuming integral w, when ~k < 1 the current solution xk is optimal. Thus, the number
of non-degenerate pivots by Dantzig's rule is bounded by O(nlog~). Cunningham [30]
ea;rlier bounded the number of degenerate pivots at an extreme point by n( n - 1) by
utilizing strongly feasible trees and a certain pivot rule. Hung performs all available
degenerate pivots (O(n2)) to ensure that the first available non-degenerate pivot has the
largest reduced cost. Combining these, one obtains the given bound.
Cunningham [29], Orlin [63] and Srinivasan-Thomson [76] observed the relation be-
tween strongly feasible trees and a classical perturbation technique. For E small enough,
consider the perturbed b vector
b; = -1 + E, i E U, b~ = 1 - nE, b~ = 1, v E V, v =1= r, (14)
where r is the root of the tree. Then any basic feasible solution of Ax = b' , X ~ °is
non-degenerate, and the resulting tree is strongly feasible tree for the unperturbed system.
100
Orlin [63J using this perturbation technique reduced the bound on the number of pivots
to O(n 2 log b..). To see this, notice that for the perturbed system every pivot is non-
degenerate, ( for each tree edge e we have Xe ;::: t). Thus, for each pivot with Dantzig's
rule we have
and for t = 21n' we obtain
1 k
b.. k < b.. (1 - -)- 0 2n 2 '
which implies O(n 2 log b..) pivot bound. He later reduced the bound to O(n 2 m log n)
where m is the number of edges and n is the number of nodes in the graph by showing
that there exists an equivalent network with cost coefficients bounded by 4m (m!)2 ; and
hence proving log b.. is O(mlogn). The above algorithms, [51, 63J at least implicitly,
are influenced by the ellipsoidal algorithm. Their common feature is the reduction of
the objective function value by a fraction depending on n or m, independent of the rest
of the problem parameters. In ellipsoidal algorithm this ratio is exp( :~ ) , whereas, say,
in Orlin's algorithm exp( :i). It is worth mentioning that for totally unimodular linear
programs the ellipsoidal algorithm needs O(m 2 log (mllclillbll) iterations [5, ch 4], where
as before m is the number of variables, and Ilxll denotes the (euclidean) norm of the
vector x.
In [6J we presented a primal simplex algorithm with O(n2 ) pivot and O(n3 ) time bound.
We cast the problem as an instance of transshipment problem and work on a directed
graph. The algorithm has three features. We consider an increasing sequence of sub-
graphs, the last of which is the graph of the original problem, and each one differs from
the previous one by addition of some of the edges incident with one node. In matrix
terms, we solve the subproblems defined by principal minors of the cost matrix. The
motivation for this approach came from the author's work on the shortest path problem
[8J. Moreover, we restrict the feasible basis to strongly feasible trees. Interestingly, de-
generacy together with strongly feasible trees is very helpful, at least theoretically. The
third component of the algorithm is the use of Dantzig's rule restricted to the current
subgraph. Our algorithm is a purely primal simplex algorithm, because we carry a full
basis of the original problem all the time. We do not attempt to evaluate the change in
the objective function value. Instead, we study the structure of the set of nodes on which
we make dual variable changes during the solution of the current subproblem. We call
these sets cutsets. It turns out that: i) cutsets are disjoint, ii) edges originating from a
cutset are dual-feasible once for all for the subgraph under consideration, iii) dual infea-
sible edges have the property that their tails have no dual variable change and iv) each
101
node is subject to at most one dual variable change. Thus passing from a subproblem of
size k x k to a subproblem of size (k + 1) x (k + 1) can be done with at most k + 2 pivots.
Hence, we have the bound tn(n + 3) - 4 for the number of pivots. The total number of
non-degenerate pivots is bounded by n - 1. The total number of consecutive degenerate
pivots is bounded by t(n + 2) (n - 1). All of these bounds are sharp.
Ahuja and Orlin [2] presented a new primal simplex algorithm with O(n 2 log W) pivot
and O(nmlogW) time bound where W is an upper bound 0]1 Wij' Their pivot rule is a
variation of Dantzig's rule and employs scaling. Initially, the parameter a = W, and any
edge with We ::; -ta is a valid pivot edge. When there is no valid edge then ais replaced
with ~.
5 Signature Methods
The dual simplex algorithm for the transshipment problem starts with a dual feasible
tree. If Xj ~ 0, V f E T, then T is optimal. Otherwise the algorithm chooses an f E T
with Xj < 0, as the leaving edge (cut-edge), and chooses a co-tree edge e E Tl. as the
entering (pivot) edge to satisfy dual-feasibility via
E=we=min{wj :jED-(T,f)}. (15)
Thus the result of a pivot is the new tree T' = T + e - f. A pivot will increase flows
on the edges C+(T, e) by () = -xj, decrease flows on C-(T, e) by (), and increase the
reduced cost of the edges in D+ (T, f) by E and decrease that of the edges in D- (T, f) by
Eo Note that for Y being the component of T - f containing t(f) we have the equalities
5+(Y) = D+(T, f) and 5-(y) = D-(T, f).
Since a SFT is automatically primal feasible, a dual feasible SFT tree is optimal.
Balinski [15] starts with a specially structured dual-feasible tree and tries to obtain a
SFT. Balinski's algorithm performs essentially dual-simplex pivots, but the algorithm
never evaluates or updates flow values explicitly. Its behaviour is dictated by the degree
structure or signature of the tree. Even though Balinski starts with what is known as the
Balinski tree, one can start with any dual feasible tree [48]. Let V+ = {v E V : d( v) ~ 3},
V_ = {v E V : d(v) = I}. A SFT can be characterized by either IV+I = 0, or equivalently
by IV-I = 1.
Balinski defines the level of a tree as the cardinality of V_. By stage we mean the
total work involved in reducing level by 1. Then Balinski's algorithm for a stage can be
described as:
102
Algorithm A4 /* stage */
Input: s E V+, t E V_
reroot T at t, S +- S
while S ~ V_ do
1= (p(s),s) /* cut-edge */
e = argmin{wj : j E D-(T, 1)}
T+-T+e-I
/* link-edge */
s +- t(e)
endwhile
Let Sl, S2,···, Sk be the nodes encountered at the while loop, and iI,···, Ik, and
e1,· .. ,ek be the corresponding cut-edges and link-edges respectively. Let Xi be the
component of T i- Ii containing Si. Thus J; E 5-(Xi) and ei E 5+(Xi). Since Si+1 = t( ei) E
X i+1, and Si+1 ~ Xi, it follows that X i+1 J Xi and s;'s are distinct, and {Sl' S2,···, Si} C
Xi. Since the while loop breaks once Si E V_, the number of iterations in a stage is
bounded by IV \ V_I. SO, if the current level is k, then one can have at most n - k pivots.
Thus the total number of pivots is bounded by
n-1 n-2 ( )
~(n-k)=~j= n~l
Goldfarb [48] developed a sequential version of the signature method which starts
with 1 x 1 problem, and solves k + 1 x k + 1 problem using an optimal SFT solution
to k x k problem, for k = 1,···, n - 1. Given a SFT TI for the k x k problem of the
graph GI = (UI, VI) rooted at r E VI, with dual vector y, TI and y is extended for the
k + 1 x k + 1 problem of G = (UI + u, VI + v) as:
Yv = min{Yi + Wiv : i E UI} = Yil + Wilv
and
Yu = max{Yj - Wuj : j E Vi U V } = Yv - Wuv
and the new tree as
T +- TI + (u,v) + (u,v) .
If v = v or v = r then T is a SFT with root r or root v, and the solution of the new
subproblem is at hand. Otherwise d(v) = 3 and d(r) = d(v) = 1. Then for s = v and
t = r or t = v the previous algorithm requires at most k pivots.
103
6 Purely Dual-Simplex Algorithms
The above algorithms, strictly speaking, are not dual simplex algorithms for they may
cut an edge with zero flow (Balinski tree) or with positive flow (arbitrary dual-feasible
tree or Goldfarb's variant).
Balinski [16] later introduced a notion of Dual Strongly Feasible Tree (DSFT) for the
assignment problem with very strong properties. Paparrizos [69] extended this concept to
the transportation problem somehow, but the resulting algorithm for the transportation
problem is pseudo-polynomial.
Let T be a dual-feasible tree for AP rooted at a sink node r. Let L be the set of edges
in T attached to r and to a leaf, i.e. L = {(u,r) E T: d(u) = I}. Then T is DSFT if:
i) JET \ L, reverse ==? xi ~ 0,
ii) JET, forward ==? xi 2: 1.
Notice that for J E L we have xi = 1. Let u+ = {u E U : d(u) 2: 3} and
U_ = {u E U : d(u) = 1 }. The relevant properties of DSFT are given in [16] as:
Lemma 3 Let T be a DSFT rooted at rEV. Let u E U+ and let J = (u,p(u)). Then
i) xi ~ -I, and
ii) the selection oj J as the cut-edge oj a dual-simplex pivot maintains DSFT.
Thus DSFT is maintained as long as the cut-edge is a reverse edge J = (u, v), with
d(u) 2: 3.
Balinski's dual-simplex algorithm for a stage is:
Algorithm A5 (s) 1* stage */
Sf-S
while des) 2: 3 do
let J = (s,p(s)) and e E TJ.. via (15)
Tf-T+e-J
Sf- tee)
endwhile
Letting ei, Ji, Ti, and Y; be respectively pivot edge, cut-edge, tree and the component
of Ti - Ji containing Si = t(Ji), with Ti+1 = Ti + ei - 1;, it follows easily that Y; c Y;+1,
104
s;'s are distinct and {SI,"" s;} c Yi. Thus the number of pivots in a stage is bounded
by 1U \ U_I measured at the beginning of the stage. Hence we obtain the same bound for
the total number of pivots as before.
We should point out that the final SFT is rooted at a source node. This is so because
the algorithm works with row signatures instead of column signatures (this is our version
of Balinski's algorithm).
Akgiil's Sequential Algorithm
Akgiil [7] presented a sequential algorithm which, starting with trivial problem (of a
perturbed system) APo, solves API,' .. APn and from the last one, obtains an optimal
solution of the original problem. The final solution need not be a tree solution, but a
collection of strongly feasible trees each rooted at a source node. Let V = {VI, vz, ... ,vn }
be an arbitrary ordering of sink nodes, and let r == va be a dummy sink node. Form the new
graph G# == (U, V, E + {(u, r), u E U}), and define Gk as Gk = G#[U + {va"'" vdJ, and
let APk be the transshipment problem over G k with bu = -1, u E U, bVJ = 1, 1 s:: j s:: k,
and br = n - k. Assign W ru = K, for u E U (for the artificial edges). Clearly Yr = K,
Yu = 0 for u E U is feasible for APo, and Go is a feasible hence optimal tree for APo' Here
K is a large constant.
Let T; be an optimal D8FT for APk • Then T; - r will be disjoint union of (primal)
8FT's each rooted at a source node together with n - k isolated source nodes. Letting
v == Vk+I, Gk+l contains, in addition to Gk, the node v and the edges 8(U, v). Given T;
and v, the dual vector Y is extended to the node v and a new edge is added to T; to
obtain T, a D8FT for Gk+l via:
Yv == min {w uv + Yu : (u, v) E E } = WSV + Y.
and T = T; + (s,v). If d(s) = 2 then T is optimal. Otherwise, d(s) = 3, and all the
reverse edges from r to s have flow value -1. Even though a dual simplex algorithm can
choose anyone of them as a cut-edge, there is a unique cut-edge which maintains DSFT,
namely f = (s,p(s)). Solving APk+l starting with the above T will be referred as stage
k + 1. The sequential algorithm for solving APk+l is as follows:
Algorithm A6 (8) /* stage */
Sf--8
while d(s) = 3 do
let f = (s,p(s)) and e E T.l via (15)
Tt-T+e-J
s t- t(e)
endwhile
105
The arguments given for the Balinski's algorithm remain valid. One can easily show
that the total number of pivots is bounded by G).
The O(n2 ) bound on the number of pivots will not translate into O(n3 ) time bound for
the dense case. For this one needs to utilize the nested structure of various Xi'S or Y;'s.
One can implement these algorithms so that the total work in a stage takes O(n 2 ) for
the dense case with simple data structures and O(m + nlog n) for the sparse case using
Fibonacci heaps. For details see [16,48, 7,9].
Paparrizos [68] developed a sequential dual simplex algorithm similar to ours. He starts
with a Balinski tree and from that tree he drives the sequence of problems to be solved.
The solution of the subproblems are essentially the same with ours.
7 Signature Guided Algorithms
Paparrizos [66] introduced a non-dual signature method which solves the n by n assign-
ment problem in at most O(n 2 ) pivots and O(n4) time.
In [9], a modification of Paparrizos' algorithm, is given: it is a dual-feasible signature-
guided forest algorithm which terminates with a strongly feasible tree.
First, we will describe Paparrizos'[66] algorithm in our notation. His algorithm works
with, what we call, layers. Initial tree is dual-feasible and is rooted at a source node
and all sink nodes of degree 1 are attached to this source node, i.e., a Balinski tree. A
layer consists of two parts: decompose and link. To decompose a tree, a sink node
of degree ~ 3 which is minimal in distance to the root is identified. If there is no such
node, then T is S FT and hence it is optimal. Let v E V be such a node. Then the edge
(p( v), v) is deleted and the cutoff subtree rooted at v is identified as a 'candidate tree',
and is denoted as say, Tv. The process is continued until the tree rooted at r contains no
sink nodes of degree ~ 3. The tree rooted at r is called T+ and T_ is the collection of
candidate trees. The link part of the algorithm is as follows.
while T_ i- 0 do begin
e t- argmin{w e : e E 8(T_,T+)}
f t- We, let t(e) E Tk
Y1J f- Y1J - E Vv E Tk
T_ f- T_ \ Tk
T+ f- T+ + Tk + e
endwhile
106
The main invariant during link is that the subtree T+ is dual-feasible, i.e., edges in
,(T+) are dual-feasible. Consequently, when a layer is finished, the new tree is dual-
feasible. Since the layer algorithm is continued until T is SFT, the algorithm stops
with an optimal tree. The pivot bound is O(n 2 ) but the number of layers also has the
same bound. This results in an O(n4) algorithm. Moreover, during a layer, dual-feasibility
may be violated.
In the new algorithm, the layer concept is abandoned altogether. After linking a subtree
to T+ via sink node v, instead of linking other trees in T_ to T+, decompose is applied if
possible. So the algorithm performs a simpler form of link and decompose alternatively
(some decompose could be vacuous). The whole process is divided into stages which will
facilitate an efficient implementation of the algorithm.
We also make dual variable changes on the whole T_ rather than on a subtree of it.
Consequently, we obtain a dual feasible algorithm with the state of the art complexity.
Now, we describe the new algorithm.
For a tree (forest) T, let 0'1 = 0'1(T),0'2,0'3 be the number of sink nodes of degree 1,
degree 2 and degree at least 3 respectively. Hence, T is SFT if and only if 0'1 = 1,0'2 =
n - 1,0'3 = O. The level of a tree is O'l(T). Our algorithm works with stages through
each of which 0'1 is reduced by 1. The computational cost of a stage will be O(n2) for
the dense case and O( n log n + m) for the sparse case.
We start with the well-known 'Balinski-tree' rooted at a source node 'f'. We then apply
I
decompose. Thus, we obtain T+, and T_ = UTi and l ~ 0'3'
i=l
Our link routine (at say kth iteration) is as follows:
begin
end
e = (u,v) = argmin{w e : e E 5(T_,T+)}
let E = We and t( e) = u E Tq
Y~ f- Y z - E VZ E T_
T~ f- T+ + Tq + e
T!.. f- T_ \ Tq
107
where T = T_ UT+ is the forest at the kth iteration and T' = T~ UT~ is the forest obtained
after the kth link.
A link followed by a, possibly vacuous, decompose is called a pivot. Let d( v) be the
degree of v in T~. Depending on d(v) where v = h(e) (e is the link-edge at kth link), we
identify 3 types of pivots.
d( v) = 3: In this case, we cut the edge (p( v), v) from T~, and add the cutoff subtree
rooted at v to T~. This is called a type 1 pivot.
d( v) = 2: In this case, a stage is over. Here, we check whether the subtree of T~ rooted
at v, which is Tq + e contains any sink node(s) of degree ~ 3. If so, we apply
decompose and add the resulting subtrees to the collection T~. Otherwise, we just
continue. The former case is called type 2 pivot and the latter type 3 pivot. In
type 2 pivots, the number of subtrees in T~ may increase by more than one. In
type 1 pivots, the number of subtrees in T~ is the same as that of T_, and in type
3 pivots the number of subtrees in T~ is one less than that of T_.
The algorithm continues until T_ = 0 and terminates with a strongly feasible and
hence an optimal tree T+.
Lemma 4 The new forest T' = (T~, T~) is dual-feasible.
Proof: It suffices to show that with respect to dual variables y', forest T is dual-feasible
and the reduced cost of the link-edge e is zero. Clearly, the reduced costs of the edges in
,(T_) and ,(T+) do not change. The reduced costs of the edges in 5(T_, T+) decrease by E
and those in 5(T+, T_) increase by Eo Since E ~ 0, edges in 5(T+, T_) remain dual-feasible.
Edges in 5(T_, T+) are also dual-feasible simply because of the way link-edge e is chosen.
With respect to y', edge e has zero reduced cost. Therefore, T + e is dual feasible. Clearly,
decompose routine does not affect dual-feasibility. As a result, T' is dual-feasible. 0
Since the algorithm maintains dual-feasibility and stops with SFT, it is valid.
The total number of pivots is bounded by (n -l)(n - 2)/2.
8 Forest Algorithms
Here we present a forest version of the classical primal-dual algorithm of Kuhn imple-
mented in the spirit of successive shortest paths. Strictly speaking, we successively solve
a shortest path problem over the residual graph whose arc costs are reduced costs, until
108
optimality. We grow a forest of alternating trees each rooted at a free source node, and
allow strongly feasible trees each rooted at a sink node 'float' around. We do not neces-
sarily stop when an augmenting path is found. When an augmentation happens we do
not discard the whole alternating forest. We reroot the tree subject to augmentation on
the free sink node causing augmentation and obtain a strongly feasible tree rooted at that
sink node. When we grow an alternating tree with an edge whose head lies in a non-trivial
strongly feasible tree, if necessary we decompose that tree and append a maximal subtree
to the alternating tree. The subtree may contain more than one matched edges, which
we call as 'block pivot'.
Block Pivots
The key to our 'block pivot' is the relationship between an alternating tree and a strongly
feasible tree. Recall that if T is an alternating tree rooted at a source node rand T is
subject to augmentation with edge e = (u, v), then T' = T + (u, v) is a strongly feasible
tree when rerooted at the (previously free) sink node v. Recall also that it is very easy to
identify the matched edges in a SFT. Thus after an augmentation we reroot the current
alternating tree and retain it as a SFT.
Let T be an alternating tree rooted at r, and T be a SFT rooted at q E V, and
suppose in the primal-dual algorithm we apply grow _tree step with the edge e = (u, v),
where u E T and vET. Ordinarily, we grow the alternating tree by adding edges e and
(v, mate(v)) to T. In a block pivot, we add e and Tv the subtree of T rooted at v to T
(N(Tv) = N(Tv)). Thus, if v = q then we have T ~ T + (u, v) + T, otherwise we delete
the edge f = (p( v), v) from T obtaining, say, Tq and Tv and let T ~ T + (u, v) + Tv.
Clearly, in both cases T will be an alternating tree, and in the latter case Tq will remain
a SFT.
Like other primal-dual/successive shortest path algorithms, the new algorithm works
in stages which involve finding a set of cheapest augmentations, updating matching and
dual variables and continues until an optimal perfect matching is found.
Let us set up some notation. We use UF , VF to denote set of free source/sink nodes.
We will maintain a set of alternating trees each rooted at a free source node, possibly
a trivial tree consisting of a root. This collection will be called Planted Forest (PF).
Since each isolated node is a trivial tree, the set VF will be called, alternately, Trivial
Forest (TF). Moreover, we will have several SFT's containing equal number of source
nodes, sink nodes and matched edges. Each such tree is rooted at a sink node of degree
109
1. The collection of such trees will be called Matched Forest (MF). The union of TF
and MF will be called as Floating Forest (FF). PF, MF and TF will be maintained via
a circular list containing roots of the trees in each forest.
Q denotes set of nodes that can be appended to planted forest. It is initially identical
with node set of floating forest, but may differ slightly later. For j E Qn V, 7r(j) holds the
minimum reduced cost of the edges whose tail lies in Planted Forest and whose head is j
and tail of such an edge is stored in nb(j). In order to facilitate multiple augmentations,
we carry the field sroot (source root), which identifies for each node in the planted forest,
root of the alternating tree which contains that node. We also carry two scalars labeled
and augmented which counts number of nodes in VF which are reached and which will
be subjected to augmentation. T denotes any tree, and T; denotes the tree containing the
node i. y denotes the cumulative dual vector and 7r denotes dual vector for the shortest
path problem.
The Algorithm AT
Input: (y, M), PF, FF
global: y, 7r, E, sroot, augmented, labeled
while VF i 0 do begin /* shortest path */
InitializeJlhortest_path
repeat 1* solve shortest path */
k = argmin{7r(i) : i E Q n V} /* findmin */
E = 7r(k), u = nb(k), £ = Jsroot(u)J
If k E VF then /* k is free sink node */
Count_labeled_and_augmented
else /* k is not free */
Grow_Tree
until labeled and/or augmented is large enough
Augment
Extend_Dual
y+-y+7r
endwhile
CounLlabeled_and_augmented (k, u, £)
Q +- Q \ k, labeled +- labeled + 1
If sroot(£) = £ then 1* augmentation */
sroot(k) = £, p(k) = u
sroot(£) = -£ 1* mark £ as used */
augmented t- augmented + 1
endif
endCount_labeled_and_augmented
Grow_Tree (k, u, £)
If p(k) = 0 then delete'TJ. from MF
else
r = p(k)
delete k among children of r
endif
parent(k) = u
for j E N('TJ.) do
1r(j) = E, Q t- Q - j
if j E U then Scan(j), sroot(j) = £,
end Grow _Tree
Initialize.J!!hortest_path
labeled t- 0, augmented t- 0
Q t- N(FF)
for j E Q, 1r(j) = +00, nb(j) = 0,
for 'T E PF
for j E 'T 1r(j) = 0,
if j E U scan(u),
endfor
endlnitialize.J!!hortest_path
110
Augment
for v E VF do
if sroot( v) :I 0 then
remove v from VF
reroot Tv making v root
transfer Tv from PF to MF
endif
endfor
endAugment
Extend_Dual
begin
for j E Q
7r(j) = E
endfor
endExtend..Dual
/* the last E */
Scan(i) /* i E U */
for (i, j) E E and j E Q do
temp = Wij - Yj + Yi + 1r(i)
if temp < 1r(j) then
7r(j) = temp, nb(j) = i
endif
endfor
endS can
Augment does not actually perform augmentations, but instead reroots the alternat-
ing trees on the free sink node causing augmentation. Thus each such tree becomes a
SFT and transferred to MF. Matching is defined via parent pointers of the source nodes.
Initialize.J!!hortest_path calculates 7r(j), nb(j), for j E V n F F by scanning source
nodes in planted forest. Clearly, there is some freedom in ending a stage: from labeling a
free sink node to labeling all free sink nodes.
111
A Faster Version of Hong-Rom Algorithm
We now apply the ideas presented in the above algorithm to semi-assignment algorithms
of Dinic-Kronrod and Hung-Rom [39, 52].
Our Initialize routine is the classical row minimum routine followed by a slight varia-
tion of column minimum applied to free sink nodes. At initialization we allow formation
of stars rooted at source nodes as well as at sink nodes. Our initial forest F decomposes
into 3 parts: F_, Fo, F+ where Fo is a collection of matched edges, F_ is a collection of
stars rooted at sink nodes, and F+ is a collection of stars rooted at source nodes. Each
star in F_ has deficit of sink nodes and each star in F+ has surplus of sink nodes. Initially
the root of a star in F_ U F+ has degree::::: 2. When degree of such a root decreases to 1,
the tree rooted at that node is transferred directly into forest containing Fo. We let
(16)
When we grow forest and perform augmentations, structures of F_, F o, F+ will change
and identities in (16) will be maintained at the beginning of each stage. F_ will become
the Planted Forest (PF), a collection of trees each rooted at node in V_. When nodes in
V_ are deleted, resulting collection of trees will be alternating trees rooted at nodes in U_.
We would like to view PF as a collection of alternating trees each rooted at a node in U_
(which is true for PF\ V_). Fo wil become a collection of SFT's rooted at sink nodes. This
forest will be called Matched Forest (MF). F+ will be called Surplus Forest (SF) and
will be treated as a collection of stars each rooted at a node in U+, except some isolated
sink nodes (SF will replace TF the trivial forest). Only isolated nodes in the current forest
could be in V+ which is in SF. The union of MF and SF will be called Floating Forest
(FF).
Q, y, 7r, nb, sroot} labeled and augmented will be the same as before. Actually, the
main routine will be the same. Only routines Grow_Tree} CounUabeled_and_augmented
and Augment will change slightly to handle stars.
The main operation in a primal-dual/successive shortest path algorithm is findmin
followed by Grow_Tree or Augment. Our findmin is
e=(u,k)=argmin{wij: iEPF, jEQ}. (17)
Normally, for k as defined by (17), k E SF means an augmentation. However, this
is no longer true in our algorithm since we are allowing multiple augmentations. We
check whether root £ of the subtree containing u, with £ = Isroot(u)l, is marked for
112
augmentation. If £ is marked before, we remove k from Q and increase labeled by l.
Node k is temporarily taken from SF but kept in V+ as an isolated node for later stages.
Otherwise, we mark £ as augmented via 8root(£) = -£ and delete edges (l,8) and (k,q)
where 8 = p(£), q = p(k) from:F_ and :F+ respectively. Furthermore, we check degrees
of 8 and q. If d(q) = 1, we move the tree consisting of (q,r) to MF with root r, where r
is the only child of q after deletion of k. If the new degree of 8 is 1, we mark u' the only
neighbour of 8 in :F as augmented, but keep in PF. At the end of augmentation we move
such trees from PF to MF. We also increase labeled and augmented by 1. This is what
Count_labeled_and_augmented does.
If k 1:. SF we call Grow_Tree. If k is not root of a SFT, we delete (k, p(k)). Then we
append SFT rooted at k to PF, and for each i E N(Tk) we set 7l"(i) = 7l"(k), delete i from
Q, and perform Scan(i) if i is a source node. Details are given in [ll].
Paparrizos [67] developed a pivotal algorithm which he calls 'exterior point' algorithm.
The algorithm as presented attains primal feasibility and dual feasibly only at optimality.
The selection of pivot edge (co-tree edge) is done in the spirit of dual simplex algorithm,
and cut-edge is selected from the fundamental cycle using signature guided considerations.
It can be made dual feasible quite easily via lemma 4. More importantly, it can be realized
as a variant of the above algorithm.
Achatz, Kleinschmidt and Paparrizos [1] presented another dual simplex based forest
algorithm where pivot selection is guided by signature of a SFT. It is very similar in
principle to our algorithm A 7 and can be made dual feasible via lemma 4.
9 A Few Other Algorithms
In this section we will discuss some old and some new algorithms with different motivations
and different characteristics.
Balinski-Gomory Primal Algorithm
We start with the primal algorithm of Balinski-Gomory [17]. It maintains a matching M
and a (non-basic) dual vector y and the invariant that they are complementary throughout
the algorithm. y and M are complementary if e EM==? we(y) = O. Let us define
E+ = E+(y) = {e E E : we(y) > OJ, and similarly E_ and Eo. Clearly Eo(Y) = E(y) in
our terminology. A second invariant of the algorithm is that as y is updated to y', we have
113
E+(y) U Eo(Y) C E+(y') U Eo(Y'); in other words once an edge is dual feasible, it remains
dual feasible throughout the algorithm. If E_(y) = 0 then the pair (y, M) is optimal. A
stage chooses eo = (u, v) E E_, and ends with new y and M with eo E E+ U Eo. Since
eo E E_, eo <t M. Let (r, v) EM. The algorithm grows an alternating tree rooted at r in
the graph (N, E+ U Eo) (with edges in M reversed in orientation). As usual it performs
grow _tree and dual-update operations and changes the matching if the edge eo is on a
negative alternating cycle.
An equivalent description of the algorithm for a stage is the following. Let Eo = wuv
at the beginning of the stage. Solve the single source shortest path problem for root r
over the edges E+(y) U Eo(Y) with node v deleted and edges in M \ (r,v) reversed in
orientation. Let G' be the resulting graph, T be resulting shortest path tree, and 7r be
the dual vector for SP. To describe the new y and M we need to look at 3 cases:
i) u <t T, i.e. u is not reached, which means 8~,(T) = 0. 8+(T) n E+(y) = 0 is obvious.
8+ n Eo(Y) = 0 follows from the facts that T is alternating and every node in G'
except r is matched. Define
{
Yi + 7ri
y; = Yi + E
Yi
i E T
i <t T, ii-v (18)
l=V
where E is the last label. Clearly T C Eo(y'), and E+(y') ::) E+(y). Let E'
-wuv(y') S Eo· If E' S 0 then eo <t E_(y') and we are done. Otherwise, let
A {y; - E' i = v, i E T
Yi = y; otherwise
Notice that for e E 8-(T+v), we(f)) = we(y')+E', and in particular wuv(f)) = O. Since
8~,(T) = 0, we have the invariant E_(f)) C E_(y'). Then (f), M) is complementary
pair and eo = (u, v) ~ E_, i.e. the stage is over.
ii) u E T and 7r(u) ~ Eo. Let y' be as in (18). Then wuv(Y') = wuv(y) + 7r(u) ~ 0, and
y~ = Yr, y~ = Yv implies (r,v) E Eo(Y'). Thus (y', M) is a complementary pair and
eo = (u, v) ~ E_(y').
iii) u E T and 7r ( u) < Eo. In this case, the path P from r to u in T and the edges
(r, v), (u, v) form a negative alternating cycle C. Let M' be the matching obtained
from M by switching matching and free edges in C, i.e., M' = M E9 C. Let y' as in
(18) and define
ii-v
l = V
114
Clearly, wuv(fJ) = 0, (y, M') is compatible and the stage is over.
By choosing edge (u, v) as argmin {w;v : i E U} and (u, v) E E_ we can bound the number
of stage by n. This algorithm is generalized to non-bipartite matching by Cunningham
and Marsh [31]. Balinski-Gomory also presented a sequential version of their algorithm.
Cost Operator Algorithms
Srinivasan and Thompson [76] presented the so called 'cell cost operator' and 'area cost
operator' algorithms for the assignment problem which require O(n 2 ) pivots. We conjec-
ture that with proper design they can be implemented in O(n 3 ) time. The algorithms
maintain a (non-basic) dual feasible y and SFT which defines the matching. They came
to the notion of SFT in an attempt to handle degeneracy, and of course they do not use
the term SFT. Let us define E+(y) = {e E T : we(y) > 0, Xe > O}. Since they introduce
a perturbation so that for e E T we have Xe > 0, the latter condition is not needed in the
definition of E+(y).
The cell cost operator algorithm can be described as:
while E+(y) -=I- 0 do
eo E E+(y), eo = (p, q),
while eo E E+(y) do /* stage */
Let X be the component of T - eo containing p
e = argmin{ Wj : j E 8+(X)}, E = we(y)
if E = weJy) then
Update y via (9); break
else
Update x, y and T via (12), (9)
endwhile
endwhile
Since y is unrelated to T, the algorithm is not a pivotal algorithm in LP sense; though
each pivot looks as a combination of primal pivot and semi-dual pivot. A similar pivot
appears in Paparrizos [67]. By using the classical perturbation technique (14), it is easy
to see that each stage requires O(n) pivots, since eo E C-(T, e) for each e in a stage.
Thus the total number of pivots is O(n 2 ). Actually, one can work with the combinatorial
definition of SFT and using essentially the algorithm of [6], one can show that a stage
requires at most IXo n UI pivots, where Xo is the X set at the beginning of the stage.
115
Area cost operator algorithm inherits some of the notions of cell cost operator al-
gorithm, and introduces some more. Let y, T, E+(y) as before. Let X be the characteristic
vector of E+, and 7r be a basic solution of
7rj-7ri=Xij, (i,j)ET.
Let E = {(i,j) E E : Xij < 7rj - 7ri} and define
Jl = min { ~e(Y) : e E E}Xe(7r)
(19)
(20)
Let e = e(Jl) be the edge defining Jl, if any. Letting tr r = 0 and T be a SFT rooted at a
source node, 7ri is equal to the algebraic sum of the edges of E+(y) on the path P from r
to i: each non-degenerate edge in E+(y) n P contributes 1, and each degenerate edge in
E+(y) n P contributes -1. Since for e E T, Xe(7r) = 0, and Xe = 0, for e E Tl.; E can be
described alternately as E = {(i,j) E Tl. : 7ri < 7rj}.
The algorithm can be described as:
while E+ (y) i:- 0 do
Compute Jl and e(Jl) via (20)
Jl' = min{wj : j E E+(y)}
( = min{Jl, Jl'}
y <- y + (7r
if e(Jl) defined Pivot via (12) using e(Jl)
endwhile
Letting y' = y + (7r, we have for e E E+(y), we(y') = we(y) - (. Thus, when Jl = 00,
we have (= Jl', and the edge defining Jl' will leave E+(y). By working with the classical
perturbation (14), each pivot with ( = Jl' will decrease the sum I:{Xj : j E E+(y)} by
() ~ 2~. Thus the total number of pivots can be bounded by O(n 2 ).
Again, y is non-basic and we do not see any stage concept. For details see [76] and the
references given there.
A Criss-Cross Algorithm
Criss-Cross method of Terlaky[78] performs 'primal' and 'dual' pivots with a well-defined
pivot selection rule, but does not maintain primal or dual-feasibility. The Criss-Cross
algorithm of [78] stops with primal-infeasibility, dual-infeasibility or finite-optimum for
the given linear program. A pivot in a network-simplex method performs 'link' and 'cut'
operations, which changes a tree to a new one.
116
Akgiil-Ekin [12] present a new algorithm for AP, termed Criss-Cross, which performs
essentially primal pivots and dual pivots. Like many algorithms, it works in stages, at
the end of which a measure towards optimality is improved. In the basic model, it start
with a specially constructed strongly feasible tree. At the end of stage i, primal strong
feasibility is regained and dual-feasibility of the set of edges incident with first i source
nodes is obtained. In a non-trivial stage, the algorithm first destroys the tree by cutting
an edge and linking another one and gain dual-feasibility for the desired set of edges.
This part of the algorithm is termed as the dual-phase. Then a strongly feasible tree
is obtained by performing essentially dual-simplex pivots. This part of the algorithm is
called a primal-phase. Later, a modified algorithm is given which is faster, and can
start with any strongly feasible tree.
The algorithm has the pivot bound of O(n 2 ), and time bound of O(n 3 ) for dense graphs
and O(n2 logn+nm) for sparse graphs using Fibonacci heaps of Fredman and Tarjan[44].
In [79], Thompson presented a Recursive algorithm which may look similar to criss-
cross [12] at the first sight. His subproblems are similar to that of the basic model in [12].
However, inner working of the algorithm is quite different: in [79], first an optimal primal
solution is obtained, then strong feasibility of the basis tree is restored with degenerate
pivots. The complexity of both algorithms is the same.
10 Implementation
We now discuss briefly some implementation details. To maintain a tree we use parent
(p(.)), first (child), left (sibling) and right (sibling) pointers for each node. Children of a
node are maintained as a doubly linked circular list using left and right sibling pointers.
Since a doubly linked circular list allows addition and deletion of an element in 0(1) time,
each cut, link can be performed in constant time. Thus all tree operations in a stage takes
O(n) time. All dual updates in a stage also takes O(n) time. We maintain roots of trees
in Planted Forest, MF, TF and SF in a doubly linked circular list. Q is maintained in two
ways simultaneously: once as a doubly linked circular list and once as a 0-1 array. For
dense graphs we maintain 7r in an array and for sparse graphs in a fibonacci heap. The
array mate is not needed since that information is available in parent and first fields. For
more information about data structures see, e.g. [77,44,3,4, 13,35].
117
References
[1] H. Achatz, P. Kleinschmidt, and K. Paparrizos, A dual forest algorithm for the as-
signment problem, Tech. Rep., Universitiit Passau, Germany, 1989.
[2] R.K. Ahuja and J.B. Orlin, Improved primal simplex algorithms for shortest path,
assignment and minimum cost flow problems, Working Paper OR 189-88, M. 1. T.
1988
[3] R.K. Ahuja, T.L. Magnanti, and J.B. Orlin, Network Flows, in, Optimiza-
tion, Handbooks in Operations Research and Management Science Vol. 1, Eds.,
G.L. Nemhauser, A.H.G. Rinnoy Kan, and M.J. Todd, (Nort-Holland, New York,
1989).
[4] R.K. Ahuja, K. Mehlhorn, J.B. Orlin, and R.E. Tarjan, Faster Algorithms for the
Shortest Path Problem, Journal of ACM 37 (1990) 213-223.
[5] M. Akgiil, Topics in Relaxation and Ellipsoidal Methods, (Pitman, London, 1984.
Research Notes in Mathematics Vol. 97.)
[6] M. Akgiil, A genuinely polynomial primal simplex algorithm for the assignment prob-
lem, Technical Report, North Carolina State University, 1985, and Working Paper
IEOR 87-07, Bilkent University, 1987. (To appear in Discrete Applied Matbematics)
[7] M. Akgiil, A sequential dual simplex algorithm for the linear assignment problem,
Operations Research Letters 7 (1988) 155-158.
[8] M. Akgiil, Shortest paths and the simplex method, Working Paper IEOR-8804, Bilkent
University, 1988.
[9] M. Akgiil and O. Ekin, A dual feasible forest algorithm for the assignment problem,
Working Paper IEOR 90-11, Bilkent University, 1990. (to appear in RAIRO)
[10] M. Akgiil , A forest primal-dual algorithm for the assignment problem, Working
Paper lEOR 90-14, Bilkent University, 1990.
[ll] M. Akgiil , A faster version of Hung-Rom algorithm for the assignment problem,
Working Paper lEOR 90-16, Bilkent University, 1990.
[12] M. Akgiil and O. Ekin, A criss-cross algorithm for the assignment problem, Working
Paper lEOR 90-22, Bilkent University, 1990.
118
[13] A.I. Ali, R.V. Helgason, J.L. Kennington, and H.S. Lall, Primal simplex network
codes: state-of-the-art implementation technology, Networks 8 (1978) 315-339.
[14] J. Araoz and J. Edmonds, A case of non-convergent dual changes in assignment
problems, Discrete Applied Mathematics 11 (1985) 95-102
[15] M.L. Balinski, Signature method for the assignment problem, Operations Research
33 (1985) 527-536. Presented at Mathematical Programming Symposium, Bonn
1982.
[16] M.L. Balinski, A competitive (dual) simplex method for the assignment problem,
Mathematical Programming 34 (1986) 125-141.
[17] M.L. Balinski and RE. Gomory, A primal method for the assignment and transporta-
tion problems, Management Science 10 (1964) 578-598.
[18] RS. Barr, F. Glover, and D. Klingman, The alternating basis algorithm for assign-
ment problems, Mathematical Programming 13 (1977) 1-13.
[19] D. Bertsekas, A new algorithm for the assignment problem, Mathematical Program-
ming 21 (1981) 152-157.
[20] D. Bertsekas, P.A. Hosein, and P. Tseng, Relaxation methods for network flow prob-
lems, SIAM J. Control & Optimization 25 (1987) 1219-1243.
[21] D. Bertsekas, The auction algorithm: a distributed relaxation method for the assign-
ment problem, Annals of Operations Research 14 (1988) 105-123.
[22] R.B. Bixby, Matroids and operations research, in Advanced Techniques in the Prac-
tice of Operations Research, H.J. Greenberg et. al., ed., (North-Holland, 1982) 333-
459.
[23] G.H. Bradley G.G. Brown G.W. Graves, Design and implementation of large scale
primal tra~sshipment algorithms, Management Science 24 (1977) 1-34.
[24] R.E. Burkard, Travelling salesman and assignment problems: a survey, Annals of
Discrete Mathematics 4 (1979) 193-215.
[25] R.E. Burkard, W. Hahn, and W. Zimmermann, An algebraic approach to assignment
problems, Mathematical Programming 12 (1977) 318-327.
[26] G. Carpaneto and P. Toth, Primal-dual algorithms for the assignment problem, Dis-
crete Applied Mathematics 18 (1987) 137-153.
119
[27] P. Carraresi and C. Sodini, An efficient algorithm for the bipartite matching problem,
EJOR 23 (1986) 86-93.
[28] V. Chvatal, Linear Programming, (Freeman, San Francisco, 1983.)
[29] W.H. Cunningham, A network simplex method, Mathematical Programming 11
(1976) 105-116.
[30] W.H. Cunningham, Theoretical properties of the network simplex method, Mathe-
matics of Operations Research 4 (1979) 196-208.
[31] W.H. Cunningham and A.B. Marsh, A primal algorithm for optimum matching,
Mathematical Programming Study 8 (1978) 50-72.
[32] G.B. Dantzig, Linear Programming and Extensions, (Princeton University Press,
Princeton, 1963.)
[33] E. Denardo and B. Fox, Shortest-route methods: 1. reaching, pruning, buckets, Op-
erations Research 27 (1979) 161-186.
[34] N. Deo and C. Pang, Shortest path algorithms: taxonomy and annotation, Networks
14 (1984) 275-323.
[35] U. Derigs, The shortest augmenting path method for solving assignment problems-
motivation and computational experience, Annals of Operations Research 4
(1985/6) 57-102.
[36] R. Dial, Algorithm 360: shortest path forest with topological ordering, Communica-
tions of ACM 12 (1965) 632-633.
[3}] R. Dial, F. Glover, D. Karney, and D. Klingman, A computational analysis of alter-
native algorithms and labeling techniques for finding shortest path trees, Networks
9 (1979) 215-248.
[38] E.W. Dijkstra, A note on two problems in connexion with graphs, Numerische Math-
ematik 1 (1959) 269-271.
[39] E.A. Dinic and M.A. Kronrod, An algorithm for the solution of the assignment prob-
lem, Soviet Mathematics Doklady 10 (1969) 1324-1326.
[40] S. Dreyfus, An appraisal of some shortest path algorithms, Operations Research 17
(1969) 396-412.
120
[41] J. Edmonds and R.M. Karp, Theoretical improvements in algorithmic efficiency for
network flow problems, Journal of ACM 19 (1972) 248-264.
[42] M. Engquist, A successive shortest path algorithm for the assignment problem, Infor
20 (1982) 370-384.
[43] L.R. Ford and D.R. Fulkerson, Flows m Networks, (Princeton University Press,
Princeton, 1962.)
[44] M.L. Fredman and R.E. Tarjan, Fibonacci heaps and their uses in improved network
optimization algorithms, Journal of ACM 34 (1987) 596-615 Also in Proc. 25'th
FOCS (1984) 338-346.
[45] J. Gilsinn and C. Witzgall, A performance comparison of labeling algorithms for
calculating shortest path trees, Tech. Rep. 772, Washington, D.C., 1973.
[46] F. Glover and D. Karney, Implementation and computational comparisons of primal
, dual, primal-dual codes for minimum cost network flow problems, Networks 4
(1977) 191-212.
[47] F. Glover, R. Glover, and D. Klingman, Threshold assignment algorithm, Mathemat-
ical Programming Study 25 (1986) 12-37.
[48] D. Goldfarb, Efficient dual simplex algorithms for the assignment problem, Mathe-
matical Programming 37 (1985) 187-203.
[49] D. Goldfarb, J. Hao, and S. Kai, Efficient Shortest Path Simplex Algorithms, Oper-
ations Research 38 (1990) 624-628.
[50] M. Gondran and M. Minoux, Graphs and Algorithms, (Wiley, New York, 1984.)
[51] M.S. Hung, A polynomial simplex method for the assignment problem, Operations
Research 31 (1983) 595-600.
[52] M.S. Hung and W.O. Rom, Solving the assignment problem by relaxation, Operations
Research 27 (1980) 969-982.
[53] E. Johnson, On shortest paths and sorting, in Proc. 25'th conference of ACM, Boston,
1972, 529-539.
[54] E.L. Johnson, Flows in networks, in Handbook of Operations Research, J.J. Moder
& S.E. Elmaghraby, ed., Von Nostrand, 1978.
121
[55] R. Jonker and A. Volgenant, Improving the Hungarian assignment algorithm, Oper-
ations Research Letters 58 (1986) 171-176.
[56] R. Jonker and A. Volgenant, A shortest path algorithm for dense and sparse linear
assignment problems, Computing 38 (1987) 325-340.
[57] P. Kleinschmidt, C. Lee, and H. Schannath, Transportation problems which can be
solved by the use hirsch-paths for the dual problems, Mathematical Programming
37 (1987), 153-168.
[58] H.W. Kuhn, The hungarian method for the assignment problem, Naval Research
Logistics Quarterly 2 (1955) 83-97.
[59] J. Munkres, Algorithms for the assignment and transportation problems, SIAM Jour-
nal 5 (1957) 32-38.
[60] W.M. Nawijn and B. Dorhout, On the expected number of assignments in reduced
matrices for the linear assignment problem, Operations research Letters 8 (1989)
329-336.
[61] E. Lawler, Combinatorial Optimization: Networks and matroids, (Holt, Rinehart and
Winston, New York, 1976).
[62] E. Lawler, Shortest path and network algorithms, Annals of Discrete Mathematics
4 (1979) 251-265.
[63] J.B. Orlin, On the simplex algorithm for networks and generalized networks, Mathe-
matical Programming Study 24 (1985) 166-178.
[64] J.B. Orlin and R.K. Ahuja, New scaling algorithms for the assignment and minimum
cycle mean problems, Working Paper OR 178-88, M. 1. T. 1988.
[65] C. Papadimitriou and K. Steiglitz, Combinatorial Optimization: Algorithms & Com-
plexity, (Prentice-Hall, Englewood Cliffs, 1982.)
[66] K. Paparrizos, A non-dual signature method for the assignment problem and a gen-
eralization of the dual simplex method for the transportation problem, RAIRO Op-
erations Research 22 (1988) 269-289.
[67] K. Paparrizos, An infeasible (exterior point) simplex algorithm for assignment prob-
lems, Mathematical Programming (to appear)
122
[68] K. Paparrizos, A relaxation Column signature method for assignment problems,
EJOR 50 (1991) 211-219.
[69] K. Paparrizos, Generalization of a signature method to transportation problems, Tech-
nical Report, Democritus University of Thrace, 1990.
[70] A. Pierce, Bibliography on algorithms for shortest path, shortest spanning tree and
related circuit routing problems, Networks 5 (1975) 129-143.
[71] R.T. Rockafellar, Network Flows and Monotropic Optimization, (Wiley, New York,
1984.)
[72] Roohy-Laleh, Improvements to the theoretical efficiency of the simplex method, PhD
thesis, University of Charleton, Ottawa, 1981. Dissertation Abstract International
vo1.43, August 1982 p.448B.
[73] D.D. Sleator and R.E. Tarjan, A data structure for dynamic trees, Journal of Com-
puter and System Sciences 26 (1983) 362-391.
[74] V. Srinivasan and G.L. Thompson, Accelerated algorithms for labeling and relabeling
of trees, with applications to distribution problems, Journal of ACM 19 (1972)
712-726.
[75] V. Srinivasan and G.L. Thompson, Benefit-cost analysis for coding techniques for the
primal transportation problem, Journal of ACM 20 (1973) 184-213.
[76] V. Srinivasan and G.L. Thompson, Cost operator algorithms for the transportation
problem, Mathematical Programming 12 (1977) 372-391.
[77] R.E. Tarjan, Data Structures and Network Algorithms, (SIAM, Philadelphia, 1983.)
[78] T. Terlaky, A convergent criss-cross method, Mathematische Operationsfurchung und
Statistics ser. Optimization 16 (1985), 683-690.
[79] G.L. Thompson, A recursive method for the assignment problems, Annals of Discrete
Mathematics 11 (1981) 319-343.
[80] C. Witzgall, On labeling algorithms for determining shortest paths in networks, Tech.
Rep. 9840, Washington, D.C., 1968.
[81] N. Tomizawa, On some techniques useful for solution of transportation network prob-
lems, Networks 1 (1972) 173-194.
Cost Allocation In The Oil Industry: An Example
Kurt O. J !'1fllsten *
Abstract
We consider the problem of allocating the costs among the potential users of a
gas processing system. The cost allocation problem is solved using cooperative game
theory. We show how the cost allocation problem can be solved when the charac-
teristic function is given implicitely as the solution of a mathematical programming
problem.
1 Introduction
We consider an investment problem in which a number of producers must build a pro-
duction facility either on their own or as a joint venture together with other producers
in the same region. The investments to be made in the processing system are substantial
and there is a considerable return to scale for investments. This fact means that there
exists a potential for the producers to get together in order to build a common processing
system. The situation described above is a classical situation in which some method for
cost allocation can be used in order to divide the total investment costs among the par-
ticipants in the cooperative project. In cooperative game theory many different concepts
for fair cost allocation have been suggested such as the Shapley value and the nucleolus,
just to mention two. Most of these solution concepts suffer from the fact that the amount
of data required to compute them is enormous. Vve present how we can calculate a fair
cost allocation in cases when the characteristic function is given implicitly as a solution to
a mixed integer programming problem. The application we have in mind is the problem
of allocating costs among different offshore gas fields which have the potential of using a
common gas processing system.
·The Norwegian School of Economics and Business Administration, Institute of Finance and Manage-
ment Science, Helleveien 30 N-5035 Bergen-Sandviken, NORWAY
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. AkgUl et al.
© Springer-Verlag Berlin Heidelberg 1992
124
2 Background
Once geological surveys have been completed and estimates for the quantity of oil and
natural gas are obtained, the natural question is how to develop these fields in order
to derive the most economic benefit. Physical constraints limit the total oil and gas
production due to limited capacity of transportation systems and regional processing
facilities. In some cases, smaller fields are developed only after a larger field has been
developed, thus introducing more restrictions on the problem. Here we use a sequencing
model for the development of the gas fields in a region. For each field we assume that a
set of possible development scenarios have been determined and that the main problem
consists of determining if the fields should have their own processing equipment or if they
should use the processing equipment of any other field.
The optimal solution for this problem depends on many parameters such as the future
prices for petroleum products, the actual quantity of oil and gas available in the reservoirs,
the optimal transportation network etc. We assume that the development sequencing
problem has been solved and an optimal sequence of the different fields and process
equipment investments has been found. This global problem can for instance be analysed
with the help of a large scale mixed integer programming model as described in Aboudi
et. al. (1989). Here we will concentrate on the investments made in the partly common
processing system. Since the sequencing of the petroleum fields have been determined
based on for instance the maximization of the net present value, we have a solution which
is optimal for the region as a whole. We assume that the exact sequencing of the fields
are set by the governmental agencies and that it can not be changed. The corresponding
necessary investments in the processing and transportation facilities is the most profitable
one as seen from the society. However for a certain oil field there might exist a better
alternative. As the investments in the processing system is a joint venture involving all
future users of the system there is a need to find good rules for the allocation of the
investment costs or profits from the cooperation among the participating fields.
3 An Illustrative Example
Assume that there are five hypothetical fields I, II, III, IV, and V located as illustrated
in Fig 1.
Field I is a large gas field and have to have its own processing system on the platform.
The processing system handles the well stream and produces dry gas for further trans-
portation to the market. The other fields II-V are relatively small fields and it might be
125
economically beneficial to send the well stream directly to field I for processing instead
of installing process equipment on the platform. However all fields are profitable even if
V
I II
IV III
Figure 1: Map of gas fields in an off-shore region.
they have to invest in their own processing system. The capacity of the processing system
at field I has to be dimensioned in order to handle the largest capacity needed over the
life time of the fields. Thus the total production profiles of the gas fields will determine
the demand for capacity of the processing equipment and also the best development plan
for the region as a whole. Included in this is the optimal sequencing of the fields, i.e. the
time when they should be put in production. The data for the example is presented in
the tables below.
Field Production profile mill.Sm 3 /year Cost of processing
field on its own
1 2 3 4 5 6 7 8
I 35 35 10 10 5 5 5 5
II 12 12 7 7 7 7 7 2.5 mrd. NOK
III 7 5 5 5 5 5 1.2 mrd. NOK
IV 7 4 4 4 4 4 1.4 mrd. NOK
V 5 4 4 4 1.0 mrd. NOK
Here time periods numbered 1-8 denotes a five year time period, thus time period 1 is
the production over the years 1-5. In order to simplify the example we assume that the
production from the fields is the same over each 5 year period. The earliest start up time
for production from the region is 1993 and is the same for all the fields. We also assume
that there are no market restrictions or other restriction that permits that the fields are
sequenced to start up production in 1993. The only possible bottleneck is the use of a
common processing system with a limited processing capacity. In the example we use a
discount rate of 8%, making the cost of the processing system time dependent. The cost
for a processing system on the platform of field I is dependent on the maximum processing
capacity installed. The different options and their associated costs is given below:
126
Capacity Cost
mill. Sm 3 /year mrd. NOK
40 8.0
45 8.5
50 9.0
55 12.5
60 13.0
65 13.5
70 14.0
The discontinuity at the capacity of 50 mill. Sm 3 can for example depend on the need
to use another platform concept at and above that level of capacity. There is an option
to postpone the production start for one or more of the fields in the case that this leads
to a more efficient use of the joint processing capacity. This has to be weighted against
the loss in net present value for this delay. The profitability for the 5 fields given that
they start producing at the earliest possible starting time is as follows:
Field Expected NVP without incorporation
of costs for processing equipment
I 11.0 mrd. NOK
II 4.0 mrd. NOK
III 3.0 mrd. NOK
IV 2.0 mrd. NOK
V 3.0 mrd. NOK
The problem consists of determining when the fields should be sequenced to start
production and where the processing of gas should take place. This is the first stage of
the solution process. This problem can be formulated as a discrete optimization problem.
The solution of this problem is given below and indicates that a partially cooperative
solution is the most economically beneficial alternative for the region. Thus given that we
have used an investment sequencing model to determine the best development, this is the
regional developement scheme that has been suggested. Given this, the second problem
that we phase is how to allocate the profit we achieve from the cooperation among the
parties in the region. We assume here that the authorities or the parties involved have
decided that the development plan determined by the use of the sequencing model should
be used. If we use cooperative game theory to calculate a reasonable cost allocation there
are many different solution concepts to choose from. Here we will use the Shapely value.
However for larger problems involving more participants this might be too expensive to
127
use computationally. In such cases with many participants the nucleolus is preferable to
use as solution concept from a computational point of view. The reason for this is that
the nucleolus apart from being a reasonable solution concept has favorable characteristics
when the characteristic function is given implicitly as the solution to an optimization
problem. The reason for this computational superiority is that in order to calculate the
nucleolus we don't necessarily need the value of the characteristic function for all possible
coalitions. In practical applications when the number of cooperating fields are large i.e.
10-20 this might be a significant issue.
Before we present the mathematical programming model mentioned above we give the
solution for the illustrative example.
The optimal solution for the global optimization problem is:
Field I starts producing in 1993 with a processing system with capacity 45 mill. Sm 3 •
Field II starts producing in 2003 and makes use of the processing system at field I.
Field III starts producing in 1993 and uses the equipment at field I.
Field IV starts producing in 2003 and uses the equipment at field I.
Field V starts producing in 1993 and invest in its own processing system.
The net present value for this solution is 10.28 mrd. NOK whereas the solution in
which each field invest in their own processing system yields a net present value of 8.9
mrd. NOK. The difference 1.38 mrd. NOK is the profit achieved from cooperation. Using
the Shapley value as a solution concept yields the following
Field Profit from Total profit Pays to field I
Cooperation
I 0.S8 3.88
II 0.30 1.80 0.05
III 0.02 1.82 1.18
IV 0.09 0.69 0.24
V 0.09 2.09 -0.09
Sum 1.38 10.28 1.38
Some comments on the profit allocation:
Field I need only to invest in a system with capacity 40 in order to care for its own
production. The investment in a system with capacity 45 requires an extra amount of 0.5
mrd. NOK. Since the field on its own can achieve a profit of 3 mrd. NOK they have a
minimum requirement of 3.5 mrd. NOK in order to take part in the cooperative solution.
However as the field is the main contributor to the cost reduction that is achieved from
cooperation it is reasonable to assume that they will require more than the 3.5 mrd. NOK
in order to install the extra capacity required in the regional solution.
128
Field II saves 2.5 mrd. NOK by not having to invest in their own processing system.
The profit from a solution on their own is 1.5 mrd. NOK. However in the cooperate
regional solution the fields production has been delayed by 10 years. Field II has to be
compensated for this loss in net present value in order to accept the cooperate solution.
Field III has the same situation as field II but there is no need for compensation due to
the postponement of the production start in this case.
Field IV has the same situation as field II.
Field V should be developed with there own processing system. It can be tempting to say
that this field should not have any part in the profit from cooperation. The field has a
profit of 2 mrd. NOK as is and this is in that case the only profit that should be allocated
to field V. However this can be viewed as unfair since the field by the decision not to
use the processing equipment at field I has freed capacity in the processing system that
can be used by the other fields. This should be beneficial for field V. As is seen in the
Shapley-value cost allocation solution this benefit is of size 0.09 mrd. NOK.
4 An Integer Programming Model
In what follows we will present an integer prgramming model for The calculation of
optimal sequencing and investment in process equipment for a set of offshore gasfields.
We will give a description of the investment and sequencing model used and how it is
extended when used to calculate the profit allocation.
For the gas production model, the following data are required:
• For each gas field, the production of gas in time period 1, ... , T.
• All possible capacities of the processing equipment, and time limits if any for these
capacity expansions.
• Cost data for the investment activities.
• Product prices for each time period.
The decision variables in the model are:
xt = 1 if field j starts producing in time period k and process alternative i is used, 0
otherwise.
In the illustrative example we have for field I, 7 possible process equipment alternatives
corresponding to the different capacity levels for the process equipment. For the other
129
fields the alternatives are only two, a process unit on the platform of that field or the use
of the process equipment on field 1.
Let t = 1 correspond to the sta.rting year of the planning horizon.
vt = expected net present value for field j, given that the field starts producing in time
period k and processs alterna.tive i is being used.
bjt = the demand for process capacity for field j in time period k.
Ui = the capacity level for process alternative i on the mother field, the field which can
be used as place for processing for other fields. (i = 1 corresponds to the minimum
capacity alternative).
The basic multiperiod sequencing model for the example can be formulated as:
Max
s.t.
j = 2, ... ,5
t = 1, ... ,6
V i,j,k
(1)
(2)
(3)
Inequality (1) means that field 1 should be sequenced to start only once. Inequality
(2) means that fields 2 - 5 should be sequenced to start only once and either use their
own processing time or use the unit at field 1. Inequality (3) means that the processing
at field 1 cannot be larger than the installed capacity.
This model must be made more complex in a real life application where more than one
field can be used as common processing unit, budget constraint are present etc. However
this small example shows the main structure of the integer programming model used in
larger applications. In order to get the values for the characteristic function for a certain
coalition the model has to be solved, including only the fields included in the coalition.
5 Some Basic Definitions And Notation
In order to be able to describe how the calculations of the cooperative game solutions are
performed we need to give some definitions and notation.
Let N be the set of players in the game, and let INI = n. Let v be the characteristic
function of the cooperative game (n; v). The set 5 is any subset of N, referred to as a
coalition.
130
The set of imputations is defined as Y = {y E Rn : L,jEN Yj = v(N)}.
The core of the game is defined as the set C = {y E Rn : L,jEN Yj = v(N), L,jES Yi ;:::
v(S)}.
Yi can be interpreted as the part of the total profit allocated to player i.
The nucleolus is, intuitively, the center of the core, if the core is nonempty. In case the
core is empty it is the point closest to the core. Generally the nucleolus is the imputation
vector having the lexicographically smallest associated excess, where the excess is given
as e(S,y) = L,jESYi - v(S).
The problem of calculating the nucleolus of a profit allocation game can be formulated
as a lexicographic linear programming problem. The first linear program to be solved has
the following form.
.Af a.T w
s.t. w::; L,jES Yi - v(S) 'tiS S;;; N
L.,jEN Yj = v(N)
Since this is a linear programming problem with n + 1 variables and 2n constraints we
know from linear programming theory that it is not necessary to know the characteristic
function value for all coalitions S in order to find the nucleolus. This fact can be taken
advantage of in an iterative procedure for calculating the nucleolus for a game in which
the characteristic function is given implicitly as the solution to an optimization problem.
The Shapely value of the game (n, v) is defined as:
Yi = L (s -l)!\n - s)![v(S) - v(S - {i})].
sES n.
Here S denotes the cardinality of the set S. As we see from the formula we need to know
the characteristic function value for all coalitions S in order to calculate the Shapley value.
However there exists games in which this is not needed and in which the Shapley value is
relatively easy to compute.
6 Using The Sequencing Model To Find The Char-
acteristic Function Values In The Cooperative Gam
Let us introduce the zero-one variable vector s = (Sj) describing a coalition.
Sj = 1 if player j is a member of the coaJition, 0 otherwise.
131
The characteristic function v(S), of the process equipment investment design game,
for coalition S can then be calculated by solving a revised integer programming problem
in which the constraints are rewritten as:
;=1 k=1
2 0
LLkx 7j
;=1 k=1
< S'- J
The difference lies in the introduction of the s variables in the sequencing inequalities.
If we were to calculate the characteristic function values for all coalitions this would not
be of any use. However as discussed above if we use the nucleolus as solution concept
this is not necessarily the case. In an iterative constraint generation approach we only
need to be able to calculate a dissatisfied coalition and its corresponding characteristic
function value in order to generate a constraint that violates the current suggestion for
profit allocation. This can be achieved by solving the revised integer programming model
with an objective function that also includes a term of the form L-jES yj Sj, where yj
denotes the current profit allocation suggestion. For a more detailed description of the
constraint generation approach see lIallefjorcl et al (1988).
In the small example presented above we assumed that we calculated the investment
costs for all alternative forms of cooperation. Given this information we could calculate
the Shapley value. If instead we wanted to use the nucleolus as solution concept we had to
solve a lexicographic linear programming problem using these values. However in a game
with n players the number of coal itions is 2n. In our type of game this means that we have
to solve 2n integer programming problems in order to be able to calculate the Shapley
value. This is a hard and expensive task for the problem sizes that we are interested in
which n ranges from 10 - 20. For n = 10 we have 1024 coalitions and for n = 20 there are
1048576 coalitions. In such large games the nucleolus is a far superior solution concept
since it is not necessarily so that we need to compute all characteristic function values.
In numerical experiments for games with 10 players based on a real life data the number
of characteristic function values that we have been generating in order to calculate the
nucleolus ranges from 29-74 out of the 1024 possible. (We always have to compute v(N)
i.e. the 1024-th coalition.
It should be noted that our illustrative example is far from a realistic situation. We
assumed that it is the fields that are the players, not taken into consideration that the
petroleum field are owned by more than one company. Furthermore we have omitted
to take into consideration the uncC'rtainty in the situation as for instance the potential
132
appearance of new, at the planning states unknown fields in the region. We also assumed
that the production profiles for the fields we determine apriori. This means that we have
already selected the development stategy for the fields without taking into consideration
the joint venture alternative and the effects of this. Some of these simplifications can be
handled at a higher computational cost by using for instance models in which the reservoir
characteristics are more explicitely taken into account. Hallefjord et al (1986) presents a
field sequencing model of the kind needed.
7 Conclusion
We have briefly described a way to calculate a cost allocation scheme for a sequencing and
process equipment design game in which the characteristics function is given implicitly as
the solution to a large scale mixed integer programming problem. Preliminary computa-
tional tests shows that he method requires relatively few evaluations of the characteristic
functions. More detailed results will be given in a forthcoming paper.
References
[1] Aboudi R., Jornsten K, Bjomestad S. (1989,a) Sequencing Offshore Oil and Gas
Fields, Investigacion Opemtiva 1,:3 (1990).
[2] Aboudi R., Hallefjord A., Helgesen C., Helming R., Jornsten K., Pettersen A. S.,
Raum T., Spence P., A Mathematical Programming Model for the Development of
Petroleum Fields and Transport Systems, European Journal of Operational Research
43,1 (1989).
[3] Dragan I., A Procedure for Finding the Nucleolus of a Cooperative n Person Game,
Zeitschrijt fur Operations Research 25 119-131 (1981).
[4] Granot D., A Generalised Lineal' Production Model, Mathematical Programming 34
212-222 (1986).
[5] Hallefjord A., Nilssern T., A Dynamic Model for the Design of a Pipeline Network,
CMI working paper, Chr. Michelsen Institute Bergen Norway (1984).
[6] Hallefjord A., Helming R., Jornsten K., Computing the Nucleolus when the Charac-
teristic Function is given Implicitly: A Constraint Generation Approach, Research
Report Chr. Michelsen Institute for Science and Intellectual Freedom Report, No
30151 (1988).
[7] Schmiedler D., (1969), The Nucleolus of a Characteristic Function Game, SIAM J.
Appl. Math. 17(6) 1163-1170 (1969).
On Preference Orders for Sequencing Problems
Or, What Hath Smith Wrought?
E. L. Lawler *
Abstract
When in 1956 W. E. Smith proposed the ratio rule for solving the unconstrained
weighted completion time problem, he suggested an abstraction in the form of a
preference order. Over the years, the concept of a preference order has been much
elaborated. It is now applied to abstract problems in optimal sequencing, with
general precedence constraints being dealt with by the technique of modular de-
composition. This paper provides a unified and self contained exposition of these
results, with some new material on the treatment of contiguity constraints.
1 Introduction
A fundamental problem in combinatorial optimization is that of finding an optimal se-
quence. An abstract formulation of this problem is as follows.
Let j be a real-valued function that assigns a cost to each permutation of a set N of
n jobs. It is desired to find a permutation 7r* of N such that
j(7r*) = min{f(7r)I7r a permutation of N}. (1.1)
If the structure of the function j is unknown to us, there is no alternative but to
evaluate the cost of each of the n! permutations of N. This might happen, for example, if
j is specified by a "black box" subroutine and we are allowed only to submit a permutation
7r to the subroutine and to receive a value j( 7r) in return.
'Computer Science Division, University of California, Berkeley CA 94720
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akglil et al.
© Springer-Verlag Berlin Heidelberg 1992
134
To further complicate matters, there may be contiguity constraints and/or precedence
constraints. Contiguity constraints are specified by a family of subsets Si ~ N, i =
1,2, ... k. A permutation 7r is feasible if and only if the jobs in each Si appear consecutively
in 7r. Precedence constraints are specified by a partial order ~ on the jobs. A permutation
7r is feasible if and only if it 7r is a linear extension of this partial order, i.e., i ~ j implies
i precedes j in 7r. Given either or both of these types of constraints, the problem is to
find a feasible permutation 7r* of N such that
f(7r*) = min{J(7r)I7r a feasible permutation of N}. (1.2)
Again, if the structure of the function f is unknown, there is no alternative but to
evaluate the cost of each feasible permutation. And although the number of feasible
permutations may be much smaller than n!, this number is likely to be horrendously
large.
In practice, of course, we often know a great deal about the structure of the function
f. The weighted completion time problem is such an example. In this problem, each job
j is specified by two parameters, a processing time Pj > 0 and a weight Wj > O. The
jobs are processed one after another in the order specified by a permutation 7r, with the
processing of first job beginning at time zero. Hence 7r causes each job j to be completed
at a well defined time CJ. The value f(7r) is then the weighted sum of the completion
times, EwjCJ.
In 1956, W. E. Smith iSm] showed that the weighted completion time problem can be
solved by what is now called Smith's ratio rule: an optimal permutation 7r* is obtained
by ordering the jobs in nondecreasing order of the ratios Pj / Wj. Smith also observed that
the ratio rule lends itself to an elegant and useful abstraction. In our terminology, Smith
formulated the notion of a preference order satisfying the adjacent interchange property
on jobs.
During the 1960 1s and 70's, researchers extended Smith's results on the weighted
completion time problem to deal with precedence constraints of various kinds. Conway,
Maxwell and Miller [CMM] showed how to deal with precedence constraints in the form
of parallel chains. Horn [Ho] generalized this result by giving an efficient algorithm for
forests of rooted trees. Lawler [La] provided an O( n log n) algorithm for series parallel
precedence constraints, with rooted trees as a special case.
Over the same period, an impressive number of results were obtained for a variety
135
of related problems, including weighted discounted completion time [RoJ, least cost fault
detection [Ga], maximum cumulative cost [Mo], and two-machine permutation flow shop
problems [Si79]. Algorithms for these problems proved to be quite similar, and it was
observed that a straightforward extension of Lawler's series parallel algorithm can be
applied to each of them. An abstraction providing this insight is that of a preference
order satisfying the adjacent interchange property on sequences of jobs.
In 1977 it was shown that the weighted completion time problem is NP-hard for gen-
eral precedence constraints [La]. NP-hardness results have been obtained for some, but
not all, of the other problems mentioned above. However, it turned out that there are
ways to ameliorate the difficulties imposed by the NP-hardness barrier. Sidney [Si75,
Si81] proposed a decomposition of jobs into "modules" defined by precedence constraints.
Buer and Moehring [BM], and later Muller and Sprinrad [MS], provided efficient algo-
rithms for modular decomposition, generalizing the notion of series parallel decomposition.
Sidney and Steiner [SS] pointed out that modular decomposition allows one to compute
an optimal permutation by applying dynamic programming only to those modules that
are neither series nor parallel components. The attractiveness of this approach is that
enumerative techniques need be applied only to the irreducibly intractible parts of a prob-
lem instance. Monma and Sidney [MS87] provided sufficient conditions under which this
approach is valid.
In this paper we provide a unified and self contained exposition of these results. The
progression we make is consistently from the special case to the more general, paralleling
the historical development of the theory as summarized above. The weighted completion
time problem is offered as a prototype of the problems to which the theory applies. A few
new ideas are presented, particularly in the formulation of sufficient conditions for the
application of modular decomposition. However, the paper is intended to be primarily
tutorial in nature.
2 Preference Orders on Jobs
As noted above, the weighted completion time problem can be solved by applying the ratio
rule of W. E. Smith: A sequence is optimal if it puts the jobs in order of nondecreasing
ratios Pj/Wj.
To gain some insight, let us first prove the converse of Smith's assertion. Consider
a sequence in which the jobs are not in ratio order. Then there is a job i immediately
136
preceded by ajob j, withpj/wj > pi/wi. If job i completes at time Gi then job j completes
at time Gi - Pi. The effect of interchanging jobs i and j in the sequence is to decrease its
cost by a strictly positive amount,
[Wj(Gi - Pi) + WiG;] - [Wi(Gi - pj} + WPi] = WiPj - WjPi
WjWi(Pj/Wj - pi/Wi) > 0,
from which it follows that the sequence cannot be optimal.
A permutation or sequence can be represented as the concatenation of disjoint subse-
quences, e.g., a permutation 7r of the n jobs may be represented as 7r = (u, s, t, v), where
each of the n jobs appears in exactly one of the subsequences u, s, t, v. A single job is
considered to be a sequence of length one. What the analysis above indicates is that
pi/Wi < pj/Wj implies f(u,i,j,v} < f(u,j,i,v}, where u and v are arbitrary sequences
(including possibly the empty sequence). It is also easy to see that pi/Wj = pj/Wj implies
f(u,i,j,v) = f(u,j,i,v}.
We shall refer to a transitive and complete relation $ on the set of jobs N as a
preference order on jobs, where the implication is that any permutation conforming with
a "preferred" order is optimal. When we say that the relation is complete we mean that
for each pair of jobs i,j either i $ j or j $ i, or both. A transitive and complete
relation is sometimes called a quasi total order. Such a relation induces a linear ordering
of equivalence classes. Jobs i and j are in the same equivalence class if and only both
i $ j and j $ i, in which case we may choose to write i ~ j. If i $ j, but it is not the
case that j $ i, we may write i < j.
A preference order on N is said to satisfy the adjacent interchange property on jobs,
with respect to the cost function f, if for all pairs of jobs i,j in N,
i$j implies f(u,i,j,v)$f(u,j,i,v}, for all sequences u,v. (2.1)
Theorem 2.1. Given a preference order $ satisfying the adjacent interchange property
on jobs, an optimal permutation of N can be found by sorting the jobs according to $,
with O( n log n) comparisons of jobs with respect to $.
Proof: Let 7r any sequence conforming with the preference order. and let 7r* be an optimal
permutation. If 7r* differs from 7r, then 7r* is of the form (u,j,i,v), where i precedes j in
137
71' and hence i $ j. From (2.1) it follows that f(u,i,j,v) $ f(71'*), and hence (u,i,j,v) is
also optimal. A finite number of such interchanges transforms 71'* into 71' and shows that
71' is optimal. o
Smith's ratio rule is now seen to be a corollary of the theorem, by creating a preference
order according to the obvious rule: i $ j if and only if P;fWi $ Pj/Wj.
3 Preference Orders on Sequences
In order to deal with contiguity and/or precedence constraints, we find it necessary to
extend our notion of preference orders. Following standard computer science usage, let
N* denote the (infinite) set of all sequences that can be formed from elements in N. A
transitive and complete relation on N* will be said to be a preference order on sequences.
A preference order on N* is said to satisfy the adjacent interchange property on se-
quences, with respect to the cost function f, if for all pairs of sequences s, t in N*,
s $ t implies f(u,s,t,v) $ f(u,t,s,v), for all sequences u, v. (3.1)
It is important to observe that the existence of a preference order satisfying the inter-
change property on jobs (2.1) does not imply the existence of a preference order satisfying
the interchange property on sequences (3.1). There are many examples of sequencing prob-
lems for which this extension is not possible. However, there is no such difficulty in the
case of the weighted completion time problem. The appropriate extension is: s $ t if and
only if p(s)/w(s) $ p(t)/w(t), where p(s),p(t) denote the sums of the processing times of
the jobs in s, t, and w(s), w(t) are the sums of the weights. We leave it as an exercise for
the reader to verify that this extension satisfies (3.1).
4 Contiguity Constraints
As a first application of preference orders on sequences, let us consider the matter of
contiguity constraints. Suppose contiguity constraints are specified by a family of pairwise
disjoint subsets Si in N, i = 1,2, ... , k. The procedure is intuitively clear: First put the
jobs in each subset Si in preference order. Then, having replaced each subset Si by an
optimal sequence Si, place the resulting set of sequences in preference order.
138
Theorem 4.1. Let Si ~ N, i = 1,2, ... , k be a family of pairwise disjoint subsets
specifying contiguity constraints. Given a preference order:::; satisfying the adjacent
interchange property on sequences, an optimal feasible permutation of N can be found
with O( n log n) comparisons of sequences with respect to :::;.
Proof: Sort each Si by :::; and replace Si by the resulting sequence Si. This requires no
more than O( n log n) comparisons of jobs. Then sort the resulting set of sequences by
:::;, which requires no more than O( n log n) comparisons of sequences. Let 'Jr denote the
feasible permutation induced on the jobs and let 'Jr' be an optimal feasible sequence. If
'Jr' differs from 'Jr in the order of the jobs in any Si, a finite number of interchanges of jobs
yields an optimal feasible permutation in which each Si is represented by sequence Si. A
finite number of interchanges of sequences transforms this permutation into 'Jr and shows
that 'Jr is optimal. 0
In Section 10 we deal with contiguity constraints that are specified by arbitrary families
of subsets. We show that for these general constraints, it is possible to compute optimal
sequences in O( n log n) time, provided we can make some stronger assumptions about the
preference order.
5 Parallel Chains Precedence Constraints
Given arbitrary precedence constraints -+, we might choose to ignore them and to simply
sort the jobs by the preference order :::;. If the resulting sequence turns out to be feasible
then we are done. This follows from the fact that, in effect, we have solved a relaxation
of the original problem in which all jobs are independent. If an optimal schedule for this
relaxation happens to be feasible with respect to the precedence constraints, then it must
be optimal with respect to those constraints.
If the permutation 'Jr obtained by sorting jobs by preference order is not feasible, it is
because of the collision of one or more pairs of jobs i, j where j :::; i and i -+ j. If also
i :::; j, the collision can be resolved by interchanging the jobs in 'Jr. But even if this is not
the case, i.e., j < i, it may be possible to resolve the collision by other means.
Consider the case of precedence constraints in the form of parallel chains as shown
in Figure 1. The only possible collisions that can occur are collisions between jobs in
the same chain. Colliding jobs that are adjacent in a chain satisfy the hypotheses of
the following lemma, which assumes that the adjacent interchange property holds for
sequences.
139
o
Figure 1: Parallel Chains.
Lemma 5.1. Let i,j be distinct jobs such that i -t j and j ~ i. Suppose for each job
k distinct from i and j either (i) k -t i, or (ii) j -t k, or (iii) k is unrelated to both i
and j by -to Then there exists an optimal feasible permutation in which i immediately
precedes j.
Proof: Let 7r* = (u,i,v,j,w) be an optimal feasible permutation in which i does not
immediately precede j. From the hypotheses of the lemma, each job k in v is unrelated
to either i or j by the precedence constraints. (For example, if i -+ k for some job k
in v, then j -+ k, which would imply that 7r* is infeasible.) It follows that precedence
constraints are not violated by interchanging i and v or interchanging v and j. It must
be the case that either v ~ i or j ~ v, else we would have i < v < j, contradicting the
hypothesis that j ~ i. Hence at least one of the two interchanges results in an optimal
feasible permutation in which i immediately precedes j. o
When the hypotheses of the lemma are found to apply to a colliding pair of jobs i,j,
the pair can be replaced by the sequence (i,j), with precedence constraints induced on
this sequence in the obvious way, e.g., j -+ k implies (i,j) -+ k. For the purpose of
reapplying the lemma, the sequence (i,j) plays the role of a single job. When precedence
constraints are parallel chains, at most n - 1 applications of the lemma suffice to resolve
all collisions. When all collisions have been resolved, we have a set of sequences such that:
(5.1) There exists an optimal permutation of the jobs in which the jobs in each
sequence s appear consecutively, in the same order as in s.
(5.2) Any ordering of the sequences by ~ is feasible. (That is, s ~ t implies t -;.. s,
for all sequences s, t.)
140
From properties (5.1) and (5.2) it follows that we have, in effect, transformed the
parallel chains problem to an unconstrained sequencing problem of type (1.1), to which
Theorem 2.1 can be applied. We have thus proved the following.
Theorem 5.2. Let precedence constraints -+ be specified in the form of parallel chains.
Given a preference order:::; satisfying the adjacent interchange property on sequences, an
optimal feasible permutation can be found with O( n log n) comparisons of sequences with
respect to :::;.
6 Series Parallel Precedence Constraints
Series parallel partial orders are defined recursively as follows:
(6.1) Any partial order (N, -+), where N is a singleton set, is series parallel.
Let (Nl' -+) and (N2' -+) be disjoint partial orders that are series parallel. A partial order
(Nl U N2, -+) is also series parallel, when relations between jobs in Nl and jobs in N2 are
determined by either
or
(6.2) series composition: each job i in Nl precedes each job j in N2, i.e., i E Nl,j E
N2 implies i -+ j,
(6.3) parallel composition: the jobs in Nl and N2 are unrelated, i.e., i E N},j E N2
implies i f+ j and j f+ i.
(Relations between pairs of jobs in Nl or N2, are unaffected.)
The structure of series parallel precedence constraints is represented by a composition
(or decomposition) tree in which each leaf of the tree is identified with a single job and each
internal node corresponds to a series or parallel composition operation, and is accordingly
labeled either S or P. The left and right children of an S node are respectively identified
with the subsets Nt. N2 of the series composition operation. The same is true of the
children of a P node, except that the left-right ordering of its children is immaterial.
Some examples of series parallel precedence constraints and their composition trees
are indicated in Figure 2. Note that parallel chains, intrees, outtrees, and forests of intrees
and outtrees, are all special cases of series parallel constraints. The smallest nonseries
141
Figure 2: Series Parallel Constraints.
Figure 3: "Z" Constraints.
142
parallel partial order is the "z" digraph shown in Figure 3. In fact, a partial order fails
to be series parallel if and only if it contains four elements in a "z" relation.
There are efficient algorithms for testing for series parallelism. In particular, there is
an Oem + n) algorithm for testing whether or not a given digraph G with m arcs and n
nodes induces a series parallel partial order [VTL]. If the partial order is series parallel,
the algorithm "parses" it and returns a decomposition tree. If the partial order is not
series parallel, the algorithm returns a "Z", proving that it is not.
Given a decomposition tree for series parallel precedence constraints, a plausible strat-
egy for finding an optimal sequence would seem to be to work from the leaves of the tree
toward the root, dealing with the subproblem at an internal node only after the subprob-
lems at its children have been dealt with. At each node of the tree we propose to obtain
a collision-free set of sequences satisfying properties (5.1) and (5.2). An optimal feasible
permutation for the complete set of jobs is then obtained by sorting the set of sequences
obtained at the root.
At a leaf of the tree, one simply creates a set S containing the single sequence j, where
j is the job identified with the leaf. At a P node with sets Land R for its left and right
children, all that is necessary is to form the set S = L U R, since none of the sequences
in Land R collide. At an S node, things are a bit more complicated. However, collisions
can be resolved by repeated application of the following generalization of Lemma 5.1. In
the statement of this lemma, max L and min R are meant to denote jobs in Land R that
are respectively maximal and minimal with respect to the preference order :::;.
Lemma 6.1. Let L, R be disjoint subsets of jobs such that i ---t j, for all i E L, j E R
and min R :::; maxL. Suppose for each job k (j. L U R either (i) k ---t i, for all i E L, or (ii)
j ---t k, for all j E R, or (iii) k is unrelated to all jobs in L U R by ---to Then there exists
an optimal feasible permutation in which max L immediately precedes min R.
Proof: Let I = max L, r = min R, and let Jr* = (u, I, v, r, w) be an optimal feasible
permutation in which 1 = max L fails to immediately precede r = min R. From the
hypotheses of the lemma, each job k in v is either in L or in R or is unrelated to either
lor r by the precedence constraints. A finite number of interchanges of sequences yields
an optimal feasible permutation in which no jobs in L or R appear between I and r. By
the same reasoning used in Lemma 5.1, a final interchange of either I with v or v with r
yields an optimal feasible permutation in which 1 immediately precedes r. o
Note that when max L and min R are replaced by a single sequence s = (max L,
143
min R), there may be a collision between s and one of the jobs remaining in the set L, or
between s and one of the jobs remaining in R. In either case, the hypotheses of Lemma
6.1 are again satisfied, with the singleton set s in the role of either L or R.
We are now prepared to describe a recursive procedure for computing a collision-free
set of sequences Sex) at node x of a series parallel decomposition tree. In the following
pseudocode it is to be understood that when L is empty the max function returns a
dummy job such that max L < j, for all jobs j; similarly, if R is empty then min R > j,
for all jobs j.
Sex):
Case (x is a leaf):
return S := {j}, where j is the job at x;
Case (x is a P node):
return S := S(left(x)) union S(right(x));
Case (x is an S node):
L := S(left(x))j
R := S(right(x))j
if max L < min R then return S := L union R
else
s := (max L, min R);
L := L - max L;
R := R - min R;
while (max L >= s or min R <= s)
if max L >= s then
s := (max L, s) j
L := L - max Lj end if
if min R <= s then
s .= (s, min R)j
R := R - min Rj endif
endwhile
return S := L union R union {s}j
endif
A good implementation of the series parallel algorithm is to store the set of sequences
Sex) computed at node x in a priority queue supporting the operations of findmin, find-
max, deletemin, deletemax, and merge. Each of these operations can be implemented to
144
require no more than O(log n) time, and each operation is performed no more than O( n)
times. The final sort of the strings obtained at the root of the tree requires no more than
O( n log n) comparisons. Hence we have the following result.
Theorem 6.2. Let series parallel precedence constraints ---+ on N be specified by a
decomposition tree. Given a preference order S satisfying the adjacent interchange prop-
erty on sequences, an optimal feasible permutation of N can be found with O( n log n)
comparisons of sequences with respect to S (and at most O( n log n) time for other oper-
ations ).
7 Modular Decomposition of Precedence Constraints
Series parallel decomposition is a special case of "modular" decomposition, which applies
to arbitrary precedence constraints.
A subset of jobs M in N is said to be a job module with respect to precedence con-
straints ---+, if for each job kEN - M either
(7.1)
(7.2)
(7.3)
k ---+ j,
j ---+ k,
k ~ jand j ~ k,
for all j E M,
for all j E M,
for all j E M.
A job module for which (7.3) applies for no kEN - M is a series component. A module
for which neither (7.1) nor (7.2) applies is a parallel component.
Every set of precedence constraints on n 2 2 jobs admits of n + 2 trivial modules,
namely the empty set, all singleton sets and the set N of all jobs. Nonseries parallel
precedence constraints are said to be prime when the only modules are trivial. For
example, prime precedence constraints induced by the nonseries parallel "z" digraph are
prime. At the other extreme, the empty set of precedence constraints has all 2n subsets
of N as job modules.
Theorem 7.1. Let M and M' be job modules.
(7.4) M n M' is a module
(7.5) If M n M' =1= 0 then MUM' is a module.
(7.6) If M and M are overlapping sets, i.e., none of M n M', M - M', M' - M is an
empty set, then M - M' and 111' - M are modules.
145
Proof: Left as exercise.
One way to summarize the content of Theorem 7.1 is as follows: If M and M' are
overlapping modules, then so are M n M', MUM, M - M', and M' - M. The reader
may be interested to verify that the modules of any precedence constraints form a lattice
with respect to the relation of set inclusion, where the least upper bound (l.u.b.) of M
and M' is the interestion of all modules containing MUM' and the greatest lower bound
(g.l.b.) of M and M' is simply M n M'.
Let us call a module M =I- N maximal if it is not a proper subset of any module
distinct from N. The trivial singleton modules are maximal modules of prime precedence
constraints.
Theorem 7.2. For any set N of two or more precedence constrained jobs, exactly one
of the following decompositions exists:
(7.7) a series decomposition into two or more series components, and the minimal
such modules uniquely partition N.
(7.8) a parallel decomposition into two or more parallel parallel components, and the
minimal such modules uniquely partition N.
(7.9) a nonseries parallel decomposition, and the maximal modules of the decompo-
sition uniquely partition N.
Proof: Let us show that the maximal modules in a nonseries parallel decomposition are
pairwise a.isjoint, leaving the remainder of the proof for the reader. Assume that there
are no series or parallel components. Let .M and A1' be distinct maximal modules, with
M n M' =I- 0. M and M' must be overlapping, else one would be a proper subset of the
other and hence not maximal. But then M n M', M - M',M' - M and MuM' are all
modules. It must be the case that M UAI' = N, else neither M nor M' would be maximal.
Let i be a job in M - ,M' and suppose i --+ j for some j EM'. Then it must be the case
that i --+ j for all j EM', to be consistent with property (7.1) for M'. But then we must
have i --+ j for all i E M - M' and each j E M', in order to be consistent with property
(7.2) for M - M'. But this would mean that M - M' and M' are series components, a
contradiction. By symmetry, M - M' and M' would also be series components if i+- jfor
some i E M - M' and j E M. If neither i --+ j nor i +- j, M - M' and M' would be
parallel components, also a contradiction. It follows that M and M' must be disjoint. 0
146
Figure 4: Decomposition Tree.
A complete modular decomposition of a set of precedence constraints is represented
by a decomposition tree in which each internal node is labelled S, P or N, corresponding
to the three cases in Theorem 7.2. That is, the complete set of jobs is decomposed into
modules, each of these modules is further decomposed into modules, and so forth. For
example, Figure 4 shows the decomposition tree for the precedence constraints shown in
Figure 5. A decomposition into three or fewer modules must be either series or parallel,
because the "z" digraph with four nodes is the smallest nonseries parallel digraph. Hence
each N node of the decomposition tree must have at least four children. There is an
algorithm for obtaining the modular decomposition tree of n jobs in O(n 2 ) time [MSj.
Unlike the decomposition trees dealt with in the previous section, the tree in Figure 4
permits an S or P node to have three or more children, one for each minimal series or
parallel component. It is a straightforward matter to restructure the tree so that each S
or P node has exactly two children, and hereafter we shall assume that this is the case.
As in the case of series parallel decomposition trees, the leaves of the tree are identified
with individual jobs. An N node, all of whose children are leaves, corresponds to a prime
decomposition of jobs identified with its leaves.
As in the more special case of series parallel decomposition, a plausible strategy for
finding an optimal permutation for the complete set of jobs is to work from the leaves of
147
Figure 5: Precedence Constraints.
the decomposition tree toward the root. At each node of the tree we propose to obtain
a collision-free set of sequences satisfying properties (5.1) and (5.2). An optimal feasible
permutation for the complete set of jobs is found by sorting the set of sequences obtained
at the root.
The operations that are performed at leaves, S nodes and P nodes are exactly the
same as those prescribed in the previous section. At an N node, it seems there is lit-
tle alternative but to employ an enumerative technique, like dynamic programming, to
obtain an optimal sequence for the N node from the sets of collision-free sequences al-
ready obtained for its children. If it is possible to establish that there exists an optimal
permutation for the complete set of jobs that is consistent with the optimal sequence 7r
computed at the N-node, 7r can be taken to impute a chain of precedence constraints on
the sequences contained within it. Collisions between sequences in this chain can be elim-
inated by precisely the same procedure used to eliminate collisions in the parallel chains
algorithm. When this is done, the computation at each N node x returns a collision-free
set of sequences S(x), like the sets returned at S nodes and P nodes. To summarize, the
algorithm is the same as in the case of series parallel decomposition, with the addition of
one case,
Case (x is an N node):
obtain a collision-free set of
sequences SCy) for each child y of Xj
apply dynamic programming, or any other technique,
to compute an optimal sequence pi from the
sets S(y);
148
impute a chain of precedence constraints from pi
and apply the parallel chains algorithm to
resolve collisions in the chain;
return the resulting collision-free set S;
The advantage of modular decomposition is that it isolates exactly those parts of
the problem instance that we are forced to solve enumeratively. There is also reason to
believe that time bounds for dynamic programming computations at the N nodes of the
decomposition tree may be acceptable. Recursion relations for the weighted completion
time problem are of the form
F(0) = 0,
F(S) min{F(S - (j)) + wjp(S)1 j has no successors in S}. (7.10)
The time required for the computation is bounded by O(wnW), where W is the width
of the precedence constraints, i.e., the maximum size of an antichain. As pointed out in
[SS], W is at most two less than the number of children of an N node.
8 The Strong Substitution Property
We have yet to establish that a permutation 7rM is optimal for a module M in N if and
only if 7rM is consistent with an optimal permutation 7rN of N. Without this key result,
which we shall prove in Section 9, we cannot be assured that it is possible to compute an
optimal sequence for a set of jobs from optimal sequences for its individual modules.
The fact is that the adjacent interchange property on sequences (3.1) is, by itself,
not a sufficiently strong assumption to imply the i'esult we desire. The reader is invited
to construct an counterexample with five jobs and four modules with "Z" constraints.
What we propose to do is to make some additional assumptions about the properties
of the preference order. These assumptions are satisfied by the preference order for the
weighted completion time problem, and by other similar problems.
The first thing that we propose to do is to replace (3,1) by the strong adjacent inter-
change property:
149
s :::; t if and only if f( u, s, t, v) :::; f( u, t, s, v), for all sequences s, t, u, v. (8.1)
From (8.1) it follows that
s < t implies f(u,s,t,v) < f(u,t,s,v).
Hence, with property (8.1) as a hypothesis, the conclusion of Theorem 2.1 of this chapter
is strengthened so that the converse of its conclusion is true, i.e., a permutation of N is
optimal if and only if it places jobs in order with respect to :::;.
Property (8.1) is useful because of the structure it imposes on the preference order
with respect to sequences s, t of disjoint sets of jobs. It turns out to be useful for the
preference order to satisfy certain properties with respect to pairs of sequences s, s' of the
same set of jobs.
For example, suppose
s :::; s' if and only if f(s):::; f(s'), (8.2)
for all sequences s, s' of the same subset S. When (8.2) holds, sequences of a given subset
S are quasi-linearly ordered by the preference order:::; exactly as they are ordered by their
cost, as determined by the function f. This means that we have the agreeable property
that a sequence is optimal if and only if it is minimal with respect to :::;.
We next observe that dynamic programming is likely to be the method of choice
for computing optimal sequences at the N nodes of a decomposition tree. Accordingly,
it would seem to be a good thing to validate Bellman's Principle of Optimality. For
recurrences like (7.10), it is sufficient to require
f(s) :::; f(s') implies f(s,j):::; qf(s',j), (8.3)
for all jobs i and sequences s, s' of the same subset of jobs. However, we propose to assume
a property that implies both (8.2) and (8.3), namely,
s:::; s' if and only if f(u,s,v):::; f(u,s',v), (8.4)
150
for all sequences u, s, S', v, where sand S' are sequences of the same set of jobs S. We
shall call (8.4) the strong substitution property for sequences.
Hereafter, without further mention, we shall assume that the preference order ~ sat-
isfies properties (8.1) and (8.4). As a useful consequence of these properties, we have the
following.
Lemma 8.1. (Monotonicity)
s < t implies s < (s,t) < (t,s) < t
s~t implies s~(s,t)~(t,s)~t
Proof: s ~ t implies f(s, (s, t)) ~ f(s, (t, s)) = f«s, t), s), by (8.1), which implies
s ~ (s,t), by (8.1), and (s,t) ~ (t,s), by (S.4). Proofs of the remaining inequalities are
left for the reader. o
9 The Structure of Optimal Permutations
The technical development on which we are about to embark is adapted from Monma and
Sidney [MSS7j. This development leads to an interesting and useful characterization of
the structure of optimal permutations. But first we need some more definitions. A subset
S in N is said to be an initial set of N if all predecessors of jobs in S are in S. That
is, S is an initial set of N if, for each job j E S, i -t j implies i E S. If 7r is a feasible
permutation of N, then the subset S of jobs contained in any prefix s of 7r is an initial set.
(Initial sets are precisely the sets S defining useful states in the dynamic programming
recurrence relations (7.10).) Conversely, a feasible sequence s for an initial set S may be
said to be is a feasible prefix of a permutation of N. Let us say that an optimal sequence
s for an initial set S is a preference-order minimal feasible prefix for N if
s ~ S', for all feasible prefixes S' of N,
and, moreover, s < S', where S' is a feasible sequence for any initial set S' that is a proper
subset of S. (Because of the latter minimality condition, it might be more precise to use
the term "minimal preference-order minimal feasible prefix"; we shall usually prefer the
term "minimal prefix".) An initial set S whose optimal sequence s is a minimal prefix
will be said to be simply a minimal initial set.
It follows from their definition that the union or intersection of two initial sets is an
initial set. The union or intersection of two minimal initial sets is definitely not a minimal
151
initial set. One of the consequences of the important technical lemma below is that the
minimal initial sets are pairwise disjoint.
Lemma 9.1. Let 8 be a minimal prefix, and let (8', t), be a feasible sequence for the
same jobs appearing in 8, where neither 8' nor t is empty and t is optimal. Then 8 > t.
Proof: For the purpose of obtaining a contradiction, assume there is a feasible sequence
(8', t) for which 8:5 t. Further, without loss of generality, assume that the sequence (8', t)
has been chosen so that the set of jobs appearing in t is minimal with respect to the
property 8 :5 t. Remove all precedence constraints between jobs in 8' and jobs in t and
consider an optimal sequence for the jobs in s with these precedence constraints removed.
Such a sequence is of the form
where each 8i contains jobs in 8', each ti contains jobs in t and possibly 8~ and/or s~+1 is
empty. From the optimality of this sequence it follows that
and that each 8i and ti is optimal.
Case 1: s~ is nonempty. Then s < s~,
from the definition of s as a minimal prefix, and
which by Lemma 8.1 implies s < s", a contradiction.
Case 2: si is empty and r > 1. Then s < s~, again by definition of s as a minimal
prefix, and so s < i r • But s > tTl from the assumption that the set of jobs appearing in t
is minimal with respect to the property t ;::: 8, a contradiction.
Case 3: s~ is empty and r = 1, i.e., s" is of the form (t,s'), where s < s' and, by
assumption s :5 t. It then follows from Lemma 8.1 that s < s", again a contradiction. 0
Corollary 9.2. Minimal initial sets are pairwise disjoint.
Proof: Suppose S1 and S2 are minimal initial sets, with optimal sequences S1 and S2,
and S1 n S2 # 0. Neither S1 nor S2 is a subset of the other, else the larger set could not
be minimal, by definition. Let t he an optimal sequence for S2 - S1. Then i < S2, by
152
the lemma, (S1, t) is a feasible prefix, and (Sl' t) < Sl ~ S2, by monotonicity. But this
contradicts the preference-order minimality of Sl and S2' 0
Corollary 9.S. A minimal initial set cannot be partitioned into two nonempty initial
sets.
Proof: Let S be a minimal initial set with an optimal sequence s. Suppose S can be
partitioned into nonempty initial sets S' and T, with optimal sequences s', t. Then (s', t)
is a feasible sequence for Sand t < s, by the lemma, contradicting the preference-order
minimality of s. .0
Now for a most fundamental theorem concerning preference-order minimal initial sets.
Theorem 9.4. The jobs in each minimal initial set N occur consecutively in every
optimal permutation of N.
Proof: Suppose 7r is an optimal permutation of N in which the jobs in a minimal initial
set S do not occur consecutively. This means that the jobs in S are separated into two or
more subsequences s~, s~, ... , s~, i, with jobs in N - S between them. From the optimality
of 7r it follows that s~ ::; s~ ::; ... ::; s~ ::; i, and that each of these subsequences is optimal.
((s~, .. . , 5~), i) is a feasible sequence for S, and i < s, where 5 is an optimal sequence for
5, by Lemma 9.1. But 5 < 5~, from the minimality of S. The contradiction t < 5 < 5~ ::; t
proves the theorem. 0
Corollary 9.5. Every optimal permutation is prefixed by a minimal prefix.
Proof: Suppose 7r is an optimal permutation that is not prefixed by a minimal prefix,
and let s be the first minimal prefix to occur in 7r. Then 7r is of the form (tI, s, t 2 ),
where tl ::; s, else 7r is not optimal. But this implies that some subset S' of the jobs
appearing in tl is a minimal initial set, and s is not the first minimal prefix occuring in
7r, a contradiction. 0
Corollary 9.6. Every minimal prefix is the prefix of an optimal permutation.
Proof: Let 7r be an optimal permutation and let s be a minimal prefix. From the theorem
it follows that 7r is of the form (iI, s', t 2), where s' is an optimal sequence for the same
optimal initial set as s. If s =f:. s', substitute s for s'. If i 1 is nonempty then 5 <= tl and
interchanging tl and s yields an optimal feasible permutation with 5 as a prefix. 0
153
Corollary 9.6 clearly suggests that an optimal permutation 1r of N can be found by
repeatedly finding minimal initial sets and optimally sequencing them. This procedure,
due to Sidney [Si75], is indicated below in tail recursive form.
pi(N) :
if N is empty then return pi := empty sequence;
else
find a minimal initial set S in N;
compute an optimal sequence s for S,
perhaps by dynamic programming;
return pi := (s, pi(N-S));
endif
There are some practical difficulties with this procedure. Efficient algorithms for
finding minimal initial sets are lacking, except in the case of the weighted completion
time problem, for which a polynomial-time algorithm has been developed [La]. Also, in
the worst case the procedure yields no benefit; the only minimal initial set in N may
be N itself. However, the procedure is interesting for us because of its practical, not
theoretical, implications. The procedure is nondeterministic, in that there are various
choices of minimal initial sets that can be made, and various optimal sequences that can
be computed for them. Corollary 9.6 indicates that any sequence of these choices yields
an optimal permutation. Corollary 9.5 indicates that there are no other choices. These
observations imply the following result.
Theorem 9.7. A permutation 7r of N is optimal if and only if it can be obtained by
applying Sidney's procedure to N.
o Before concluding with our main result, we need to make one more observation about
Sidney's procedure. Suppose the procedure is applied to a set of MUM', where M and
M' are unrelated, i.e., in parallel, with respect to precedence constraints -to Corollary 9.3
states that no minimal initial set can be partitioned into two initial sets. It follows that
each minimal initial set found in the course of the execution of the procedure is a minimal
initial set of either M or of M'. It is not hard to see that any optimal permutation of
MuM' that is obtained by the procedure is consistent with an optimal permutation of
M, and conversely.
Theorem 9.S. A permutation 7rM for a module 111 in N is optimal if and only if 7rM is
consistent with an optimal permutation 7rN of N.
154
Proof: Consider an optimal feasible permutation 7rN of N. 7rN is of the form (u,v,w),
where all the jobs in M appear in v, and the other jobs appearing in v comprise a subset
M' in N that is unrelated to M. The validity of the theorem follows from the observations
made in the previous paragraph.
10 Arbitrary Contiguity Constraints
In Section 4 we introduced the notion of contiguity constraints and observed that con-
straints imposed by a pairwise disjoint family of subsets can be dealt with quite easily.
We shall now describe how to compute an optimal sequence when constraints are specified
by an arbitrary collection of nonempty sets Sj C N, i = 1,2, ... ,k.
It is a nontrivial problem even to determine whether an arbitrary set of contiguity
constraints is consistent. However, this question is equivalent to a well-solved problem
concerning the consecutive 1's property of (0,1) matrices. Let C = (Cjj) be a k X n matrix
of O's and 1's, with
Cjj = 1,
= 0,
if j E Sj,
otherwise.
It is easy to see that the contiguity constraints are consistent if and only if it is possible to
permute the columns of C so that all the 1's in each row occur consecutively. Furthermore,
there is a one-one correspondence between feasible sequences of jobs and permutations
yielding consecutive 1 's in rows.
Booth and Leuker [BL] have provided an effficient algorithm for testing matrices for
the consecutive 1 's property, or equivalently, the consistency of contiguity constraints. If
the contiguity constraints are consistent, their algorithm outputs a data structure called
a PQ-tree that implicitly represents all feasible sequences. The PQ-tree is a rooted tree
whose leaves are labeled with jobs. It has two kinds of internal nodes: P-nodes and
Q-nodes. (The bars on the letters P and Q are to to distinguish these nodes from the
nodes of a decomposition tree.) The ordering of the children of a P-node is unimportant,
while those of a Q-node must occur in one fixed order or its reverse. We can think of
a PQ-tree as a Calder-like mobile capable of taking on a variety of configurations. (In
the literature of PQ-trees, it is customery to reinforce this imagery by drawing a Q-node
as a rectangle, giving the impression of a rigid bar.) Any reconfiguration of the PQ-tree
is legitimate, so long as it can be obtained by an arbitrary permutation of the children
of each P-node and, for each Q-node, either leaving its children in their given order or
155
Figure 6: PQ-tree for Example.
reversing the order. Each such reconfiguration induces a different feasible sequence of the
jobs, which is obtained by reading the labels of the leaves from left to right. Conversely,
each feasible sequence can be obtained from such a configuration.
As an example, a PQ-tree for the sets {1, 2}, {3, 4}, {4, 5}, {2, 3, 4, 5}, {8,9},
{8, 9, 10}, {9,10} is pictured in Figure 6. This tree shows that 1,2, ... ,10 is a feasi-
ble sequence, but the tree can also be reconfigured, as shown in Figure 7, to obtain
6,10,9,7,8,1,2,5,4,3 as another feasible sequence.
It is important to observe that the jobs identified with the leaves of any subtree of a
PQ-tree must appear consecutively in any feasible sequence. Or, to put it differently, for
each internal node of the tree, there is an implied contiguity constraint that applies to
all the leaves below it. Keeping this observation in mind, let us consider how to compute
an optimal feasible sequence by working upward from the bottom of the PQ-tree, in the
same way that we worked upward in decomposition trees. Each child of an internal node is
either a leaf, or another internal node for which there is an implied contiguity constraint.
When we are ready to deal with a given internal node of a PQ-tree, we have already
computed a single optimal sequence for each of its children. For a F-node, we place these
sequences in preference order and concatenate them, obtaining a single optimal sequence
for the F-node. For a Q-node, we concatenate the optimal sequences for its children
in each of the two permissible orders, make a preference-order comparison of the two
resulting sequences, and discard the one that is not optimal. It is easy to see that the
strong substitution property (S.4) guarantees that the sequence we obtain at the root of
the tree is an optimal feasible sequence of the n jobs.
156
Figure 7: Reconfiguration of PQ- Tree.
The Booth-Leuker algorithm runs in time O(n + s), where s = Ei ISil. The computa-
tion of an optimal sequence requires at most O( n log n) comparisons with respect to the
preference order, for sorting at P-nodes. We thus have the following.
Theorem 10.1. Let Si C N, i = 1,2, ... , k, bean arbitrary collection of nonempty
sets specifying contiguity constraints. Given a preference order:::; satisfying the adjacent
interchange property on sequences and the strong interchange property, an optimal feasible
permutation of N can be found in O( n log n + s) time, where s = Ei ISil.
11 Mixed Contiguity and Precedence Constraints
For our technical finale, we shall show how to deal with combined contiguity constraints
and precedence"constraints. We have already constructed virtually all the pieces of al-
gorithmic machinery we shall need; all we have to do is to apply them properly, in the
proper order.
First use the Booth-Leuker algorithm to construct a PQ-tree for the given contiguity
constraints. If the algorithm finds the constraints are inconsistent, then stop. Otherwise,
for each internal node of the PQ-tree, there is a well-defined sequencing problem with
precedence constraints. From the precedence digraph G = (N, A) for the complete set
157
of jobs N, define precedence relations for the sequencing problems at the internal nodes
of the PQ-tree as follows. For each arc (i,j) E A, find the least common ancestor, k,
of the leaves labeled i and j. If the leaves labeled i and j are below children x and y,
respectively, then x --+ y at node k.
The precedence constraints induced at each F node must be tested for acyclicity. If
at any F-node the constraints fail to be acyclic, then stop; the contiguity constraints and
the precedence constraints specified for the complete set of jobs are mutually inconsis-
tent. Likewise, the precedence constraints induced at each Q-node must be tested for
consistency with each of the two permissible orders at the node. If at some Q-node the
precedence constraints are consistent with neither order, then stop.
Next process the nodes of the PQ-tree from the bottom of the tree upward.
At each F-node, construct a decompostion tree for the precedence constraints at that
node and compute an optimal sequence, as described in Section 7. At a Q-node, the com-
putation is quite simple. If there are no precedence constraints at the node, concatenate
the optimal sequences already computed for iLs children in each of the two permissible
orders and determine the better of the two by making a preference order comparison. If
there are precedence constraints at the node, then simply concatenate the optimal se-
quences in the unique order that is consistent with the precedence constraints. When
processing has been completed at the root of the tree, an optimal sequence has been
obtained for the complete set of jobs.
The construction of the PQ-tree requires O(n + s) time, where s = L: IS;I. With
the use of a static least-common ancestor algorithm, the arcs of the precedence digraph
G can be assigned to the internal nodes of the PQ-tree in O(m + n) time, where m is
the number of arcs in G. The testing of acyclicity of constraints requires no more than
O(n + m) time. The construction of decompostion trees can at F-nodes can be carried
out in O(n 2 ) time overall. Thus the total time required is O( n 2 + s), exclusive of the time
required to actually carry out the optimization computations over the decomposition trees
at the F-nodes.
12 What Hath Smith Wrought?
It all began with some uncomplicated ideas of W. E. Smith. He gave us the ratio rule
and its abstraction as a preference order. In unsmooth progression, many others shaped
158
these ideas to deal with precedence constraints, first as parallel chains, then as rooted
trees, then as series parallel structures. Today we have a method for dealing with general
precedence constraints via modular decomposition. The theory becomes more elegant,
but also much harder.
The thirty five years of progress we have recapitulated here could well serve as a case
study in a very small slice of science. But if anything is sure, it is that we are not at the
end of this story.
References
[1] Booth, K. S., and G. S. Leuker. Testing the consecutive ones Property, interval
graphs, and graph planarity using PQ-tree algorithms. J. Comput. Syst. Sci., 13:
335-379, 1976.
[2] Buer, H., and R. H. Moehring. A fast algorithm for the decomposition of graphs and
posets. Math. Oper. Res., 8: 170-184,1983.
[3] Conway, R. W., W. L. Maxwell, and 1. W. Miller. Theory of Scheduling, Addison-
Wesley, 1967.
[4] Garey, M. R. Optimal task sequencing with precedence constraints. Discrete Math.,
4: 37-56, 1973.
[5] Horn, W. A. Single-Machine job sequencing with treelike precedence ordering and
linear delay penalties. SIAM J. Appl. Math., 23: 189-202, 1972.
[6] Korte, N., and R. H. Moehring. Transitive orientation of graphs with side con-
straints. Proc WG '85, Workshop on Graphtheoretic Concepts in Computer Science,
H. Notemeier (ed.), Trauner Verlag, Linz, 143-160, 1985.
[7) Lawler, E. 1. Sequencing jobs to minimize total weighted completion time subject
to precedence constraints. Ann. Disc. Math., 2: 75-90, 1978.
[8] Lawler, E. 1., and B. D. Sivazlian. Minimization of time-varying costs in single-
machine sequencing. Operations Research, 26: 563-569, 1978.
[9] Monma, C. 1. Sequencing to minimize the maximum job cost. Operations Research,
28: 942-951, 1980.
[10) Monma, C. L., and J. B. Sidney. Sequencing with series parallel precedence con-
straints. Math. Oper. Res., 4: 215-224, 1984.
159
[11] Monma, C. L., and J. B. Sidney. Optimal sequencing via modular decomposition:
Characterization of sequencing functions. Math. Oper. Res., 12: 22-31, 1987.
[12] Muller, J. H., and J. Spinrad. Incremental modular decomposition. J. Assoc. Comput.
Mach., 36: 1-19, 1989.
[13] Rothkopf, M. E. Scheduling independent tasks on parallel processors. Management
Science, 12: 437-447, 1966.
[14] Sidney, J. B. Decomposition algorithms for single-machine sequencing with prece-
dence relations and deferral costs. Operations Research, 23: 283-298, 1975.
[15] Sidney, J. B. The two-machine maximum flow time problem with series parallel
precedence relations. Operations Resea1'ch, 27: 782-791, 1979.
[16] Sidney, J. B. A decomposition algorithm for sequencing with general precedence
constraints. Math. Opel'. Res., 6: 190-204, 1981.
[17] Sidney, J. B., and G. Steiner. Optimal sequencing by modular decomposition: Poly-
nomial algorithms. Oper. Res., 34: 606-612, 1986.
[18] Smith, W. E. Various optimizers for single-stage production. Naval Res. Logist.
Quart., 3: 59-66, 1956.
[19] Valdes, J., R. E. Tarjan, and E. L. Lawler. The recognition of series-parallel digraphs.
SIAM J. Comput., 11: 298-:313, 1982.
Dynamic Basis Partitioning for Network Flows
with Side Constraints
Wonjoon Choi*
Siileyman Tiifek<;i t
Abstract
This paper deals with the network flow modeling of emergency evacuation prob-
lems in densely populated buildings or geographical areas. The underlying opti-
mization model is a dynamic network flow model with side constraints. We propose
a new dynamic basis partitioning procedure for the primal partitioning simplex al-
gorithm. The proposed algorithm is used in solving very large size network flow
problems with many side constraints. Our experimental results show that the pro-
posed dynamic basis partitioning algorithm is up to 36 times more efficient than a
regular basis partitioning algorithm on the problems encountered in solving emer-
gency evacuation problems.
1 Introduction
During the past 10 years, an entirely new body of modeling has begun to develop in emer-
gency evacuation modeling which uses dynamic network flow modeling of these problems.
The dynamic network modeling of the emergency evacuation problem typically involves
representing the underlying transport structure as a network evolving through time, called
dynamic network or time expanded network. The objective is usually to find an evac-
uation pattern through this time expanded network model which empties its content at
the smallest possible time. There may be other objectives also equally important; such
as evacuating maximum number of people within a given time period, ~vacuating some
"Department of Industrial Engineering, University of Ulsan Ulsan, Korea.
tDepartment of Industrial and Systems Engineering, University of Florida, Gainesville, Florida 30332,
USA. This research has been sponsored in part by the National Science Foundation, Grant No. CEE
8215437 and by the Florida Industry and High Technology Council.
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgill et al.
© Springer-Verlag Berlin Heidelberg 1992
162
group of people ahead of the others (such as children and elderly or sick people) or finding
an evacuation pattern that minimizes wondering through the network, to name a few.
For the completeness of the paper, we will briefly introduce network flow problems,
network flow problems with side constraints, dynamic networks and the art of developing
network models of building evacuation problems and area evacuation problems. The
related literature is presented in sections 2, 3 and 4, respectively. Section 5 presents
the process of dynamic basis partitioning for networks with side constraints problem.
Computational results are presented in section 6. We conclude our work in section 7.
2 Pure Network Flows and Network Flows with Side
Constraints
2.1 Pure Network Flow Problems
Consider a directed network G, consisting of a set of nodes V, labeled with integers
1,2, ... , n and a set of directed arcs E each represented by a pair of node numbers (u, v).
An arc (u, v) is said to be incident to nodes u and v and points from node u to node v.
Figure 1 represents a network with 4 nodes and 5 arcs. problem:
Figure 1. An example network (static).
With each node v in the network G we associate an integer number Tv, which represents
the available supply (if Tv > 0) or the needed demand (if Tv < 0) of the items the network
is to transport through its nodes and arcs. All nodes with Tv> 0 are called source nodes
(or supply nodes), and all nodes with Tv < 0 are called sink nodes (or demand nodes).
The nodes with Tv = 0 are called transshipment nodes. With each arc (u, v) in E of G we
associate several numbers. One is the cost Cuv, which represents the unit shipping cost of
163
the item along the arc (u, v), and t uv , the traversal time of the units from node u to node
v along the arc (u,v).
Any network drawing which simply represents the nodes and the arcs of the network
is called a static network. Figure 1 is a static network. Note that no time dimension is
involved in this representation. We will first present network problems associated with
static networks. We then introduce dynamic networks (time-expanded networks) that
incorporate the time dimension to the transportation networks.
The minimal cost network flow problem is described as shipping the necessary amount
of supplies to demand points at minimum cost. Mathematically, we can write this problem
as a linear programming
Minimize L CuvXuv (1)
(u,v)EE
Subject to L XU'U -
L Xwu T"u, U = l, ... ,n (2)
(u,v)EOUT(u) (w,u)EIN(u)
luv :::; Xuv < Suv, (u,v) E E. (3)
Here, IN(u) and OUT(u) represent the set of incoming and outgoing arcs to node u,
respectively. Constraints (2) are called flow conservation equations. They merely insure
that the balance of incoming flow and outgoing flow at node u is equal to T"u. Constraints
in (3) represent minimum and maximum amount of flow that an arc must have in any
feasible solution.
In compact matrix notation the above problem can be represented as
Minimize ex
Subject to Ax T"
1:::; x < s
(4)
(5)
(6)
Here, e is an (1 x m) vector of flow variables associated with xuv's, r is an (n xl) vector
associated with the requirements T" u, A is an (n x m) matrix associated with the coefficients
of the xuv's in (2) with T"ank(A) = n - I, and finally, 1 and s are the (m x 1) vectors
associated with the lower and upper bounds, luv and Suv, respectively. To make A full
rank we add an artificial variable x a to the problem with a corresponding column ek , with
one in position k and zero elsewhere. In what follows we assume that total demand is
equal to total supply in the network. For such a balanced problem, any feasible solution
must have x a = O. Therefore, the cost coefficient of the artificial variable in the objective
function can be set to zero.
Each column ofthe A matrix in (5) contains only two nonzero coefficients: +1 at loca-
tion u and -1 at location v, for the variable Xuv of the arc (u, v). An important property
164
of the A matrix is that any (n x n) nonsingular submatrix of [A, ek ] has determinant of
+ 1 or -1 and can be put into an upper triangular form by appropriate row interchanges.
Moreover, given an (n x n) nonsingular submatrix B of A, the arcs corresponding to the
columns in B form a rooted spanning tree in G. It is this special structure of the matrix
[A, ek ] in (5) that makes network problems solved very efficiently by specialized simplex
algorithms [12]. All calculations involving column update, dual update, basis update and
reduced cost update may be performed by simply using the rooted spanning tree repre-
senting the current basis. For further details on network simplex, we refer our readers to
an excellent text by Kennington and Helgason [12].
2.2 Network Flows with Side Constraints
Sometimes additional restrictions are imposed on the arc flows of a given network flow
problem. For example, a flow in an arc may be restricted to assume a value not more
than a certain fraction of the sum of the flows on some other arcs. Another case may be
that the weighted sum of the flows on a set of arcs being linearly proportionate to the
weighted sum of the flows on another set of arcs. All these added linear constraints are
called side constrains and compactly represented as
Dx~d. (7)
Each inequality in (7) represents one additional side constraint to the regular problem
given by (4) - (6). Thus, the network flow problem with side constraints can be written
as
Minimize ex (8)
Subject to Ax+ ekxa r (9)
Dx+Ix' d (10)
I ~ x < s (11)
x' ~ o. (12)
Here, x' is the vector of slack variables. It is well known (see Kennington and Helgason)
that any basis T to be formed from (9) and (10) of this problem can be cast into the
following form (possibly after rearranging columns)
(13)
165
Here, B is an (n x n) square nonsingular submatrix of [A, ek ] which corresponds to a
rooted spanning tree of G. It is well known that
(14)
where Q = F - DB-lC. Thus, in the execution of the simplex algorithm, any operation
involving B- 1 can be performed very easily on the corresponding tree without explicitly
computing the inverse. It is this property of the basis T that allows one to solve these
network flow problems with side constraints very efficiently.
As a preliminary analysis, let CT = [Cl' C2], 7r = [7rl,7r2] be the partitioning of the
cost vector and the dual vector according to the partitioning dictated by T. Here, we will
obtain the dual variables 7r = cT(Ttl .
Algorithm DUAL { dual variable calculation}
Step 1) Set, = clB-l .
Step 2) Set 7r2 = (C2 _,C)Q-l.
Step 3) Set 7rl = (Cl - 7r2D)B-l =,- 7r2DB-l.
In a similar manner, we can calculate the updated column of an entering variable as
Y = [Ylt,Y2t]t = (T)-la according to the following algorithm:
Algorithm UPDATECOLUMN { updating the entering column}
Step 1) Set, = B-lal.
Step 2) Set Y2 = Q-l(a2 - D,).
Step 3) Set Yl = B-l(al - CY2) = ,- B- l CY2.
We now can give the steps of the primal partitioning simplex method for the network
problem with side constraints (NPWSC) as follows:
Algorithm PPSIMPLEX { primal partitioning simplex method}
Step 1) Given a feasible basis B and its partition, get the dual variables by the algorithm
DUAL.
166
Step 2) Determine a variable candidate to enter the basis by using the reduced cost
(7raj - Cj), where aj and Cj are the column and the cost associated with the variable Xj,
respectively.
IF no candidate exists THEN
stop with an optimum solution.
ENDIF
Step 3) Compute the updated column Yj of the entering variable Xj, by the algorithm
UPDATECOLUMN.
Step 4) Determine the leaving variable by the regular ratio test.
IF there is no leaving variable THEN stop with an unbounded solution
ELSE
update the flow
update the partition of the basis T
update the working basis inverse Q-l.
ENDIF
Go to Step 1.
We refer the reader to the text by Helgason and Kennington [12] for further reading
on the network flows with side constraints problem.
3 Dynamic Networks
Consider the static network G = (V, E) as given in Figure 1. With each arc (u, v) there is
an associated integer number t uv , representing the traversal time on that arc. For a given
time horizon p > °we construct a network Gp = (Vp, Ep), the p-time expanded network
from G as follows:
Vp = {Vi: V E V, i = O,l, ... ,p},
Ep = ((Vi,Wj): (V,W) E E, j = i + t vw , i = 0, .. . ,p, j < p}U{(Vi,Vi+1): V E V, i =
0,1, ... ,p -I}.
The network Gp is also called a dynamic network. Here, Vi is the ith time copy of node
V E V. The arcs (Vi, Vi+l) in Ep are called holdover arcs whose contents indicate the
number of items still remaining in node V at time i. Generally, one associates capacities
with each holdover arc. The arcs (Vi,Wj) in Ep are called the movement arcs, which
represent the movement of commodities from node V to node w. This movement start at
time i at node V and ends at time j = i + tvw at node w.
We can have numerous objective functions associated with a dynamic network flow
167
problem. For example, finding a flow pattern that supplies all the demand points in min-
imum time, maximizing the number of units reaching designated demand points within a
prescribed time T, and minimizing the transportation costs while maximizing the number
of goods shipped within a prescribed time, T, are just to name a few.
o 1 2 3 4 5 6 7
Figure 2. The dynamic network representation (7-time expanded) of Figure 1.
4 Dynamic Network Flow Modeling of Evacuatiort
Problems
In this section we will illustrate the art of dynamic network modeling of evacuation prob-
lems with an example taken from Chalmet et al. [2]. Figure 3 depicts a simple three-story
office building. There is one workplace in floors 3 and 2 connected to their respected hall-
ways through a door, and the hallways end at the stairwells which will channel people
to the lower levels. The stairwells open to the lobby in the first floor. The workplace in
the first floor also opens to the lobby through a door. The flow of people through the
building during an evacuation is also indicated in Figure 3 with arrows. If each com-
ponent of this building is represented by a node ( workplaces, hallways, and stairwells)
and the passageways between these components are represented by directed arcs between
the adjacent components, we obtain the static network representation of this building
as shown in Figure 4. With each node u of the static network, we associate an integer
r u , indicating the number of people present at that component at the beginning of the
evacuation (at time t = 0). We also associate another positive integer hu with each node
u, indicating the static capacity of the node. This is the maximum capacity that the
component associated with that node can accommodate, at any instant of time during
the evacuation. With each arc (u, v) of the static network, we associate an integer number
t uv , indicating the time it takes for units to travel from its beginning node (component)
168
to its ending node (component). With each arc (u, v), we also associate a function Cuv(x),
called the capacity function (a flow rate function) which will regulate the amount of flow
an arc might transmit from its beginning node to its ending node, in one unit of time.
This function, in actuality, is a function of the congestion at an arc. The congestion for
an arc (u,v) is expressed as a function of incoming flow in other arcs incident to node u,
as well as the number of units currently present at node u. This function will be analyzed
more in the following sections.
A Three Story Building
Workplace Hall Stairwell
Floor 3
Workplace Hall Stairwell
Floor 2
Floor 1 Workplace Lobby
Exit
Figure 3. A simple three story building
Finally we associate a node with what is called the exit. This node represents the
termination of the evacuation process. All components of the building with an opening
to outside are connected to this exit node.
In Figure 4, the numbers near each node inside a parenthesis represents its static
capacity, the numbers next to each arc inside a parenthesis represent constant flow rate
that can flow through that arc per unit time ( the function cuv (.) is constant). The
traversal time of each arc is also indicated in Figure 4.
169
The dynamic network representation of the static network of Figure 4 is given in
Figure 5.
WP3 H3
t =1 t=1 SW3
30 (12) (15) (15)
(40) (25)
WP2 H2
t=l t=1 SW2
40
(45) (12) (20) (15)
WPI (25)
t=1
25
(50) (20)
t = 1
Figure 4. The static network representation of the buiding in Figure 3.
Note that each horizontal sequence of consecutive nodes represent the time copies of a
particular node in the static network. Each holdover arc is a horizontal arc connecting two
consecutive time copies of a given node, and each arc of the static network is replicated
diagonally throughout the network. We call these arcs the movement arcs. We finally
add a super destination node D to this time-expanded network where all exit arcs are
connected. This trick is applied merely to be able to represent the models in the sequel
with ease. The destination node D will be the only demand node of the time-expanded
network. All flow reaching to any copy of an exit node will be automatically transferred
to D.
For more information on how to build a dynamic network representation of a building,
we refer readers to Chalmet et al. [2].
Note that we have not yet elaborated on the costs associated with each arc of G and
G p in evacuation modeling. One of the most important goals of emergency evacuation is
to find a flow pattern that, when followed, will evacuate the contents of the network in
the shortest possible time. We will assign some costs to the arcs of G p such that, when
170
the corresponding minimum cost network flow problem is solved on Gp , it will yield an
evacuation pattern which will minimize the total evacuation time.
(40)
30
40
Figure 5. The dynamic network of Figure 4.
Consider the follClwing formulation:
Minimize L x(vi,D).i (15)
(v;,D)EEp
Subject to
= {T"V" if Vi #- D
-T"D, if Vi = D, V Vi E Vp (16)
0:::; x(e) < u(e), VeE Ep , (17)
171
where
() { hu if e = (Vi, Vi+!) E Ep
u e = c(x) if e = (Vi, Wj) E Ep and V f- w.
Here, rD is equal to the sum of all supplies r v , in Gp •
In modeling hurricane evacuation problems r Vi , for each time i copy of a source node
v, is determined by a traditional method which utilizes a cumulative response curve. This
curve dictates how the evacuees will be loaded onto the traffic network in time. This
cumulative curve is an S-shaped curve. It is steep for quick response and flatter for slow
response. The steepness is usually determined by the potential severity and the imminence
of the approaching hurricane.
With this formulation, we can allow nonlinear flow capacities c( x) on the movement
arcs. Also note that in this model all of the arcs except the arcs connecting the exit node
to the supersink node D, have zero costs. Moreover, people may arrive at the network at
times, governed by an externally selected arrival process, such as the cumulative response
curve mentioned before. The exit arcs (Vi, D) in Ep receive increasing cost, representing
the fact that it is more expensive to exit later than in earlier time periods through the
network. This model is called the Turnstile Cost Model [2]. It can be interpreted as
having a turnstile at the exit and charging each evacuge exactly the time they spent in
the network.
The above model with constant arc flow capacities (i.e., u(e) is constant) has been
studied in detail and a user friendly software named EVACNET+ was developed by Kisko
and Francis [13] which distributed many copies world-wide. The model and the associated
software EVACNET + has two principal limitations. First, as most analytical models do,
it lacks the behavioral aspects of the evacuation phenomenon. Secondly, the passageway
capacities are assumed to be constant implying that there is no congestion factor through
the exits and passageways (halls, corridors or roads) due to the density of the evacuees
at the component of the facility adjacent to the exit or the passageway.
There is substantial evidence that passageways, when modeled as arcs, have flow
capacities which are functions of the flows on the adjacent components as well as the
density in the component itself. For example, Fruin [8, 7] provides actual experimental
results that on a one-way pedestrian walkway, the walkway flow measured in people per
minute divided by the width of the walkway more than quadruples as the density, the
number of square feet per person decreases from 50 to about 6. He also suggests that the
flow decreases rapidly to zero as the density (jt 2 /person) approaches zero. One of the
conclusions of his work is that pedestrians can adjust their interdistances and by doing
so affect the flow rate.
172
Similarly, in highway traffic flow, it is well known that traversal times change dramat-
ically, and thus affecting the flow rates, as the density of vehicles increase on a roadway.
For further analysis on traffic flows we refer readers to the Highway Capacity Manual by
the Transportation Research Board of the National Research Council [10].
For a more realistic model, the capacity of a movement arc (Vi, Wj) in Ep must be a
function of the density of people (people/ ft 2 ) at the component V at time i. In dynamic
networks associated with buildings or roadways, since the area of a component is fixed,
these capacities can be expressed as a function of the number of evacuees present at that
component at time i. If we let x(Vp, Vi) represent all the incoming flow to component
V at time i, then the capacity of an exit arc e = (Vi, Wj) from component V to com-
ponent W is given by ce(x(Vp, Vi)) for some function c e (.). Here, x(Vp, Vi) is defined by
I:(Zk>Vi)E(Vp,Vi) X(Zk' Vi)'
We can now present a turnstile cost model with flow dependent arc capacities as
follows.
Minimize L x(vi,D).i (18)
(vi,D)EEp
Subject to
XCV' w·)-" J
= {rV" if Vi f= D
-rD, if Vi = D, V Vi E Vp, (19)
x(e) < Ce(X(Vp,Vi)), V e = (Vi,Z) E Ep ,(20)
0:$ x(e) < h(e), Ve = (Vi,Vi+1) E Ep. (21)
In what follows, we refer to C e as a flow dependent arc capacity function.
The model above is studied by Choi [3], Choi, Hamacher and Tiifek~i [5], as well as
Choi, Francis, Hamacher and Tiifek~i [4]. The emphasis is given on the linearization of
the capacity function ce(x(Vp, Vi)). They use the method of piecewise linearization of the
capacity functions. This yields the formulation:
Minimize L x(v;,D).i
(vi,B)EE p
Subject to
X(Vi,Wj) -
(Vi ,Wj )EOUT( Vi)
O:$x(e) <
{ r Vi , if Vi f= D
-rD, if Vi = D, V Vi E Vp,
O'e.x(Vp, Vi) + (3e, VeE E p ,
(22)
(23)
(24)
(25)
173
In matrix representation, we can write the problem as
Minimize ex
Subject to
Ax + eDXa = r,
xe < O:e(Vp, ir( e)) + (3e, VeE F,
Xe < he, VeE Ep\F
Xe > 0, VeE Ep
x a O.
(26)
(27)
(28)
(29)
(30)
(31)
Here, O:e and (3e are constants satisfying 0 :::; O:e :::; 1, (3e ;::: 0, e = (ir( e), toe e)) E
F, and F is a subset of the arc set in Ep with flow dependent capacities associated
with the movement arcs. We will call this problem NPWSC for network problem with
side constraints. Note that constraints (27) correspond to constraints (10) in the earlier
formulation of the NPWSC in (8)-(11). The artificial variable x a is added to (27) to make
the matrix [A, eD ] full row rank. Note that the column eD corresponding to the artificial
column is a unit vector with 1 in the location corresponding to the super sink node D.
5 Dynamic Basis Partitioning for NPWSC
Although this problem is a linear programming problem and can be solved as such, its
special structure allows us to exploit the network structure inherent in the problem. One
possible approach is to use the algorithms designed for the network problems with side
constraints. However, the sheer magnitude of the side constraints in (28) makes even this
approach prohibitively time consuming. Choi [3], and Choi and Tiifek«i [6] have developed
the idea of dynamic working basis where the dimension of the F matrix in (13) changes
from iteration to iteration. This in turn affects the dimension of the working basis Q in
(14). It is usually this part of the basis matrix in NPWSC problems that requires full
accounting and the update of the inverse. Therefore, its size contributes significantly to
the solution time of the problem. In regular basis partitioning for NPWSC, the upper left
corner of the basis shown as B in (13) corresponds to a spanning tree in the underlying
network with the artificial variable xD, as its root arc. It is also well known that, perhaps
after rearranging rows and columns , B as given can be put into upper triangular form.
Therefore, all operations involving B-1 may be performed on the spanning tree as in
primal simplex for regular network flow problems. Therefore, an algorithm for a NPWSC
never maintains B-1 but stores the corresponding spanning tree. On the other hand, the
working basis and its inverse Q-1 must be kept and updated at each iteration.
174
Given a T matrix as in (13), let II and 12 be the number of rows in Band D,
respectively. Similarly, let J1 and J2 be the columns in Band C, respectively. In standard
primal partitioning for NPWSC II = J 1 = n = the number of nodes in the network. The
idea of the dynamic working basis can be expressed as follows: We would like to redefine
the partition of the T matrix at each iteration. In doing so, we liberally allow more
than n rows and columns to be allocated into the B matrix such that the resultant B
matrix will not lose a significant portion of its important properties. This is rather a loose
statement. It can be broadly interpreted as any possible relaxations of upper triangular
and unimodular property of B, as long as it leads into solving the NPWSC problem more
efficiently. By efficiency we mean both the time and space efficiency.
In this paper, we have relaxed the upper triangular requirement of the B matrix
associated with (27) and (28), with what we term as Block Upper Triangular (BUT)
property. A typical structure of the BUT matrix is depicted in Figure 6.
J1
0 D 11
-
0
~
C F
Figure 6. A BUT matrix.
As can be seen in this figure, if we ignore the blocks, the only nonzero entries are on
or above the main diagonal of B . Moreover, each of the blocks have the following simple
structure as given in Figure 7. We call each block a primary diagonal block. The inverse
175
of such a block is also given in Figure 7. At each iteration, we maintain a partition of a
basis T such that the submatrix B in T always satisfies the following properties:
1) B is a block upper triangular matrix,
2) each block of B is a nonsingular primary diagonal matrix,
3) if a row associated with a capacity constraint for an arc Xe is placed in II in the ph
position, then the kth basic variable is either Xe or the slack variable, Se associated with
that constraint.
We will call this restriction the diagonal restriction.
We will now describe the procedure of primal partitioning simplex using dynamic
working basis. The algorithm starts as in regular NPWSC algorithm, with a basic feasible
solution and a Partition of the basis T. The algorithmic steps are the same as in NPWSC
algorithm. The differences are in the computation of the dual variables, column updating
and updating the working basis inverse Q-l. For example, in Step 1 of the algorithm
DUAL, in solving 71"1 = C1B-1 or 71"lB = C1 we use the back substitution until we come to
a block. The entries of 71"1 associated with the block are computed simultaneously using
the corresponding primary diagonal matrix of that block. In a similar manner, to solve a
system Y1 = B-1a1 or BY1 = aI, we use forward substitution until we come to a block.
The entries of Y1 corresponding to the block are computed simultaneously by using the
primary diagonal matrix associated with that block.
We note here, that with proper ordrering of the basic arcs in B, the computation of 71"1
may simply be performed by traversing an acyclic network constructed by the basic arcs
in a certain order. Moreover, after each iteration, a new acyclic network and its associated
ordering of the basic arcs can be maintained similar to network simplex implementations.
The order of the computations are dictated by a thread function, that visits the nodes of
the underlying acyclic basis graph, in a certain order. The updating of this order (thread)
js also efficiently done by an appropriate concatenation procedure. We refer our readers
to Choi [3] for further details.
In determining how to handle entering and leaving variables, we provide a heuristic
procedure to check if the working basis size needs to be reduced by one, increased by
one, or be kept the same in each iteration. Let a row i in T be called a key row if
i :::; II. Similarly, let a column (correspondingly, the varable Xj) j in T be be called a
key column (key variable) if j > J1 • The heuristic procedure works as follows. If the
leaving variable is a key variable then try to replace its column with another nonkey basic
variable column in T. If successful, the size of Q stays the same, and if unsuccessful,
then the key row, whose defining variable is the leaving variable, is declared nonkey and
placed at the bottom of T. This implies the possibility of increase in size of Q if the next
176
test is also unsuccessful. The second part of the test involves the entering variable. An
attempt is made to find a nonkey row such that, together with the entering column they
may be declared a key column and a key row. The resulting augmented B matrix must
still maintain the BUT property. If successful, the size of Q is reduced by one else it
increases by one. Of course, if the leaving variable is not a key variable, then we simply
drop it and check if the entering variable column together with a nonkey row in T can
be declared key variable and key row, simultaneously. If successful, the size of Q gets
reduced by one. If not, the size remains the same.
-1 1 1 1
-at 1 0 0
L= -a2 0 1 0
0 1 0
0
-ak 0 0 0 1
-1 1 1 1 1
-al 1'1 al al
L-t =(.!:.) -a2 a2 1'2 a2
I'
-ak ak ak ak I'k
I' = 2:~1 ai, I'i = I' + ai, i = 1,2, ... ,k.
Figure 7. A primary diagonal block and its inverse.
In updating the working basis Q-l, we note that Q-l resides in the lower right corner
of T-l in (14). Let Told and Qold denote the current basis matrix and the working basis
matrix, respectively. Let Tnew and Qnew denote the new basis matrix and the new working
basis matrix after the pivot, respectively.
For a given constant k, let M be a (k X k) matrix and q = (qt, q2, ... , qk)t be a (k X 1)
vector. Consider the following operations:
i) replace the rth row of M by the rth row of M divided by qr, and
ii) for each i = 1,2, ... , k except i = r, in M, replace the ith row of M by the sum of the
177
ith row and the new (replaced) rth row multiplied by -qi .
Here, qr is assumed to be nonzero. We will call this process, pivoting M by q with qr as
the pivot element.
Consider now a T;;i~ as given in Figure 8. Let y be the updated column of the
entering variable. Suppose the ith column of Told is the leaving column. Then T;;-;w may
be obtained as follows: "Pivot T;;i~ by y with Yi as the pivot element" and rearrange (if
necessary) the rows and columns of the resultant matrix. The desired Q-l will reside in
the lower right corner of T;;;;w'
yi
Sii
y
Cj
The following Lemma provides an insight into this updating process.
Lemma 1
Let T be a basis matrix and Q be the working basis of T. Then Q is invariant to any
rearrangements within the columns in Jl, and rows in II.
Proof: Let T" be the resultant matrix obtained from T by interchanging columns in J 1
and rows in II. We can write
T" = [P 01[B C1[R 01= [PBR PC 1o .I D F 0 I DR F
where P and R are some permutation matrices and B, C, D and F are the partition of
T as given in (13). Let Q" be the working basis corresponding to T". Then
178
Therefore, only the necessary portion of T;;i~ needs to be updated to get Q;;w' The fol-
lowing procedure updates the Q;;w(8ee Figures 7 and 8 for the terminology).
Procedure Q-l update { updating working basis inverse}
INPUT: Sii, ri, Ci, y, Q;;i~, entering variable, leaving variable and their positions in
Told-
BEGIN
WHILE leaving column is in a key column DO
Check if a nonkey column in T can be declared key to maintain the BUT property. If
successful, proceed with handling the entering column. If unsuccessful, declare the key
row of Told corresponding to the leaving variable as nonkey and sned the row to the end
of Told. In either case, check if the entering column together with one of the current
nonkey rows be declared key to reduce the size of Q-l and preserve the BUT property
of the Bnew matrix. These two tests yield four possible outcomes. We will name them as
(8S), (SF), (FS), and (FF) to indicate success(8) or failure(F) of the attempt to maintain
BUT structure with respect to leaving variable as the first letter and with respect to the
entering variable as the second letter inside the parentheses.
IF the outcome is (FF) THEN pivot the matrix
[ Q;;i~ Ci ] by [Yy2,.]
ri Sii
with Yi as the pivot element and take the complete matrix as Q;;-;w'
ELSEIF outcome is (SF) THEN
Let us assume that a non key column JET is declared key due to the success in the first
attempt. Then pivot the matrix
with Yi as the pivot element and delete the ph row from the resultant matrix.
ELSEIF outcome is (FS) THEN
Let us assume that a nonkey row JET is declared key, due to the success in the second
attempt. Then pivot the matrix
[ Q;;i~ Ci] by [Yy2,.]
ri Sii
179
with Yi as the pivot element and then delete the ph column and the last row and take the
resultant matrix as Q;;;w.
ELSEIF outcome is (SS) THEN
Let us assume that a nonkey column JET is declared key due to a success in the first
attempt, and a nonkey row k E T is declared key in T due to the success in the second
attempt. Then pivot the matrix
with Yi as the pivot element and delete the ph column, kth and the last row from the
resultant matrix. Take the remaining matrix as Q;;;w.
ENDIF
ENDWHILE { the leaving variable is a nonkey variable}
In this case, we have two sub cases only with respect to entering variable. We will either
be able to declare the entering column together with a nonkey row trom T key column
and key row, respectively, or else we will declare the entering column a nonkey column.
We will represent the former outcome with (S) and the latter with (F). In the case of (S)
let this entering column and the corresponding row be moved into position j in T.
IF the outcome is (S) THEN
pivot Q;;j~ by y as Yi as the pivot element and delete the ith row and the ph column from
the resultant matrix to obtain Q;;;w.
ELSE { the outcome is F}
pivot Q;;j~ by y with Yi as the pivot element to get Q;;;w.
ENDIF
END.
We now discuss how to get ril Ci and Sii. We know from straight linear algebra that
ri = _(B- I CQ-l )i. = _(B-1 h.CQ-l
ci -(Q- I DB- l ).i = -(Q-l).iDB-l
'sii = (B- 1 + B- I CQ-IDB- l )ii
= (B-l )ii + (B-1 h.CQ-l D(B-l ).i
= (B-l )ii + ri D (B- l ).i .
Here (.)i. and (.).i represent the ith row and ith column, respectively. To get (B- l )i., we
use the algorithm DUAL with 71"2 = 0 and Cl = eil where ei is the unit vector with 1
in position i. Similarly, to get (B- l ).i, we use the algorithm UPDATECOLUMN with
Y2 = 0 and al = ei.
180
We finally discuss how to update 11"2. Let Yo be the reduced cost associated with the
entering variable. We augment Q;;i~ and y by 11"2 and Yo, respectively. We pivot
with Yi as the pivot element.
6 Computational Experiments
We present some computational experience with our algorithm which performs 5 to 36
times faster in the test problems considered. Table 1 presents those problems where flow
dependent arc capacities are in the form given in (28).
no. of no. of no. of capacity ct' s
Problem nodes arcs constraints range
1 293 542 266 0.Ql-0.40
2 465 1060 624 0.2
3 575 1113 539 0.01-0.40
4 1028 2230 1244 0.2
5 2134 4287 2341 0.01-0.45
6 110 434 377 0.10-0.40
7 250 496 484 0.01-0.45
8 500 1994 972 0.Ql-0.40
9 750 1494 733 0.01-0.45
10 1000 1991 633 0.01-0.35
Table 1. Test problems for PART1 and CLASSICI.
Table 2 presents some problems where each nonlinear flow dependent arc capacity
function is linearized by using two-segment piecewise linear function. The first five prob-
lems in each table represent dynamic network representation of some real and imaginary
building evacuation problems. Although the first five networks are the same in both ta-
bles, the values of ct e and f3e as well as the number of flow dependent capacity constraints
are different. The networks corresponding to the last five experiments in Table 1 and
Table 2 are generated randomly. We have also generated the values of ct e randomly from
the range prescribed by the corresponding column. The computer codes were all written
181
at the University of Florida in standard FORTRAN 77 for an in-core implementation
and were tested on the University of Florida IBM 3090-200 system using a FORTRAN
VS compiler with the optimization level OPT(2). All the numerical computations were
carried out in single precision and the tolerances were set at 0.001. We have tested our
algorithms PARTl, for one linear flow dependent arc capacity constraint per arc, and
PART2, for two linear flow dependent arc capacity constraints per arc, for each flow
dependent capacity restricted arc. These two algorithms are tested against the classic
basis partitioning algorithm for the network with side constraints problem, CLASSICI
and CLASSIC2, respectively.
We note here that CLASSICI needs IFI x IFI array for the working basis inverse while
CLASSIC2 requires 12FI x 12FI = 41FI x IFI array to store the working basis Q-l. Here,
IFI denotes the cardinality of the set F. We employed the Big-M method of the simplex
procedure for artificial variables. An advanced starting basis procedure is employed to
get the initial spanning tree in all algorithms. We also adopted the most positive reduced
cost rule of Dantzig for the selection of the entering variable in all algorithms.
The CPU times (in seconds) are given in Table 3 and Table 4. As one can see in these
tables, further break down on the percent of total computational times are reported inside
the parentheses indicating time spent computing dual variables, time spent in updating
columns, updating the partitions and updating the working basis inverse. It is clear that
there is a tradeoff between the time it takes in getting the dual variables and updated
columns and time it takes in updating the working basis inverse.
no. of no. of no. of capacity (X's
Problem nodes arcs constraints range
11 293 542 532 0.01-0.40
12 465 1060 1248 0.2
13 575 1113 1078 0.01-0.40
14 1028 2230 2488 0.2
15 2134 4287 4682 0.01-0.45
16 110 434 424 0.10-0.40
17 250 496 484 0.01-0.45
18 500 1994 1944 0.01-0.40
19 750 1494 1466 0.01-0.45
20 1000 1991 1326 0.01-0.35
Table 2. Test problems for PART2 and CLASSIC2.
182
Classical networks with side constraints spent up to 92% of the CPU time on updating
the Q-l and 10-20% on updating the duals and the entering column. In contrast, our
algorithm spent less than 1% of the total CPU time on updating the Q-l and 50-60% on
updating the duals and the entering column. Since updating the working basis inverse
is the bulk of the computational burden, our algorithms have outperformed the classical
counterparts. The ratios of CPU times is from 5 to 36 times in favor of our dynamic basis
partitioning algorithms.
. PART1 CLASSICI
Inax. SIze (estimated)
of working % total total %
Problem basis (a,b,c,d) CPU time CPU time (a, b, c, d)
1 1 (33.0,20.0,19.0,0.2 ) 0.58 2.30 (8.5,16.0,15.0,65.0 )
2 3 (32.0,34.0,20.0,0.2) 3.04 68.46* (1. 7,5.1,1.8,90_5)
3 1 (30.0,32.6,20.1,0.1 ) 7.24 42.57* (1.8,7.2,0.6,89.2)
4 14 (31.0,34.9,21.1 ,0.2) 9.04 327.37* (1.1 ,5.1 ,0.8,91.6)
5 4 (34.1,40.9,16.0,0.05) 31.60 N/A N/A
6 3 (36.0,24.5,20.0,0.5) 0.46 12.00 (3.1,10.5,1.9,72.4 )
7 5 (29.4,31.2,24.1,0.3) 0.13 1.20 (2.8,7.2,2.2,86.3)
8 3 (35.0,29.0,30.0,0.1 ) 10.30 340.10* (1.4,18.0,1.5,77.0)
9 20 (28.7,35.2,24.5,0.5) 14.25 357.38* ( 1.3,4.6,0.9,92.2)
10 5 (28.1,33.1,29.3,0.09) 19.43 201.18* (2.5,6.7,1.5,88.1)
• II " ." "Note. a corresponds to gettIng dual varIables and the entenng vanable, b corresponds to getting the updated column
and the leaving variable; "e" corresponds to updating the partitionj "d" corresponds to updating the working basis inverse;
II." means an estimated CPU time; uN/A" means we cannot nm CLASSICl due to the excess of the avilable storage; "CPU
time" is measured in seconds.
Table 3. Computational results for PARTI and CLASSICI.
We also note here that some of the problems solved by our method cannot be solved
by the traditional network with side constraints algorithm. For example, problems 14,
15, 18, and 19 were too large to be handled by the classical NPWSC algorithm due to
memory requirements in storing the working basis inverse Q-l. For some problems, we
had to project the expected CPU time for the classical approach. This was due to the
set time limit on the CPU time we imposed on each problem. Since algorithmic steps are
identical in both approaches, our projections are very accurate. Finally, we conjecture
that if different pricing strategies were used (such as candidate queue list) our algorithm
will perform better since the percentage of the CPU time in PART 1 and PART2 are much
183
higher for pricing as compared to the percentages in CLASSICI and CLASSIC2. We also
claim that if we adopt higher precision arithmetic in both approaches, this will play in
favour of our algorithms as well.
PAIlT2 CLASSIC2
max. size (estimated)
of working % total total %
Problem basis (a, b, c, d) CPU time CPU time (a, b, c, d)
11 3 (28.0,43.0,13.0,0.1 ) 0.99 13.50 (2.6,8.4,1.6,86.0)
12 22 (40.9,42.5,9.4,0.3) 9.78 101.61 ' (3.1,14.4,2.9,78.7)
13 33 (12.8,51.2,4.3,0.9) 18.94 348.88' (1.5,11.8,0.5,85.2)
14 23 (28.5,48.8,12.0,0.1 ) 38.96 NjA NjA
15 27 (31.0,52.0,8.5,0.06) 69.42 NjA NjA
16 2 (38.6,37.0,18.0,0.1 ) 1.19 7.98 (4.7,16.5,2.6,74.8)
17 11 (26.8,47.5,14.6,0.2) 2.76 65.63" (1.4,.).8,7.9,91.5 )
18 6 (40.0,42.0,15.0,0.1 ) 22.00 NjA NjA
19 21 (26.0,49.4,20.0,0.7) 29.14 NjA NjA
20 17 (27.6,51.4,11.8,0.3) 40.60 485.7.')" (2.5,10.2,1.1 ,80.5)
" " " 11Note: a corresponds to gettIng dual varIables and the entenng varIable; b corresponds to gettIng the updated coluITUl
and the leaving variable; "e" corresponds to updating the partition; lid" corresponds to updating the working basis inverse;
"." means an estimated CPU time; "N! A" means we cannot run CLASSIC2 due to the excess of the avilable storage; "CPU
time" is measured in seconds.
Table 4. Computational results for PART2 and CLASSIC2.
7 Conclusion
We have proposed a dynamic basis partitioning simplex algorithm for solving network
flows with side constraints problems which emerge in the modeling of emergency evac-
uation problems. The side constraints in these problems stem from the fact that the
maximum flow rate through a component depends upon the congestion of traffic on that
component.
We have shown that the proposed dynamic basis partitioning is very effective and
allows us to solve these problems 5 to 36 times faster than the classical basis partitioning
approach. Moreover, we can handle much larger number of side constraints without
184
exceeding the CPU storage limitations for the in-core implementations. We conjecture
that the similar dynamic basis partitioning procedures may bedeveloped to solve other
NPWSC problems efficiently. However, in those implementations, the blocks that can
be allowed into the B matrix of the basis T will be different from the primary diagonal
matrices used in emergency evacuation models.
Further developments in solving other NPWSC problems such as the equal flow prob-
lem and other special types of side constraints are under way and will be reported in a
subsequent paper.
References
[1] Aronson, J. E. A survey of dynamic network flows. Annals of O.R., 20: 1-66, 1989.
[2] Chalmet, 1. G., R. 1. Francis, and P. B. Saunders. Network models for building
evacuation. Management Science, 28(1): 86-105, 1982.
[3] Choi, W. Network flow models of building evacuation problems with flow dependent
arc capacities. Unpublished Ph.D. dissertation, University of Florida, April 1987.
[4] Choi, W., R. 1. Francis, H. W. Hamacher, and S. Tiifekt;i. Network models of build-
ing evacuation problems with flow dependent exit capacities. Opreational Research
'84, Elsevier Publishers, North Holland, 1047-1059, 1984.
[5] Choi, W., H. W. Hamacher, and S. Tiifekt;i. Modeling of building evacuation prob-
lems by network flows with side constraints. European Journal of Operations Re-
search, 35(1): 98-110,1988.
[6] Choi, W., and S. Tiifekt;i. An algorithm for a network flow problem with special
side constraints. TIMS/ORSA Joint National Conference, Los Angeles, California,
April 14-16, 1986.
[7] Fruin, J. J. Design for Pedestrians, A Level-of-Service Concept. unpublished Ph.D.
dissertation, The Poly technique Institute of Brooklyn, June 1970.
[8] Fruin, J. J. Pedestrian Planning and Design. Metropolitan Association of Urban
Designers and Environmental Planners, New York, 1971.
[9] Hamacher, H. W., and S. Tiifekt;i. On the use of lexicographical min cost flows in
evacuation modeling. Naval Research Logistics, 34: 487-503, 1987.
[10] Highway Capacity Manual. Transportation Research Board, National Research
Council, Washington, D.C., 1985.
[11] Jarvis, J. J. and H. D. Ratliff. Some equivalent objectives for dynamic network flow
problems. Management Science, 28: 1982.
[12] Kennington, J. 1., and R. V. Helgason. Algorithms for Network Programming.
Wiley-Interscience, New York, 1980.
[13] Kisko, T. M., and R. L. Francis. EVACNET+: A computer program to determine
optimal building evacuation plans. Fire Safety Journal, 9(1,2), 1985.
185
[14] Kisko, T., and S. Tiifek<;i. Design of a regional evacuation decision support system.
Proceedings of the 1991 Western Simu lation Multiconference, Society of Computer
Simulation, Anaheim, California, 48-53, 23-25 January1991.
[15] Tiifek<;i, S., and T. Kisko. Regional evacuation modeling system (REMS): A decision
support system for emergency area evacuations, Proceedings of the Computers and
Industrial Engineering Conference, Orlando, Florida, 1991.
Combinatorial Optimization Models Motivated by
Robotic Assembly Problems
Horst W. Hamacher *
Abstract
In this paper some results are summarized which were motivated by modeling
the assembly of printed circuit boards using robots. First we consider an iterative
traveling salesman - location model for sequencing the insertion points and
locating the storage bins. The restricted location problem used in this model
is generalized and some algorithms for solving this general problem are described.
Finally, we discuss max-linear combinatorial optimization problems as models
for multi objective problems.
Emphasis will be on results obtained by the author in cooperation with various
collaborators. It is not intended to give a complete overview on the literature in
this area.
1 Printed Circuit Board Assembly and the Robot
Tour Problem
We consider printed circuit boards (PCB) represented by a rectangle R = [0, a] x [0, b]
in which M parts m E M := {I, ... , M} have to be inserted at fixed insertion points
Pm = (P m llP m 2)' respectively, by a robot arm. The robot arm may move simultaneously
or sequentially in Xl and X2 direction. In the former case the distance d(X, Y) between
two points X = (Xl, X2) and Y = (Yl, Y2) is given by the Chebyshev distance
·Department of Mathematics and Center for Industrial Mathematics, University of Kaiserslautern,
Germany
Partially supported by NATO, Research Grant RG 85/0240 and grants of the Kultusministerium and the
Ministerium fiir Wirtschaft und Verkehr, Rheinland-Pfalz, Germany.
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. AkgUl et al.
© Springer-Verlag Berlin Heidelberg 1992
188
whereas in the latter case the rectilinear distance
is used. It is assumed that the time for the robot arm to move between X and Y is
proportional to the distance.
Each of the parts belongs to one of N part types n EN:= {I, ... ,N}. Ift(m) is the
type of part m, we denote Mn := {m EM: t(m) = n}.
Different part types are stored in bins where F is the feasible region in which the bins
can be placed. In most situations F will be a non-convex set, for instance the Euclidean
plane without the interior of the rectangle R, the union of the two half spaces above and
below R, etc. Xn E F is the (unknown !) location of the bin holding all parts of type
n.
The goal of the model is to identify an insertion sequence SEQ := (m(l), ... , m(M))
of the parts m E M and a set LOC := (Xl, ... , XN) of locations for the bins such that
the length of the robot tour, i.e., the overall distance traveled by the robot arm, is
minimized. (And hence the throughput of PCB's is maximized.) We call this problem
subsequently the robot tour problem.
2 A Solution Approach Based on Iterative Solutions
of TSP and Restricted Location Problems
2.1 The Robot Tour Procedure
In this section we will show that the robot tour problem can be formulated as an iterative
solution of a traveling salesman problem and a restricted facility location problem. This
approach has been proposed in Foulds and Hamacher [1991].
For a given insertion sequence SEQ = (m(l), ... ,m(M)) and a location set LOC =
(Xl, ... ,XN) we can compute the length of the robot tour as follows.
[(LOC, SEQ) = d(Xt(m(l)) , Pm(l))
+[d(Pm(I), X t(m(2))) + d(X t(m(2)), P m (2))]
+ ...
+[d(Pm(M-l) , Xt(m(M))) + d(Xt(m(M)) , Pm(M))]
+d(Pm(M), Xt(m(l)))
We next analyze the situation in which we want to find an optimal SEQ for given
LOC. If we define
(2.1)
189
then the length of a robot tour can be written as
Z(LOC, SEQ) = L C m ,3ucc(m) + CONST
mEM
where succ(m) is the successor of m in SEQ and where CONST is a constant, inde-
pendent of the chosen insertion sequence. Thus, an insertion sequence SEQ minimizing
the length of the robot tour is given by an optimal solution of an asymmetric traveling
salesman problem (TSP) in the complete graph J(M with costC;j defined by (2.1). This
approach was developed in Francis et al. [1989].
Next we keep the insertion sequence SEQ fixed and analyze the dependency of the
robot tour length from LOC. For each bin we distinguish two types of movements, the
movement into the PCB from Xn and the movement out of the PCB to X n. The
corresponding distances covered by the robot are
(2.2) IN(Xn):= L d(Xn, Pm) , and
mEM"
(2.3) OUT(Xn):= L d(Ppred(m),Xn) ,
mEMn
respectively, where pred(m) is the predecessor of m in SEQ. An optimal set LOC =
(Xt, ... , Xn) is an optimal solution of the restricted facility location problem (R-
FLP)
(2.4)
The robot tour problem can be ta.ckled by iteratively solving TSP and R-FLP.
Procedure ROBOT TOUR (RT) (Foulds and Hamacher [1991])
START: t:= 1
(1) Bin Location - Initialization
Solve the R-FLP with objective function given by (2.2). Output the optimal solution
LOCo = (Xf, ... ,XRr)
(2) Sequencing Problem
Solve the TSP in the complete graph (I(M, c) where the weights Cij are defined by
(2.1) with respect to Xn E LOCo.
Output traveling salesman tour Tt.
If Tt = T t - 1 then STOP.
190
(3) Bin Location Problem
Solve R-FLP (2.4) and output optimal solution LOC t = (Xf, ... ,Xfv).
If LOC t = LOCt-! then STOP.
Else, set t := t + 1 and go to (2).
Procedure RT contains as subproblem the solution of an asymmetric TSP. Since TSP is
NP-hard, Step (2) will in general be replaced by a heuristic (see Lawler, Lenstra, Rinnooy
Kan, and Shmoys [1986]). It should be noted that even if the TSP in Step (2) is solved
to optimality, the Procedure RT may terminate with a local optimum such that suitable
strategies may have to be applied to leave the local optima.
In the following subsections of Section 2 we will streamline Procedure RT for some
special cases of the robot tour problem.
2.2 Finite Number of Choices for Bin Locations
The assumption that the bins En can be placed in some region F ~ E2 with the only
restriction that F is outside of the area taken by the PCB is suitable if we design the
robot assembly system. In the analysis of an existing system one generally considers a
finite set F = {Xl, . .. , XL} of possible sites in which the bins can be placed.
Let SEQ = (m(l), ... ,m(M)) be a given insertion sequence. If F = {XI, ... ,XL}
then R-FLP in Step (1) and Step (3) of Procedure RT is equivalent to a minimal cost
assignment problem in the complete bipartite graph GN,L with weights
(2.5) W nl := L d(Xz, Pm) , and
mEMn
(2.6) W nl := L (d(Ppred(m), XI) + d(X I, Pm))
mEMn
respectively, for n E Nand 1 E {I, ... , L}. (As usual in assignment problems one may
assume without los of generality that L = N.) The optimal assignment y E {D,l}N
assigns to the bin holding the parts of type n an optimal location Xn chosen from the
finite set {Xl,"" XL}. In (2.5) only the inward movement of the robot arm is considered.
This approach is considered in Drezner and Nof [1984] for the special case where each part
has its own bin. Assembly problems with this assumption are discussed in more detail
in the next subsection. In (2.6) additionally the outward movement with respect to the
given sequence SEQ = (m(l), ... , m(M)) is taken into account.
The finiteness of the set of possible sites for bin locations has no effect on the solution
of Step (2) in Procedure RT. We still have to solve a TSP with respect to Cij defined by
(2.1 ).
191
2.3 Special Algorithms if Each Part has its Own Bin
In this section we consider the situation that each part has its own bin. Thus we have
M = N bins Bm. The results of this subsection are based on an approach by Francis,
Hamacher, Lee, and Yeralan [1989].
In this situation (2.1) can be improved by choosing an optimal Xj for all i,j E M, i.e.
(2.7)
Notice that the computation of C;j in (2.7) corresponds to the solutioQof a restricted
single-facility location problems with respect to only two existing facilities Pi and Pj.
We will show next that under certain assumptions on the assembly environement a very
simple procedure will find the optimum location for X j • This procedure also serves as
motivation for the algorithm for general I-facility R-FLP which will be introduced in
Section 3.
We assume that the robot allows sequential movement in XI- and X2- direction, i.e., the
distance is modeled by the rectilinear distance d(X, Y) = It(X, Y). The feasible region F
for placing the bins is the whole Euclidean plane with the exception of the interior of R.
We analyze the objective function f(X j ) = d(Pi , Xj) + d(Xj, Pj) for points Xj on the left
side of R (SideLeft), the side above R (SideAbove), on the right side of R (SideRight), and
below R (SideBelow). Obviously, the minima on the respective sides are always attained
between the projection points of Pi and Pj to the sides, respectively, i.e.,
argmin {f(X) : X E SideLeft} = {(O, X2) : Pi 2 ~ X2 ~ Ph},
argmin {f(X) : X E SideAbove} = {(Xl, b) : Pi , ~ Xl ~ pj,},
argmin {f(X) : X E SideRight} = {(a,x2) : Pi 2 ~ X2 ~ Ph}, and
argmin {f(X) : X E SideBelow} = {(X}, 0) : Pi l ~ Xl:::; Pill,
where we assume without loss of generality that Pil ~ pj1 and Pi 2 :::; Ph (see Figure 1).
Hence, it is sufficient to check at most 8 projection points of Piand Pj to the four sides
of R, choose the best of these points as optimal location Xj, and set Cij = f(X j ). Notice
that the projection points can be obtained by intersecting the boundary of R with the
horizontal and verticai lines through Pi and Pj. This idea will be generalized in Section
3 to general restricted location problems.
The TSP with respect to weights C;j defined by (2.7) simultaneously considers all bin
location decisions and insertion sequence decision. Therefore, the RT procedure performs
only a single iteration and we obtain the following result.
192
b~----~:~------~~_~----------------~
oPi
... .... a
Figure 1: Rectangle R = [0, a] X [0, b] with two insertion points Pi and Pj and eight
projection points on the sides of R. One of the eight points is an optimal solution of the
restricted single facility location problem [2.7]
Theorem 2.1 (Francis et al. [1989]) The optimal traveling salesman tour with respect
to weights computed by (2.7) yields an optimal solution of the robot tour problem.
The following heuristic can be used to solve the TSP with respect to weights Cij defined
by (2.7).
Procedure - Clock Heuristic (Francis et al. [1989]):
(1) Project each Pm to the closest side of R.
(2) Define the sequence SEQclock by following the projected points in a clockwise fashion
around the PCB.
Using (2.7) with i = j, the distance from Pm to its projected point is e;;j2. Therefore,
L:= 2:: C mm is a lower bound, and
mEM
U := L + (2a + 2b) is an upper bound
for the robot tour defined by SEQc/ock. Since U - L = (2a + 2b) is a constant which
is independent of M, and the length of the optimal robot tour is strictly increasing, the
clock heuristic is asympoticaUy optimal for uniformly distributed insertion points.
193
2.4 A Polynomially Solvable TSP
For the special case where F is the side below the printed circuit board, Ball and Magazine
[1988] have formulated the insertion sequencing problem as a graph extension problem.
In their model one considers the insertion points and bin location points as nodes of a
network. The movement from the bins into the PCB are fixed and are therefore repre-
sented by arcs connecting the bin location nodes to the corresponding insertion nodes.
The problem of finding an optimal sequence is in this context interpreted as the problem
of finding an optimal extension of the graph such that it becomes Eulerian while all in-
sertion nodes have indegree = out degree = 1. It is shown that this problem can be solved
in polynomial time if the distance function is the rectilinear metric.
Using this interpretation of the sequencing problem we get a new case of a polynomially
solvable traveling salesman problem (see Lawler, Lenstra, Rinnooy Kan, and Shmoys
[1986, Chapter 4] for examples of other polynomially solvable cases).
Theorem 2.2 If all bins are placed on one side of the PCB and if d = h, then the TSP
with weights defined by (2.1) is solvable in polynomial time.
3 Solving Restricted Facility Location Problems
3.1 N-R-FLP and the Level Curve Approach
In Procedure RT of Section 2 we considered restricted N-facility location problems
(N-R-FLP) of the type
where f(Xn) = IN(Xn) in Step 1 and f(Xn) = (IN(Xn) + OUT(Xn )) in Step 2. In
Section 2 the feasible region F = E2 \ int(R) is the Euclidean plane E2 excluding the
interior of a rectangle R. In this section we consider general convex objective functions
Objective(Xl" .. , XN) and sets R defining the feasible set F = E2 \ int(R) for locating
facilities.
A valuable tool for solving (N-R-FLP) is the level curve of the objective function of
(N-R-FLP) with respect to a given non-negative real z defined by
LeveICurve(Objective(X1, ..• , X N ), z)
:= ((X1, ... ,XN) E EN : Objective(Xl"",XN) = z}
194
Using level curves one can tackle restricted location problems by following ideas de-
scribed, for instance, in Francis and White [1974] and Love, Morris, and Wesolowsky
[1988].
Level Curve Approach for (N-R-FLP)
Input: R subset of the Eulidean plane E2 such that F = E2 \ inteR) is feasible for locating
new facilities.
(1) Find the set LOC of all optimal solutions of the unrestricted n-facility location
problem.
(2) If LOC contains a solution (XI, ... ,XN) which is feasible for (N-R-FLP), then
output (XI, ... ,XN).
(3) Otherwise, find the smallest z such that LevelCurve( Objective(XI' . .. , XN), z) con-
tains a feasible solution (Xl, ... , X N ) and output this solution.
The validity of the Level Curve Approach is based on the convexity of the objective
function which is used to prove the following theorem.
Theorem 3.1 (Hamacher and Nickel [1991 aj) Any optimal solution of N-R-FLP is an
optimal solution of the unrestricted problem or is an element of the boundary of R.
The main issue in an implementation of this approach for special choices of Rand
Objective(XI' . .. , X N ) is how to solve Step 3 which is in itself an optimization problem.
As such the Contour Line Approach is mainly a reformulation of the original problem
which is nevertheless very helpul in the development of algorithms for solving (N-R-FLP).
3.2 Efficient Algorithms for Restricted 1-Facility Median Prob-
lems
In this subsection we locate a single new facility with respect to M existing facilities
PI, ... , PM. The objective function is the weighted sum of distances between the new facil-
ity and the existing facilities, where the weights are non-negative real numbers WI, • .• , WM.
The restricting set R is a convex subset of the Eulidean plane E2. The resulting restricted
facility location problem - the restricted I-facility median problem, is
min Objective(X) = '" wmd(X, Pm)
XEF L..;
mEM
We denote this problem in the following with l-RMP, or if we want to emphasize a
particular distance function d, with 1- RMP / d.
195
In the RT Procedure of Section 2, l-RMP is used as model for placing a single bin
holding the parts of type n, n E N. The weights Wm are dependent on the insertion
sequence SEQ and are elements of {a, 1, 2} where
Wm = 2 iff m as well as the successor of m in SEQ are parts of type n
Wm = 1 iff m is a part of type n, but the successor of m is of a different type.
Since there is no interaction between the locations of the bins, we used the N -fold
solution of this problem as a procedure for solving the N -facility problems in Step 1
and Step 3 of the RT Procedure, respectively. Further applications such as the location
of facilities subject to forbidding a union of disjoint sets, or the location of emergency
facilities subject to center constraints are discussed in Hamacher and Nickel [1991 b].
In the following we assume that we have found the set X* of optimal solutions of
1-RMP and that none of these solutions is feasible. (Otherwise we will stop and output
such a solution.)
By Theorem 3.1 we can restrict ourselves to considering Boundary(R) - the boundary
of the set R - to find the optimal solution of l-RMP. Thus 1-RMP reduces to a one-
dimensional line search problem. If we further specify the distance functions we can
improve upon this procedure.
First let d be the squared Euclidean distance, i.e., d((XI,YI),(X2,Y2)) = (Xl - X2)2 +
(YI - Y2)2. For the unrestricted median problem it is known (see, for instance Francis
and White [1974]) that the optimal solution X* of the unrestricted problem is uniquely
defined and that the level curves are circles centered at X*. Consequently, the Level
Curve Approach is in this case a suitable procedure if we can efficiently find the in-circle
of R, i.e., the circle in the set R with largest radius centered at X*. For instance, if R is a
polyhedron, we only have to compute the projections of X* to the sides of R and choose
as radius of the in-circle the minimum of the distances between X* and its projection
points.
If d = 100 is the Chebyshev metric, or d = II is the rectilinear metric, we transform
1-RMP to a combinatorial problem by replacing the infinite set Boundary(R) by a finite
subset of Boundary(R). The key in this approach is the use of construction lines.
For 1-RMP with d = 100 construction lines are 45 0 and -45 0 lines through the points
PI, . .. ,PM' Correspondingly, construction lines for problems with d = II are horizontal
and vertical lines through Pm. For each real number z
LevelCu1've(Objective(X),z):= {X E E2 : Objective(X) = z}
is in both cases a closed polygon. The breakpoints of these polygons are on construction
lines. This property can be used to prove the following result.
196
Theorem 3.2 (Hamacher and Nickel [1991 aj) Let 1-RMP be such that any optimal
solution of the unrestricted problem is infeasible, let R be a convex set, and let d = 11
or d = 100 , Then at least one of the intersection points of Boundary(R) with one of the
construction lines is optimal for 1-RMP.
The following procedure is an immediate consequence of Theorem 3.2.
Procedure l-RMP (Hamacher and Nickel [1991 a])
Comment: The procedure is applicable for d = 11 and d = 100 ,
(1) Find the optimal solutions set LOC* of the unrestricted median problem.
(2) If some X* E LOC* is feasible for 1-RMP, then output X*.
(3) Else compute for all m E M the intersection points Yi, ... " YL of Boundary(R)
with the construction lines through Pm. (This is the only step where the algorithm
for d = 11 and d = 100 differ).
(4) Output Y E argmin{Objective(Yi) : 1 = 1, ... , L}.
Since there are at most 4M points in which the construction lines intersect with
Boundary(R), the procedure is very efficient. In particular, if R is a convex polyhedron,
1-RMP can be solved with a time complexity which is linearly depending on M and the
number of sides and extremal rays of the polyhedron for Steps (1) - (3) and which is
O(MlogM) for Step (4).
In this case one can also compute the set of all optimal solutions with the same
complexity: We call two of the points Yi and Yj computed in Step 3 neighbors on
Boundary(R) if they lie on the same side of the polyhedron and if one can move on
Boundary(R) from Yi to lj without passing another point Yk computed in Step 3. Then
the following result characterizes all optimal solutions of 1-RMP.
Theorem 3.3 (Hamacher and Nickel [1991 bj): Let R be a polyhedron. Then X is an
optimal solution of 1-RMP/11 (resp. 1-RMP/100) if and only if there exist two points
Yi, lj E argmin{ Objective(Yi) : 1= 1, ... , L} computed in Step 4 of Procedure 1-RMP
which are neighbors on Boundary(R) such that X is contained on the line between Yi and
lj.
It is also interesting to note that if all Pm are integer-valued and R is a rectangle
with integer sides (as can be assumed in the case of the PCB assembly problem), then all
optimal solutions of the 1-RMP are a.lso integer-valued.
197
A code based on the results of this section has been developed by Nickel [1991].
The results of this subsection extend - with some minor modifications - to sets R which
are unions of convex sets and to some special cases of non-convex sets (see Hamacher and
Nickel [1991 b]).
4 Max-Linear Combinatorial Optimization Problems
If we produce Q types of different PCB in small lot sizes we may not be willing to solve
an optimization problem for each of the different types. Instead we want to make at
the beginning of the production the location and sequencing decisions for all PCB types
without changing them later on. A good solution which may serve as a compromise for all
different types of PCB's is obtained by applying as model the max-linear combinatorial
optimization problem (MLCO)
min f(x):= max{c1x, ... ,cQx: x E F}
where cq := (ci, ... , c~) is for q = 1, ... , Q an integer row vector in Rn, and F is a set of
feasible solutions for a combinatorial optimization problem.
Drezner and Nof [1984] used :tvILCO with the set of assignments as feasible solution
set F to model the allocation of a finite set of available locations to bins (see Section 2.2).
In this section we will summarize some results obtained for general MLCO developed in
Chung, Hamacher, Maffioli and Murty [1991].
By reduction from the interval subset sum problem and the set partitioning problem,
respectively, one can show the following result.
Theorem 4.1 (Chung et al {1991}} The unconstrained MLCO (i.e., F = {o,l}n) isN'P-
hard for Q = 2. The unconstrained AILCO with variable Q is strongly N'P-hard.
Moreover, a reduction from subset-sum yields the next result.
Theorem 4.2 (Lebrecht {1991}} The max-linear assignment problem is N'P-hard.
Solution procedures for MLCO make use of the following result:
Theorem 4.3 (Chung et al. {1991}} Among the optimal solutions of MLCO there is
always a Pareto optimal solution oj the multi-criteria optimization problem with respect
to objectives Cl, ••. , CQ.
198
Currently available implementations of max-linear assignment problems are based on
this result (Lebrecht [1991]' Righini and Colorni [1991]). In Lebrecht [1991] the La-
grange an dual is solved, and solutions are ranked for the optimal Lagrangean multiplier
until a Pareto optimal solution is identified which solves the max-linear assignment prob-
lem.
References
[1] Ball, M. 0., and M. J. Magazine. Sequencing of insertions in PCB assembly. Oper-
ations Research, 36:192-210, 1988.
[2] Chung, S. J., H. W. Hamacher, F. Maffioli, and K. G. Murty. A note on com-
binatorial optimization problems with max-linear objective function. Accepted for
Discrete Applied Mathematics, 1991.
[3] Drezner, Z., and S. Y. 'Nof. On optimizing bin picking and insertion plans for as-
sembly robots. IIE Tmnsactions, 16:262-270, ] 984.
[4] Foulds, L. R., and H. W. Hamacher. Optimal bin location and sequencing in printed
circuit board assembly. Accepted for European Journal of Opemtions Research, pub-
lished as Report No. 181, Department of Mathematics, University of Kaiserslautern,
1990.
[5] Francis, R. 1., and J. A. White. Facility layout and location: an analytical approach,
Prentice-Hall 1974.
[6] Francis, R. 1., H. VI!. Hamacher, C.-Y. Lee, and S. Yeralan. On automating robotic
assembly workplace planning. Submitted to lIE Tmnsactions, published as Re-
search Report 89-7, Department of Industrial and Systems Engineering, University
of Florida, 1989.
[7] Hamacher, H. W., and S. Nickel [1991 a]. Combinatorial algorithms for some 1-
Facility median problems in the plane. Research Report No. 191, Department of
Mathematics, University of Kaiserslautern, 1991.
[8] Hamacher, H. W., and S. Nickel [1991 b]. Restricted facility location problems
and applications. Research Report (in preparation), Department of Mathematics,
University of Kaiserslautern, 1991.
[9] Lawler, E. L., J. K. Lenstra, A. H. G. Rinnooy Kan, and D. B. Shmoys. The tmveling
salesman problem. John \tViley, 1986.
[10] Lebrecht, U. Max-Lineare Zuordnungsprobleme in der Roboteroptimierung.
Diploma Thesis, Department of Mathematics, University of Kaiserslautern, 1991.
[11] Love, R. F., J. G. Morris, and G. O. Wesolowsky. Facilities location: models f3
methods. North-Holland, New York, 1988.
[12] Nickel, S. Restriktive Standortprobleme. Diploma Thesis, Department of Mathe-
matics, University of Kaiserslautern, 1991.
[13] Righini, G., and A. Colomi. Max-linear combinatorial optimization. Technical Re-
port, Dipartimento eli Elettronica, Politecnico di Milano, 1991.
Job Shop Scheduling
1. Introduction
(extended abstract)
J .K. Lenstra *
The job shop scheduling problem is described as follows. Given are a set of jobs and a set
of machines. Each machine can handle at most one job at a time. Each job consists of
a chain of operations, each of which needs to be processed during an uninterrupted time
period of a given length on a given machine. The purpose is to find a schedule, i.e., an
allocation of the operations to time intervals on the machines, that has minimum length.
The problem allows a number of relatively straightforward mathematical formulations.
In addition, it is extremely difficult to solve to optimality. This is witnessed by the fact
that a problem instance with only ten jobs, ten machines and one hundred operations,
published in 1963, remained unresolved until 1986.
We formulate mathematical models of the job shop scheduling problem in Section
2 and review results on its computational complexity in Section 3. Optimization and
approximation algorithms are presented in Sections 4 and 5, respectively; their computa-
tional merits are discussed in Section 6.
2. Model Formulation
Given are a finite set .J of jobs, a finite set M of machines, and a finite set 0 of operations.
For each operation i E 0, there is a job Ji E .J to which it belongs, a machine Mi E M
on which it requires processing, and a processing time Pi E IN. There is a binary relation
--t on 0 that decomposes 0 into chains corresponding to the jobs; more specifically, if
i --t j, then Ji = Jj and there is no k <j. {i, j} with i --t k or k --t j. The problem is to
find a starting time Si for each operation i E 0 such that
"Eindhoven University of Technology, Department of Mathematics and Computing Science, P.O. Box
513, 5600 MB Eindhoven, Netherlands.
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgiil et al.
© Springer-Verlag Berlin Heidelberg 1992
is minimized subject to :
Si ~ 0
Sj - Si > Pi
200
maxS· + p.
iECJ' ,
for i EO,
whenever i ---+ j, i,j E 0,
Sj - Si ~ Pi V Si - Sj ~ Pj whenever Mi = Mj,i,j E 0.
(1)
(2)
(3)
(4)
The objective function (1) represents the schedule length, in view of (2). The conditions
(3) are the job precedence constraints. The conditions (4) represent the machine capacity
constraints, which make the problem N P-hard.
To obtain an integer programming formulation, we choose an upper bound T on the
optimum and introduce a 0-1 variable Yij for each ordered pair (i,j) with Mi = Mj, where
Yij = 0 (Yij = 1) corresponds to Sj - Si ~ Pi (Si - Sj ~ pj). We now replace (4) by
Yij E {0,1} }
Yij + Yji = 1 whenever Mi = Mj,i,j E 0.
Si + Pi - Sj - TYij :::; 0
(5)
This formulation is closely related to the disjunctive graph model of Roy and Sussmann
[28]. The disjunctive graph G = (0, A, E) has a node set 0, an arc set A = {(i,j) I i ---+ j},
and an edge set E = {{i,j} I Mi = M j }; a weight Pi is associated with each node
i. The (directed) arcs represent the job precedence constraints; the (undirected) edges
represent the machine capacity constraints. The basic scheduling decision is to impose
an ordering on a pair of operations {i,j} on the same machine. In the integer program,
this corresponds to setting Yij = 0 and Yji = 1 or Yij = 1 and Yji = 0; in the disjunctive
graph, it corresponds to orienting the edge in question, in one way or the other. Thus,
there exists a one-to-one correspondence between feasible values of the Yij in {(I) , (2),
(3), (5)} and orientations of the edges in E for which the resulting digraph is acyclic.
Given any such orientation, we can determine feasible starting times by setting each Si
equal to the wei~ht of a maximum-weight path in the digraph finishing at i minus Pi; the
objective value is equal to the maximum path weight in the digraph. The problem is now
to find an orientation of the edges in E that minimizes the maximum path weight. We
refer to Figure 1 for an example.
u 1
Lv. J1
-+ 1-+
fl-u MI
Pv. 2
2
J1
2-+
M2
8
201
M = {M I ,M2 ,M3 ,M4 }
J = {JI, J 2 , J 3}
(? = {1,2,3,4,5,6,7,8,9,10}
3 4 5 6 7
J1 J2 J2 J2 J2
3 4-+ 5-+ 6-+ 7
M3 M2 Ml M3 M4
4 7 3 6 3
(a) Instance.
8
8
J3
8-+
Ml
5
)-------+!2)------_+!3
3 6
9 10
J3 J3
9-+ 10
M2 M4
9 1
4
l:----ir---+-{ 5 r----t---.-{ 6 )------_+! 7
9
)-------.-{ 9 )-------+-{
(b) Instance, represented as a disjunctive graph.
9
(c) Feasible schedule, represented as an acyclic directed graph.
Figure 1: A job shop scheduling problem.
3
202
3. Computational Complexity
Two or three machines. - The two-machine job shop scheduling problem with no more
than two operations per job is solvable in O( n log n) time by a simple extension of John-
son's algorithm for the two-machine flow shop [19]. The extensions to two machines and
no more than three operations per job and to three machines and no more than two
operations per job are N P-hard [23, 15].
In case of unit processing times, the two-machine job shop scheduling problem is
solvable in time that is linear in the total number of operations [18, 5]. Again, the
extension to three machines is N P-hard, and so is the case of two machines and processing
times 1 or 2 [22].
Two or three jobs. - The job shop scheduling problem with two jobs is solvable in
polynomial time by solving a shortest path problem in a network that is based on a
geometric interpretation of the problem [31, 17]. The three-job problem has recently been
shown to be NP-hard [30].
Deadline two or three. - It is trivial to decide if a schedule of length two exists for a
given instance of the job shop scheduling problem. But, how does one decide if a deadline
of value three can be met?
4. Optimization algorithms
Optimization algorithms for the job shop scheduling problem proceed by branch and
bound. We will describe methods of that type in terms of the disjunctive graph (0, A, E),
where 0 is the set of operations, A the arc set, and E the edge set.
A node in the search tree is usually characterized by an orientation of each edge in
a certain subset E' C E. The question then is how to compute a lower bound on the
value of all completions of this partial solution. Nemeti, Charlton & Death and Schrage
[26, 8, 29] are among the researchers who obtain a lower bound by simply disregarding
E - E' and computing the maximum path weight in the directed graph (O,A U E').
A more sophisticated bound, due to Bratley, Florian & Robillard [4], is based on the
relaxation of the capacity constraints of all machines except one. They propose to select
a machine M' and to solve the job shop scheduling problem on the disjunctive graph
(0, A U E', {{i,i} I Mi = Mj = M'}). This is a single-machine problem where the arcs in
AUE' define release times and delivery times for the operations that are to be scheduled on
machine M' as well as precedence constra.ints among these operations. This observation
has spawned much work on this single-machine problem. Although it is N P-hard, there
203
exist fairly efficient methods for its solution. Most other lower bounds appear as special
cases of the single-machine bound, by relaxing the capacity constraint of M' (which gives
Nemeti's longest path bound), by underestimating the contribution of the release and
delivery times, by allowing preemption, or by ignoring the precedence constraints. These
relaxations, with the exception of the last one, turn an NP-hard single-machine problem
into a problem that is solvable in polynomial time; see [21] for details.
Fisher, Lageweg, Lenstra & Rinnooy Kan [12] investigate surrogate duality relaxations
in which either the capacity constraints of the machines or the precedence constraints
among the operations of each job are weighted and aggregated into a single constraint.
In theory, the resulting bounds dominate the above single-machine bound. Balas [2]
describes a first attempt to obtain bounds by polyhedral techniques.
The usual enumeration scheme is due to Giffier & Thompson [14]. It generates all
active schedules by constructing them from front to back. At each stage, the subset
0' of operations all of whose predecessors have been scheduled is determined and their
earliest possible completion times are calculated. It suffices to consider only a machine
on which the minimum value of the earliest completion time is achieved and to branch
by successively scheduling next on that machine all operations in 0' for which the release
time is strictly smaller than this minimum. In this scheme, several edges are oriented at
each stage.
Lageweg, Lenstra & Rinnooy Kan and Carlier & Pinson [21, 7] describe alternative
enumeration schemes whereby at each stage, a single edge is selected and oriented in
either of two ways. Barker & J'vIci\'Iahon [3] branch by rearranging the operations in a
critical block that occurs on the maximum weight path.
We briefly outline three of the ma.ny impJemented branch and bound algorithms for
job shop scheduling. McMahon & norian [25] combine the Giffier-Thompson enumera-
tion scheme with the single-machine bound (disregarding precedence constraints), which
is computed for all machines by their own algorithm. Lageweg [20] applies the same
branching rule, computes the single-machine bound only for a few promising machines
using Carlier's [6] alg<?rithm, and obtains upper bounds with a heuristic due to Lageweg,
Lenstra & Rinnooy Kan [21]. Carlier & Pinson [7] implement their novel enumeration
schemes, the preemptive version of the single-machine bound (which can be computed
in polynomial time), and a collection of powerful elimination rules for which we refer to
their paper.
204
5. Approximation algorithms
Most approximation algorithms for job shop scheduling use a dispatch rule, which sched-
ules the operations according to some priority function. A considerable effort has been
invested in the empirical testing of rules of this type [13, 9, 10, 27, 16]
Adams, Balas & Zawack [1] develop a sliding bottleneck heuristic, which employs an
ingenious combination of schedule construction and iterative improvement, guided by
solutions to single-machine problems of the type described above. They also embed this
method in a second heuristic that proceeds by partial enumeration of the solution space.
Matsuo, Suh & Sullivan [24] and Van Laarhoven, Aarts & Lenstra [32] apply the
principle of simulated annealing to the job shop scheduling problem. This is a randomized
variant of iterative improvement. It is based on local search, but it accepts deteriorations
with a small and decreasing probability in the hope of avoiding bad local optima and
getting settled in a global optimum. In the latter paper, the neighborhood of a schedule
contains all schedules that can be obtained by interchanging two operations i and j
for which Mi = Mj and the arc (i, j) is on a longest path. In the former paper, the
neighborhood structure is more complex.
6. 10 * 10 = 930
The computational merits of all these algorithms are accurately reflected by their perfor-
mance on the notorious 10-job 10-machine problem instance due to Fisher & Thompson
[11].
The single-machine bound, maximized over all machines, has a value of 808. McMa-
hon & Florian citemf-1975 found a schedule of length 972. Fisher, Lageweg, Lenstra &
Rinnooy Kan [12] applied surrogate duality relaxation of the capacity constraints and of
the precedence constraints to find lower bounds of 813 and 808, respectively; the compu-
tational effort involved did not encourage them to carryon the search beyond the root of
the tree. Lageweg [20] found a schedule of length 930, without proving optimality; also,
he computed a number of multi-machine lower bounds, ranging from a three-machine
bound of 874 to a six-machine bound of 907. Carlier & Pinson [7] were the first to prove
optimality of the value 930, after generating 22021 nodes and five hours of computing.
The main drawback of all these enumerative methods, aside from the limited problem
sizes that can be handled, is their sensitivity to particular problem instances and even to
the initial value of the upper bound.
The computational experience with polyhedral techniques that has been reported until
205
now is slightly disappointing in view of what has been achieved for other hard problems.
However, the investigations in this direction are still at an initial stage.
Dispatch rules show an erratic behavior. The rule proposed by Lageweg, Lenstra &
Rinnooy Kan [21] constructs a schedule of length 1082, and most other priority functions
do worse.
Adams, Balas & Zawack [1] report that their sliding bottleneck heuristic obtains a
schedule of length 1015 in ten CPU seconds, solving 249 single-machine problems on the
way. Their partial enumeration procedure succeeds in finding the optimum, after 851
seconds and 270 runs of the first heuristic.
Five runs of the simulated annealing algorithm of Van Laarhoven, Aarts & Lenstra
[32], with a standard setting of the cooling parameters, take 6000 seconds on the average
and produce an average schedule length of 942.4, with a minimum of 937. If 6000 seconds
are spent on deterministic neighborhood search, which accepts only true improvements,
more than 9000 local optima are found, the best one of which has a value of 1006. Five
runs with a much slower cooling schedule take about 16 hours each and produce solution
values of 930 (twice), 934,935 and 938. In comparison to other approximative approaches,
simulated annealing requires unusual computation times, but it yields consistently good
solutions with a modest amount of human implementation effort and relatively little
insight into the combinatorial structure of the problem type under consideration.
References
[1] Adams, J., E. Balas, and D. Zawack. The shifting bottleneck procedure for job shop
scheduling. Management Sci., 34:391-401, 1988.
[2] Balas, E. On the facial structure of scheduling polyhedra. Math. Programming
Stu.d., 24: 179-218, 1985.
[3] Barker, J. R., and G. B. McMahon. Scheduling the general job-shop. Management
Sci., 31:594-598, 1985.
[4] Bratley, P., M. Florian, and P. Robillard. On sequencing with earliest starts and
due dates with application to computing bounds for the (n/m/G/ Fmax) problem.
Naval Res. Logist. Quart., 20:57-67, 1973.
[5] Brucker, P. A linear time algorithm to minimize maximum lateness for the two-
machine, unit-time, job-shop, scheduling problem. In System Modeling and Opti-
mization, Lecture Notes in Control and Information Sciences, R. F. Drenick and F.
Kozin(Eds.), 38:566-571. Springer, Berlin, 1982.
[6] Carlier, J. The one-machine sequencing problem. European J. Oper. Res., 11:42-47,
1982.
[7] Carlier, J., and E. Pinson. An algorithm for solving the job-shop problem. Man-
agement Sci., 35:164-176, 1989.
206
[8) Charlton, J. M., and C. C. Death. A generalized machine scheduling algorithm.
Opel'. Res. Quart., 21:127-13'1, 1970.
[9) Conway, R. W., W. L. Maxwell, and L. W. Miller. Theory of Scheduling, Addison-
Wesley, Reading, MA, 1967.
[10) Day, J., and M. P. Hottenstein. Review of scheduling research. Naval Res. Logist.
Quart., 17:1l-39, 1970.
[1l) Fisher, H., and G. L. Thompson. Probabilistic learning combinations of local job-
shop scheduling rules. In Industrial Scheduling, Muth and G. L. Thompson(Eds.),
225-251. Prentice Hall, Englewood Cliffs, NJ, 1963.
[12) Fisher, M. L., B. J. Lageweg, J. K. Lenstra, and A. H .G. Rinnooy Kan. Surrogate
duality relaxation for job shop scheduling. Discrete Appl. Math., 5:65-75, 1983.
[13) Gere, W. S. Heuristics in job shop scheduling. Management Sci., 13:167-190, 1966.
[14) GifRer, B., and G.L. Thompson. Algorithms for solving production-scheduling prob-
lems. Opel'. Res., 8:487-503, 1960.
[15) Gonzalez, T., and S. Salmi. Flowshop and jobshop schedules: complexity and
approximation. Opel'. Res., 26:36-52, 1978.
[16) Haupt, R. A survey of priority rule-based scheduling. OR Spektrum, 1l:3-16, 1989.
[17) Hardgrave, W. W., and G. L. Nemhauser. A geometric model and a graphical
algorithm for a sequencing problem. Opel'. Res., 1l:898-900, 1963.
[18) Hefetz, N., and 1. Adiri. An efficient optimal algorithm for the two-machines unit-
time jobshop schedule-length problem. Math. Opel'. Res., 7:354-360, 1982.
[19) Jackson, J. R. An extension of Johnson's results on job lot scheduling. Naval Res.
Logist. Quart., 3:201-203, 1956.
[20) Lageweg, B. J. Private communication, 1984.
[21) Lageweg, B. J., J. K. Lenstra, and A. H. G. Rinnooy Kan. Job-shop scheduling by
implicit enumeration. A1anagcmcnt Sci., 24:441-450, 1977.
[22) Lenstra, J. K, and A. H. G. Rinnooy I(an. Computational complexity of discrete
optimization problems. Ann. Discrete A1ath., 4:121-140, 1979.
[23) Lenstra, J. K., A. H. G. Rinnooy Kan, and P. Brucker. Complexity of machine
scheduling problems. Ann. Discrete 1I1ath., 1:343-362, 1977.
[24) Matsuo, H., C. J. Suh, and R. S. Sullivan. A controlled search simulated anneal-
ing method for the general jobshop scheduling problem. Working paper 03-44-88,
Graduate School of Business, University of Texas, Austin, 1988.
[25) McMahon, G. B., and M. Florian. On scheduling with ready times and due dates
to minimize maximum lateness. Opel'. Res., 23:475-482, 1975.
[26) Nemeti, L. Das Reihenfolgepl'Oblem in del' Fertigungsprogrammierung und Linear-
planung mit logischen Bedingullgen. Malhematica (Cluj), 6:87-99, 1964.
[27) Panwalkar, S. S., and W. Iskander. A survey of scheduling rules. Opel'. Res., 25:45-
61, 1977.
207
[28] Roy, B., and B. Sussmann. Les problemes d'ordonnancement avec contraintes dis-
jonctives. Note DS no. 9 bis, SEMA, Montrouge, 1964.
[29] Schrage, L. Solving resource-constrained network problems by implicit enumeration
- nonpreemptive case. Oper. Res., 18:263-278, 1970.
[30] Sotskov, Y. N. The complexity of shop-scheduling problems with two or three jobs.
European Journal Oper. Res., 53:326-336, 1991
[31] Szwarc, W. Solution of the Akers-Friedman scheduling problem. Oper. Res., 8:782-
788, 1960.
[32] Van Laarhoven, P. J. M., E. H. L. Aarts, and J. K. Lenstra. Job shop scheduling
by simulated annealing. Oper. Res., to appear.
On the Construction of the Set of K-best
Matchings and Their Use in Solving Constrained
Matching Problems
U. Derigs*
A. Metzt
Abstract
For constructing the set of K-best solutions for a combinatorial optimization
problem so-called partitioning schemes have been developed in literature which
require the iterative solution of the basic optimization problem over a restricted
groundset, i.e. a restricted set of variables. In this paper we discuss the application
of these schemes to the problem of finding the set of K-best perfect matchings in
a graph. We show that the use of those features which make the basic shortest
augmenting path matching algorithm efficient, like the assignment start-procedure,
are essential to also reduce the computational effort for the K-best procedure.
Constructing the set of K-best matchings has been used recently in an approach
for solving matching problems with side-constraints. Here we show how the K -best
strategy can be customized for this special purpose.
1 Introduction
The matching problem (M P) is a well-solved class of combinatorial optimization problems
and has found considerable interest in the literature under theoretical and algorithmic
aspects and, only recently, as a consequence of the availability of fast matching codes,
also with respect to modelling practical problems.
*Lehrstuhl fiir Wirtschaftsinformatik, Universitiit zu Kaln, D 5000 Kaln 41, Federal Republic of Ger-
many. This work was finished while the first author was visiting the College of Business and Management
of the University of Maryland at College Park, USA.
tLehrstuhl fiir Wirtschaftsinformatik, Universitiit zu Kaln, D 5000 Kaln 41, Federal Republic of
Germany. The work of the second author was supported by a grant from the Deutsche Forschungs-
gemeinschaft.
NATO AS! Serie.s, Vol. F 82
Combinatorial Optimization
Edited by M. Akgiil el al.
© Springer-Verlag Berlin Heidelberg 1992
210
In many planning situations the decision maker may not only be interested in (one
of) the optimal solutions for an optimization problem but in a set of "good" solutions
which are then evaluated with respect to secondary criteria which may eventually not be
operational at all. In such a decision situation the construction of the series of the K-best
feasible solutions, where the parameter K is specified by the decision maker, seems to be
a valuable support.
Many decision problems from real world can be modelled as a standard graph op-
timization problem, i.e. network flow problem, spanning tree problem etc., with some
few additional constraints. Such additional constraints, although often conceptually and
mathematically simple, destroy the combinatorial nature of the basic optimization prob-
lem in general and thus they are almost always algorithmically complex.
A simple approach for such models and problems would be to neglect the additional
constraints beforehand and to "solve" the pure, standard problem only. If (one of) the
optimal solution (s) also fulfills the additional constraints, then the solution is feasible
and hence optimal for the entire problem, too. Otherwise the sequence of "next best"
solutions for the standard problem is constructed until the first time a feasible solution
occurs, which is then optimal. Again, we are confronted with the problem of determining
the set of K-best solutions, yet with an unknown parameter K this time.
Formally the concept of the set of K-best solutions can be introduced as follows:
Let S be the set of feasible solutions for a (combinatorial) minimization problem
(P) min { c( S) 1 S E S }
Then a set S' ~ S is called a set of K-best solutions for (P) if
1 S'I= K and
c(S') :s; c(S) for S' E S' and S E S\S'
In this paper we describe how the set of K-best solutions can be determined for the
case of a matching problem. We will discuss how the implementation strategy should be
customized to allow for an efficient, i.e. fast and storage saving, procedure for solving
matching problems with some additional side-constraints which are relevant in practice.
211
2 Solving matching problems
In this section we shortly review the terminology and the basic algorithmic principles
from matching theory.
Let G = (V, E) be an undirected graph and c : E -t IR?o. For any F ~ E we define
c( F) := L:eEF c( e) the cost of F. For an edge e E E connecting the nodes i and j, i, j E V,
we also write e = (ij). For E' ~ E let V(E') be the subsets of nodes covered by E', i.e.
i E V(E') if exists j E V s.t. (ij) E E'. A subset M ~ E is called a (perfect) matching
in G iff any node v E V is incident to at most (exactly) one edge in M. In the following
we only consider perfect matchings in G. With M the set of all perfect matchings in G
the matching problem (MP) can be written as follows
(MP) min { c( AI) 1 M EM}
Introducing a (binary) decision vector x E {O, I} lEI the matching problem can be formu-
lated as an Integer Program using the node-edge incidence matrix A of G:
mm c'x
Ax 1
x E {O, 1}IEI .
If G is a bipartite graph then (2) can be replaced by
x:;:: 0
and (M P) reduces to an ordinary Linear Program (LP).
(1)
(2)
(2')
If G is not bipartite then the introduction of additional constraints is essential to being
able to transform (MP) into an (LP).
For W E V let 8(W) be the set of edges having exactly one endnode in Wand let
R = {W ~ VII W I:;:: 3, odd} then the following defines the so-called set of cut-set-
inequalities
x(8(W)) :;:: 1 for W E R (3)
The system (1), (2') and (3) defines the so-called perfect matching polytope and (MP) is
"equivalent" to
minc'x s.t. (1), (2') and (3) .
For defining the matching polytope only those cut-set-inequalities are necessary which
212
correspond to odd sets having the property to be hypomatchable. Here, an odd set
W is called hypomatchable if for all i E W there exists a matching M; in G with the
following properties
(i)M; n<5({i}) = 0
(ii) I Mi n ,(W) 1= HI WI -1).
Here ,(W) is the set of edges from E having both endnodes in W.
Thus when matching any node from a hypomatchable set W with a node not in W, the
remaining nodes in W can always be perfectly matched using only edges "within W".
Yet, the knowledge of this LP-formulation for (MP), which is due to Edmonds [9], is
not used for solving (M P) in the sense that standard LP-algorithms are applied but for
characterising optimal matchings using LP-duality. For solving (M P) specific combina-
torial concepts apply which we introduce now.
Given a non perfect matching M in G an alternating path or cycle P is a path or cycle
the edges of which are alternately in M and not in M. An alternating path P is called
an augmenting path if the endnodes of P are not matched under M.
It is easy to see that reversing the role of matching and nonmatching edges on an
alternating cycle P or an augmenting path P will again produce a matching in G, where
in the latter case the cardinality of the new matching has increased by one. This process
is called an augmentation along P and we write M EEl P for this process as well as for the
new matching i.e.
M EEl P = (M\P) U (P\M) .
With every alternating cycle and augmenting path P we associate a certain "length"
l(P) := c(P\M) - c(P n M) .
With this definition we get the following useful relation
c(M EEl P) = c(M) + l(P) .
An alternating cycle P is called a negative alternating cycle iff Z(P) < 0 holds and we
call a matching M extreme if it does not allow any negative alternating cycle in G. The
following fact is the basic combinatorial optimization criterion for (M P):
Lemma 1: A perfect matching M solves (M P) iff it is an extreme matching.
213
From an algorithmic point of view the following is fundamental.
Lemma 2: Let M be extreme in G but not perfect and P a shortest augmenting path
connecting two unmatched nodes, then M EB P is extreme, too.
(Proofs for these classical results can be found in Derigs [7]).
Extreme matchings M can be viewed as optimal perfect matchings in the subgraph
which is induced by the nodes which are matched under M. Thus extreme matchings can
be characterized using LP-duality:
Lemma 3: M is an extreme matching iff there exist y : V -t IR and Z : n -t IR?o such
that
Yi + Yj + L Zw < Cij for (ij) E E (4)
W: (ij)E5(W)
Yi + Yj + L Zw = Cij for (ij) E M (5)
W: (ij)E5(W)
A triple (M, y, z) fulfilling (4) and (5) with M being a matching is called a compatible
triple. If z = 0 holds, then we write (M,y) and call it a compatible pair. Note that
in the case of a bipartite graph the introduction of the set of cut-set-inequalities (3) is
superfluous and extreme matchings induce compatible pairs.
Using combinatorial arguments it can be shown that for extreme matchings M there
always exists a so-called strongly dual solution, i.e., y and z fulfilling (4) and (5) and
R( z) = {W I Zw > O} is a family of nested subsets
of V which are hypomatchable in G' = (V, E'),
where E' = {(ij) EEl Yi + Yj + .LW:(ij)E5(W) Zw = Cij}.
A compatible triple fulfilling (6) is called a strongly compatible triple.
(6)
The shortest augmenting path method (SAP-method) - see Derigs [4] - constructs
optimal perfect matchings through a sequence of strongly compatible triples the matchings
of which are augmented along shortest augmenting paths. Throughout the procedure odd
sets W with Zw > 0 have to be shrunk to so-called pseudonodes and the path finding
algorithm works on a so-called surface graph. Due to the fact that the procedure of
shrinking odd sets and also of expanding them again is the most complex and time-
consuming part of the entire algorithm, working with compatible pairs as much as possible
reduces the overall running time significantly.
The SAP-procedure can be started from any compatible triple, yet the choice of the
start solution has shown to be a cruical step and hence clever prepocessing is necessary.
214
A startprocedure which has shown to be outmost efficient is the so-called assignment-
startheuristic where a compatible pair (M, y) is constructed from an optimal perfect
matching (assignment) in a related bipartite graph (see Derigs and Metz [8]).
3 Post optimal analysis and restricted matching prob-
lems arising in K- best-approaches
Basic exchange properties and the construction of so-called restricted problems and their
use in determining the set of 1{-best solutions for a combinatorial optimization problem
have been discussed in Derigs [5]. In this section we want to specify these concepts for
the case of a matching problem.
Two perfect matchings M, N E M are called neighbours if there exists an alternating
cycle P such that M = NEB P. With this concept the following relations hold:
Lemma 4:
- Let Ml be an optimal perfect matching then a second best matching M2 can be found
among the neighbours of MI.
- Let M 1 , ••. ,Mk be the set of k- best perfect matchings then a (k + 1)st best matching
can be found among the neighbours of M I , ... , Mk •
Having constructed the optimal perfect matching M I , a second best matching can be
found by identifying the "shortest" alternating cycle with respect to MI in G. Note that
since MI is extreme all alternating cycles have nonnegative length.
Such a shortest alternating cycle can be obtained by successively deleting one of the
matching edges, say e, from MI and then finding the shortest augmenting path connecting
the two nodes that have become unmatched by deleting the matching edge e. With P
the shortest path within this set of shortest augmenting paths M2 := Ml EB P is a second
best matching.
For finding the shortest augmenting paths one iteration of the basic SAP-method can
be applied since the "startmatching" M; := MI \ {e} is extreme. Yet the compatible triple
(MI' y, z) may have to be altered quite substantially to obtain a strongly compatible triple
(M{, y', z') for starting the SAP-method. Also the surface graph may have to be altered
before the application of the SAP-iteration which turns out to be rather time consuming in
general. We call the procedure just described a "reoptimization" of MI after the deletion
of an edge e E MI.
215
In Derigs [5] we give an approach for performing such reoptimizations. Here in a first
step a compatible triple (M~, y', z') and the associated surface graph is constructed by
applying one iteration of the SAP-method to a slightly modified graph. Then in a second
phase the "real" reoptimization is performed by another application of the SAP-approach.
Thus the whole reoptimization reduces to two iterations of the SAP-algorithm and thus
the second best matching can be found by I V I SAP-iterations in total. Hence finding
the second best matching - when knowing the optimal compatible triple - is of the same
complexity as solving (MP).
For determining a (k + 1)st best matching such an immediate application of the SAP-
reoptimization technique to the set (MI , ... , Jvh) is not possible since with the exception
of MI the other matchings may not be extreme.
During the course of the K-best procedures the essential steps are to solve so-called
"restricted" problems (M Po,!) defined by two sets 0,1 ~ E with 0 n I = 0 in the
following way:
(MPo,r) min {c(M) 1M E M, M n 0 = 0,1 ~ M}
For solving (M Po,!) we construct the following "working graph"
Go,! = (Va,!, Eo,!) with
Va,! = V\ V(I)
Eo,! = E\(U 8(i) U 0)
iEI
Now let F be an optimal perfect matching in GO,l then Mo,! := I U F is an optimal
solution for (MPo,!). The set F is called the set of "free edges" in Mo,r. Let F =
{fI, . .. , fq} then the following propositions hold.
Proposition 1: Mo,I is optimal for all problems of the type (M Po,ruF') with F' ~ F.
Proposition 2: Let (M, y) be a compatible pair in Go,! then for all F' ~ F the
matching M' := M\F' and y induce a compatible pair (M', y') in Go,ruFf and with
f E F\F' the-matching M := M\(F' U {f}) and y induce a compatible pair (M,y) in
GOU{J},lUFf.
(By inducing a compatible pair we mean that the y-vector is restricted to those com-
ponents related to nodes which are still contained in the restricted graph).
As a consequence of the above propositions, problems of the type (M POU{J} ,luFf ) with
F' ~ F and f E F\F' defined as above can be solved in two different ways - besides being
solved completely from scratch.
216
Method (1): Start from Ma,I the optimal matching in Ga,I , delete f and reoptimize
Ma,I in GaU{J},lUF"
Method (2): Start from (£1,y) the compatible pair in GaU{J},IUF' and solve the
problem using (possibly several iterations of) the SAP-method.
In their algorithm for finding K-best perfect matchings Chegireddy and Hamacher
[3] propose the use of the first method referring to the SAP-reoptimization technique of
Derigs [5]. Our experience has shown that with the use of the assignment-startheuristic
it is in general even faster to solve (M PaU{J},IUF') from scratch than to reoptimize. The
reason for this is the fact that the assignment-startheuristic produces an initial extreme
matching of such a large cardinality that in general only very few generic SAP-iterations
are necessary. Moreover it pays to start with a compatible pair, while applying the
reoptimization technique we have to deal with positive z-values from the beginning and
thus time-consuming surface-graph manipulations are necessary in general.
Here the second method is more than a compromise. Some of the information from the
solution process for (MPa,I), the initial compatible pair (M,y), is used to construct an
initial compatible pair (£1, y). Now let F be the optimal perfect matching in (GaU{J},IUF')
and d :=1 F 1- 1M I, then 1F 1- 1ft,-:f I~ d + 1. Thus solving (MPau{J},[uF') starting
from (£1,y) needs at most one more SAP-iteration than solving (MPa,I) from (M,y).
4 Partition strategies for solving K -best matching
problems
In this section we review two well known partition strategies proposed in literature by
Murty [12] and Lawler [11] and Carraresi and Sodini [2], respectively, for finding the set ofI
!{-best solutions for combinatorial optimization problems. The presentation will be cus-
tomized for the matching problem and we will discuss the potentials of both alternatives
especially for this problem.
(i) The partition strategy of Murty and Lawler.
This approach works in f{ steps or iterations. When starting iteration k, 1 ~ k ~ f{,
we have a partition of Minto
217
with M(j) a ,i-th best matching. Moreover, in every family MOJ,IJ we know the best
matching, Mj say.
Then min {c(Mj) I j = 1, ... , q} determines a (k + l)st best matching M(k+l).
Let w.l.o.g. c(Mq) be this minimum, i.e., M(k+l) = Mq. Now set F := Mq \Iq, F =
{iI, ... , fr} say, the set of "free" edges in M(k+l). Then we partition MOq,Iq into
{M(k+l)} U {MOqU{ft},lU{J" ... ,iI_d 11 = 1,2, ... ,1"}. Since with M(k+l) optimal in MOq,lq
we also have M(k+l) optimal in MOq,I for Iq S;;; IS;;; M(k+l), the optimal matchings in the
newly constructed subfamilies can be found by "reoptimizing" M(k+l).
(ii) The partition strategy of Carraresi and Sodini.
This procedure also works in [{ steps and when starting iteration k, 1 :::; k :::; [{, we
have a partition of Minto MOI,I" ... , MOk,h. In every such family MOJ,lJ we know M j
the best and MJ the second best matchings, j = 1, ... , k, and {MI , ... , Mk} is a set of
k-best matchings.
Then min {c( MJ) I j = 1, ... , k} determines a (k + 1)st best 'matching. Let w.l.o.g.
c(MJ) be this minimum, i.e., Mk+l = MJ. Now choose f E Mj\MJ arbitrarily. Then we
partition Mo I into Mo' I' and MO k +1 h+l withJ' J J' J I
OJ := OJ,
Ij := Ij U {f},
Then we know that M j is optimal in Mo~,I; and Mk+I is optimal in MO k +l ,h+l and
we only have to find second best matchings MJ and M'+l' respectively.
These second best matchings can be obtained by applying to MO~,I; and MOk+l,h+l
one partitioning step of the kind which is used in the partition-strategy of Murty and
Lawler and to our knowledge there is no more direct approach known.
Now we want to discuss these approaches with respect to their complexity and imple-
mentability. Due to the fact that one iteration in the second strategy reduces to two times
applying a procedure comparable to one iteration of the first strategy both strategies are
of the same computational complexity with the second strategy requiring twice as much
shortest augmenting path computations. On the other hand the number of matchings to
be stored may be significantly smaller for the second strategy.
Due to the fact that storage is restricted and the time for solving a matching problem is
relatively small when using efficient matching codes, the explicit storage of all matchings
not contained in the set of the [(-best matchings should be avoided if [( is large. This
can be achieved if we only store the cost of the matchings and resolve (M PO,I) if the
partitioning strategy requires the partitioning of family Mo,I.
218
For large values of /{ the strategy of saving storage through recomputing matchings
seems to be essential. Then, under this condition the advantage of the second partition-
strategy becomes immaterial and the first partition-strategy seems to be of advantage,
especially since this scheme also allows for a very efficient recording of the sets 0; and
I;, respectively, as will be shown next. In the following we describe an implementation
of the first strategy in which all matching problems are (re- ) solved using an efficient
preprocessing and also the sets 0 and I are (re- )constructed when needed rather than
stored.
Every problem (MPO,I) which is induced by one subfamily MO,I in the partition can
be viewed as creating a node in a decision tree and an entry in a problem-list. Except
for the root-entry for node (M P0,0) every other node Q in the tree was created when
partitioning a specific node (M Po,!) which we call the predecessor of Q. With F being
such that I U F is the optimal matching in (M Po,!) the new problem Q is of the form
and it creates a list-entry with 5 segments of information
where
[c(Q),OUT(Q), I N(Q),BROTH(Q),PRED(Q)]
c(Q)
OUT(Q)
IN(Q)
BROTH(Q)
PRED(Q)
value of the optimal matching in Q
f q+l
fq if q > 0 and 0 else
pointer to the problem/node (M POU{!q},IU{h, ... ,!q_d)
if q > 0 and 0 else
pointer to the problem/node (M PO,I)
219
:For the root-node we get the entry [COPT, 0, 0, 0, 0] with COPT the value of the optimal
matching in G. In the following, we graph this tree/list structure: The following routine
constructs the sets 0 and 1 from the list entries of a node Q:
procedure I-O-SCAN
Input: list entry for a subproblem Q
Output: associated sets 0 and 1
1(-0(-0
UNTIL PRED(Q) = 0 DO
0(- 0 U OUT(Q)
1 (- Iu IN(Q)
B (- BROTH(Q)
WHILE B =I- 0 DO
I(-/UIN(B)
B (- BROTH(B)
ENDDO
Q (- PRED(Q)
ENDDO
Initially the tree/list contains a single problem/entry [COPT, 0, 0, 0, 0] corresponding to
the optimal matching in G. Nodes/problems that have been used for partitioning will
be fathomed. The procedure terminates if J( nodes have been fathomed i.e. the set of
K-best matchings has been constructed. If all the less than J( problems in the list are
already fathomed, this indicates that the graph G does not allow K perfect matchings.
Otherwise the unfathomed problem with minimal c-value will be the next candidate for
a partitioning step. For this problem we first have to recalculate the optimal matching.
Thus for all fathomed nodes, the associated optimal matching has to be calculated twice.
Using the assignment-startheuristic this can be done very fast. Moreover, as already
described in section 3, the compatible pair obtained by this start procedure is then used
to define initial compatible pairs for the SAP-method when solving the new subproblems
constructed by the partitioning of this node.
If the list contains more than ]( entries it may be possible to delete some nodes/problems.
Let CK be the K-th best c-value in the list, then all subproblems/families of matchings
represented by an entry the c-value of which exceeds CK will not "contain" any of the
K-best matchings. Yet not all of these problems/nodes may be discarded since their list-
entries may contain relevant information. The following procedure will only purge those
subproblems/nodes which are superfluous for determining the set of K-best matchings.
220
After partitioning node (MPO,I) and with F = {h, ... jq} the set of free nodes let
LASTQ := M POUjq,Iu{j" ... ,jq_,}' Then we apply the following freshup-routine:
procedure PURGE
Input: tree/list with more than K nodes/entries, CK the K-th best c-value
Output: tree/list with redundant nodes/entries purged
Q +- LASTQ
WHILE c(Q) > CK DO
P +- BROTH(Q)
Delete Q from tree/list
Q+-P
ENDDO
To improve the effect of this purging-process it may be favourable to perform the
partitioning using the elements from the set F in such an order that Cil ;::: Cfl ;::: ... ;::: Ctq •
5 Customizing the K - best algorithm for solving con-
strained matching problems
A naive way to solve constrained matching problems would be to construct the sequence
of (K- )best matchings until the first time a feasible matching is obtained, i.e. a matching
which also fulfills the additional constraints. For the constrained shortest path problem
Handler and Zang [10] have developed a more efficient approach which constructs the set
of (K- )best solutions with respect to a pertubated objective function, thereby reducing
the number of solutions to be processed.
This approach can be generalized to other combinatorial optimization problems and in
Ball et al. [1] we have incorporated this technique as one module within a hybrid system
for solving constrained matching problems. In the following we want to describe how the
K-best-algorithm developed in section 4 can be customized for this purpose.
A constrained matching problem or a matching problem with n side-constraints (M P S)
is of the following form:
(MPS) ZMPS = min c'x s.t. (1), (2) and
a:x::; bi for i = 1, ... ,n
where ai E lR~J and bi E iR+, i = 1, ... ,n. Let a = (al, ... ,a n ) and b = (b1, ... ,bn ).
Each side-constraint can be thought of as a knapsack-constraint and may be representing
a budget constraint etc. Of particular interest are matching problems with so-called
generalized upper bound constraints (G UB-constraints) which are knapsack-constraints
with (0,1) coefficients. All edges with a coefficient 1 in a certain GUB-constraint can be
221
thought of bearing a certain "color" and the right-hand side b is limiting the number of
colored edges to be contained in the optimal matching. Note that for the problems that
we have discussed in Ball et al. [1] the number of side-constraints was rather small, i.e.
1 ::; n ::; 5.
For any A = (At, .. . , An) ;:::: 0 define c,,(x) = c'x + A(a'x - b). Then the problem
R(A) = min{c,\(x) I x fulfills (1) and (2) }
is called the A-relaxation of (M P S), and
(LD) ZLD = max{R(A) I >. ;:::: O}
is called the Lagrangean dual of (M P S). Now ZLD ::; ZMPS and often ZLD is a good
approximation for ZMPS. (LD) can be solved by standard techniques from non smooth
optimization (d. Ball et al. [1]).
It is a basic observation that when penalizing nonfeasible matchings, i.e. those for
which a~x > bi for some i, 1 ::; i ::; n, in the objective function, as it is done by c,\(x)
with >. ;:::: 0, the order of the matchings is changed such that when constructing the set
of K-best matchings with respect to c" instead of c, feasible matchings "occur earlier".
Yet the first feasible matching in the sequence need no longer be the optimal feasible
matching. Thus a different stopping criterion has to be introduced.
For this purpose when constructing the sequence of (J{- )best matchings with respect to
the pertubated objective function we also calculate the objective function value c(M) for
feasible matchings M occuring in the sequence and we record the value u as the minimum
over these feasible solutions, thus giving an upper bound for ZMPS.
Now the first time that we obtain a matching M(k) with C,,(M(k)) ;:::: u the procedure
can be terminated and the feasible matching giving the u-value is an optimal matching
for (MPS).
When applying the partitioning strategy described in section 4 to the above problem
the following modifications have to be implemented. Since the parameter J{ is not pre-
specified and hence the size of the tree and the length of the list, respectively, cannot be
predetermined the use of dynamic storage allocation techniques is necessary. The tree will
contain nodes (M PO,!) which are infeasible, i.e. nodes for which the set M o ,! although
not empty does not contain any feasible matching. A sufficient yet not necessary condi-
tion for a node (MPo,!) to be infeasible is that ai(I) > bi for one i, 1 ::; i ::; n. Those
nodes should be discarded from partitioning, yet they cannot be purged automatically
from the tree since they may contain necessary information for the reconstruction of the
sets 0 and I for other feasible nodes. To avoid partitioning on infeasible nodes (M PO,!)
222
we extend the list entry for every node by a flag, F LAG( M PO,I), with
FLAG(MPo I) := { TRUE
, FALSE
if (M Po,!) is infeasible
else
If the number of side-constraints is small, as it was in our application, we would also
introduce entries
Si(MPo,I):= ai(I) for i = 1, ... ,n
During the procedure we always partition on the best feasible unfathomed node. Once
that problem, (M PO,I) say, is selected and resolved and the associated set F offree edges
in the optimal matching is determined we partition on (M PO,I) in a special sequence:.
If there is only one side-constraint a/x :S b, we order F such that afl ;::: ah ;::: ... ;::: aJq-
When partitioning in that sequence we can stop the partitioning process for (M PO,I)
when for the first time
I>fl > b - a(I), 1::; r :S q
1=1
holds, since the associated subproblem 111 POU{fr+d,!U{fl, ... ,fr} as well as all following sub-
problems will not contain any feasible matching.
In the case of more than one side-constraint a similar heuristically determined sequence
of the elements should be applied. In any case, the first time the partitioning strategy
creates an infeasible subproblem the partitioning of (M Po,!) can be stopped.
During the partitioning step we use the optimal matchings obtained for the sub-
problems to update the u-value, i.e., the upper-bound, and the associated best feasible
solution. After having partitioned a problem (M Po,!) we would then try to purge sub-
problems from the list the c-values of which exceed the new u-value. This is done in a
similar fashion as refreshing was done in the usual K-best-algorithm when there were
more than K subproblems in the list/tree.
6 Final Remarks
We have shown that when applying the partitioning strategies proposed in literature for
finding J<-best solutions for combinatorial optimization problems to the matching problem
it is essential that not only properties of the matching problem but also special properties
of the algorithm which is used to solve the matching problems have to be taken into
account.
The J<-best implementation based on the shortest augmenting path method which
we have described here has been incorporated successfully as an enumerative phase in a
system for solving complex matching problems with side-constraints.
223
References
[1] Ball, M. 0., V. Derigs, C. I-lilbrand, and A. Metz. Matching problems with gener-
alized upper bound side-constraints. Networks, 20:703-721, 1990.
[2] Carraresi, P., and C. Sodini. A binary enumeration tree to find l{-shortest paths.
Methods of OR, 45:177-188, 1983.
[3] Chegireddy, C. R., and I-I. W. Hamacher. Algorithms for finding l{-best perfect
matchings. Discrete Applied Mathematics, 18:155-165, 1987.
[4] Derigs, V. A shortest augmenting path method for solving minimal perfect matching
problems. Networks, 11:379-390, 1981.
[5] Derigs, V. Postoptimal analysis for matching problems. Methods of OR, 49:215-221,
1985.
[6] Derigs, V. Some basic exchange properties in combinatorial optimization and their
application to constructing the [{-best solutions. Discrete Applied Mathematics,
11:129-141, 1985.
[7] Derigs, V. Programming in networks and graphs. Springer Lecture Notes in Eco-
nomics and Mathematical Systems 300, Heidelberg, 1988.
[8] Derigs, V., and A. Metz. On the use of optimal fractional matchings for solving the
Integer Matching Problem. Computing, 36:263-270, 1985.
[9] Edmonds, E. Maximum matching and a polyhedron with 0,1 vertices. J. Res. Natl.
Bur. Stand., 69B:125-130, 1965.
[10] Handler, G. Y., and 1. Zang. A dual algorithm for the constrained shortest path
problem. Networks, 10:293-310, 1980.
[11] Lawler, E. 1. Aprocedure for computing the [{-best solutions to discrete optimiza-
tion problems and its application to the shortest path problem. Management Sci.,
18:401-405, 1972.
[12] Murty, K. G. An algorithm for ranking all the assignments in order of increasing
cost. Opel'. Res., 16:682 - 687, 1968.
Solving Large Scale Multicommodity Networks
U sing Linear-Quadratic Penalty Functions
Mustafa Q. PIllar *
Stavros A. Zenios t
In this report we summarize current research towards the development of an algorithm for
the solution of very large multicommodity network flow problems (MCNFP). The algo-
rithm combines linear-quadratic penalty forms with a linearization technique that takes
advantage of the block structure of the constr<;tint set. The resultant procedure solves a se-
quence of smaller problems and can also be executed in parallel. Complete papers discuss
the algorithm, Zenios, Pinar and Dembo [1990], and its parallel implementation, Pinar
and Zenios [1990). We consider the multicommodity network flow problem (MCNFP)
with the following structure: commodities flow over a network such that the aggregate
flow on each arc does not exceed some joint capacity. Let 9 = {V, £} be a graph with
a set of vertices V = (m) (where the notation (m) represents the set {I, 2, ... , m}) and
a set of edges £ = {(i,j)1 i,j E V}, where 1£1 = n. Let (K) be the set of commodities
flowing on g. We will use the notation v'to denote the transpose of a vector or matrix
v.
We will use the following formulation:
[MCNFP)
mInImIZe f(x)
x
subject to Ax b
Ex < U
OS:xS:u
where:
f: 3(Kn --+ 3( is the cost function, assumed to be convex and at least twice continuously
differentiable,
• Department of Systems Engineering, University of Pennsylvania, Philadelphia, PA 19104.
t Department of Decision Sciences, University of Pennsylvania, Philadelphia, PA 19104.
NATO ASi Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgiil et al.
© Springer-Verlag Berlin Heidelberg 1992
226
x = (Xi.ik), (i,j) E £', k E (K) is the vector of arc flows, x E ~Kn,
E is the n x K n coupling constraint matrix, which consists of K identity (sub )matrices,
each of dimension n x n,
U = (Uij ), (i,j) E £, is the vector of length n of coupling arc capacities,
A is a block-diagonal matrix of dimension K m x K n with component submatrices Ak
along the diagonal,
Ak is the node-arc incidence matrix of dimension m x n for the flows of commodity k
on graph g. It is identical for each commodity k E (K),
b = (b ik ), i E V, k E (K) is the vector of dimension Km of supplies and demands for
each commodity,
U = (Uijk), (i,j) E £', k E (K) is the vector of dimension Kn of upper bounds on the
flows for each commodity on each arc.
The matrix E from the coupling constraint has a generalized upper bounding (GUB)
structure:
1 1 1
1 1 1
E=
1 1 1
By relaxing the coupling constraint, the problem decomposes into K independent sub-
problems (one for each commodity) since A has a block-angular structure
and the vectors x decomposes by commodity:
Each Xk has dimension n x 1 and is the vector of flows of commodity k. The vectors b
and U decompose similarly by commodity.
227
1 A Penalty Method Based on Linear-Quadratic
Penalty Functions
We consider the piecewise linear-quadratic penalty function:
tP(t) = { ~2<
t - ~
2
if t ~ 0
if 0 ~ t ~ E
if t 2:: E
Notice that tP is once continuously differentiable. Denote
Xo = {xlAx = b,O ~ x ~ u},
and
x = {xlAx = b,O ~ x ~ u,Ex ~ U},
and
<li(x) = L tPij((Ex - U)ij) V x E )RKn
(i,j)Ef
(1)
where (Ex - U)ij refers to the row of the vector Ex - U corresponding to arc (i,j). For
notational convenience we use tPij to denote the penalty term for each arc (i,j) where
tPij is defined as in (1) for all (i,j) E E. Define also the penalized non-linear program:
[PNLP]
minDJ.L,«x) = f(x) + fl<li(X)
xEX o
where fl is a positive real number. We consider the following algorithmic framework.
Algorithm: Linear-Quadratic Penalty Method for [MCNFP]
Step 0 Set k = 0, find xk E Xo and choose flk > 0 E > O. If xk E X stop. Otherwise
go to Step 1.
Step 1 Solve the problem [PNLP] starting with xk as the initial feasible solution. De-
note the optimal solution x*k.
Step 2 If <li(x*k) < 1] stop.
Else set
l+l f-- f3(l)
i+ 1 f-- 8(l)
k f-- k + 1, Go to Step 1.
228
The functions f3 and B are such that pk+! > pk and (Ok+! < (Ok. TJ is a final infeasibility
tolerance.
The convergence properties of the algorithm can be established within the framework
of Mangasarian [1986] or Bertsekas [1982]. The solution of [PNLP] in Step 1 is the
computationally expensive part of the algorithm. For reasons that will become clear
shortly we will be using the simplicial decomposition algorithm of Von Hohenbalken [1977]
as specialized for network problems by Ahlfeld et.al. [1990]. Simplicial decomposition
alternates between solving a linearized subproblem and a nonlinear master problem based
on the vertices generated from the solution of subproblems. The subproblem [SP] can be
formulated as:
Minimize z'\7D/L,,(x)
Z
subject to z E Xo
where \7D/LA x) E lRKn denotes the gradient vector of D/L,' at a point x. But we
observe that by the Cartesian product structure of X o , the subproblem can be solved
independently for each commodity k:
Minimize z~ \7 kD/L"(X)
Zk
subject to AkZk bk
0:::; Zk < Uk
where \7 kD/L,' (x) E lRn denotes the k-th block of the gradient vector corresponding to
commodity k. The subproblems can be solved simultaneously on multiple processors.
The solution of the master problem also involves extensive linear algebra computations
(see, Ahfeld et.al. [1990]) which can be executed very efficiently on vector and parallel
supercomputers. Details on the design and implementation of the algorithm can be found
in Zenios, Pinar and Dembo [1990]. Parallel computing designs are developed in Pinar
and Zenios [1990].
2 Numerical Results for a Large Scale Application
The algorithm was implemented in Fortran 77. It was run on a VAX 6400 and a CRAY
Y -MP. The test problems were obtained from a Military Airlift Command application.
They are referred to as the Patient Distribution System (PDS) problems and are used
to make decisions on the evacuation of patients from Europe. The PDS problems are a
class of problems; PDSt denoting a problem that models a scenario of t days. They are
linear multicommodity network problems with eleven commodities. The PDS problems
229
received considerable attention and were studied by several researchers, see for instance
Carolan et.al. [1990], Meyer and Schultz [1990], Marsten et.al. [1990], Setiono [1989].
We report results with a subset of PDS problems. The characteristics of the problems
are given in Table 1. We present in Table 2 the results illustrating the performance
of the penalty algorithm on PDS problems. All runs were made on a CRAY Y-MP
with vectorization but no parallel computing. Finally, Table 3 gives a comparison of the
performances of various methods reported in the literature.
Test problem No. of arcs No. of nodes No. of rows No. of columns
PDS1 339 126 1473 3816
PDS3 1117 390 4593 12590
PDS5 2149 686 7546 23639
PDS10 4433 1399 15389 48763
PDS15 7203 2125 23375 79233
PDS20 10116 2447 31427 105728
Table 1: Test problem characteristics.
Test Problem Simpl. No. of Subproblem Master Total
iters vertices time time time
PDS1 26 18 1.11 2.34 3.45
PDS3 39 31 1.10 12.87 23.97
PDS5 68 55 52.37 54.37 106.74
PDS10 101 83 226.29 249.11 515.4
PDS15 130 105 571.51 590.01 1161.52
PDS20 136 111 1271.54 851.85 2123.39
Table 2: Solution times of PDS problems on the CRAY Y-MP (in CPU seconds).
3 Conclusions
We presented an algorithm based on linear-quadratic penalty functions. The computa-
tional experience shows that it outperforms significantly other methods including imple-
mentations of Karmarkar interior point algorithm and is very competitive with the most
successful algorithm due to Meyer and Schultz [1990]. We finally note that the algorithm
can be extended to solve other types of mathematical programs with block structure and
general side conditions.
230
Test Problem Carolan et.al. Meyer & Schultz Setiono Marsten et.al. Pinar & Zenia
PDSI 0.008 9.58 x 10-
PDS3 0.078 6.65 x 10-
PDS5 0.133 0.59 0.02
PDSI0 3.3 0.291 4.862 0.422 0.14
PDS15 0.32
PDS20 24 1 4.436 0.58
Table 3: Comparative solution times of PDS problems with various methods in CPU hrs.
References
[1] D.P. Ahlfeld, J.M. Mulvey and S.A. Zenios, Simplicial Decomposition for Convex
Generalized Networks, Journal of Information and Optimization Sciences, to ap-
pear, 1990.
[2] D.P. Bertsekas, Constrained Optimization and Lagrange Multiplier Methods, Aca-
demic Press, New York, 1982.
[3] W.J. Carolan, J.E. Hill. J.L. Kennington, S. Niemi and S.J. Wichmann, An Empir-
ical Evaluation of the KORBX Algorithms for Military Airlift Applications. Oper-
ations Research, Vol 38, No.2, March-April 1990, p. 240-248.
[4] O.L. Mangasarian, Some Applications of Penalty Functions in Mathematical PTO-
gramming, Lecture Notes in Mathematics Vol 1190, "Optimization and Related
Fields", R. Conti, E. De Giorgi and F. Giannessi, editors, Springer-Verlag, Berlin
1986, p. 307-329. ~
[5] R. Marsten, R. Subramanian, M. Saltzman, 1. Lustig and D. Shanno, Interior Point
Methods for Linear Programming, Interfaces Vol 20:4, July-August 1990, p. 105-
116.
[6] R.R. Meyer and G.L. Schultz, A Structured Interior Point Method for Block An-
gular Optimization, Technical Report No. 934, Department of Computer Science,
University of Wisconsin-Madison, 1990.
[7] M.C. Pinar and S.A. Zenios, Vector and Parallel Computing with a Multicommodity
Network Algorithm, Working Paper, Department of Decision Sciences, The Wharton
School, University of Pennsylvania, 1990.
[8] R. Setiono, Dual Proximal Point Methods for Linear Programming, Technical Re-
port, Department of Computer Science, University of Wisconsin-Madison, 1989.
[9] B. Von Hohenbalken, Simplicial Decomposition in Nonlinear Programming Algo-
rithms, Mathematical Programming, VoLl3, p. 49-68, 1977.
[10] S.A. Zenios, M.C. Pinar and R.S. Dembo, Linear-Quadratic Penalty Functions f01'
the Solution of Multicommodity Network Flow Problems, Working Paper, Depart-
ment of Decision Sciences, The Wharton School, University of Pennsylvania, 1990.
An Analysis of the Minimal Spanning Tree
Structure
Elzbieta Trybus *
In this paper the minimal spanning tree (M ST) structure is analyzed by means of the
distance random variable and Wroclaw Taxonomy algorithm [2].
The classical M ST problem is to connect n cities by the shortest path, and the main
assumption made is that nodes are fixed points in R2 plane. During the last twenty
years, a probabilistic M ST has been studied where either nodes are random or edges are
random. For example, Karp [2] assumes that cities are points in a p-dimensional Euclidean
space and their locations are drawn independently from the uniform distribution over the
,
p-dimensional cube. He proves that the NP-hard traveling salesman problem (TSP) is
easily solvable in an approximate and probabilistic sense.
Here, two separate cases are considered: the M ST on nodes generated from a bi-
variate uniform distribution, and the M ST on nodes generated from a bivariate normal
distribution.
Consider an undirected graph G = (X, E), where X = Xl, X2, ••. , Xn represents the
set of nodes and E represents the set of edges. Associated with each edge (Xi, Xj) E E
is a nonnegative cost (or distance) d(Xi,Xj). Any connected subgraph of graph G with
no cycles is called a spanning tree. The minimal spanning tree (M ST) is a tree in G
such that the sum of the edge lengths in the tree is a minimum. The total number of all
possible connections between n points is n(n - 1)/2, and only n - 1 edges belong to the
MST.
There are many algorithms leading to the same M ST when the set of nodes is fixed.
In the M ST algorithm written by Kruskal [3], all edges are arranged into an increasing
order, and only thosethat connect Xi to its closest neighbors and do not create cycles in a
graph are edges of the M ST. Prim's algorithm [4] is based on an iterative process where
at each step, a spanning subtree is constructed by adding a new edge to the existing tree
in such a way that the resulting tree has the minimal length .
• California State University, Northridge, CA 91330.
NATO AS! Series, Yol. F 82
Combinatorial Optimization
Edited by M. Akgtil et al.
© Springer·Yerlag Berlin Heidelberg 1992
232
In the method called Wroclaw Taxonomy, see Florek, K. et al. [1], the following
algorithm is used.
Step 1. Connect each node Xi E X with its nearest neighbor node, e.g. Xj EX - {Xi}.
This step will generate at least n/2 edges which form no more than n/2 subtrees. If only
one subtree is generated, then it is the MST.
Let ViI, Vi2, ... , Yinl' be the set of subtrees of set X where ViI U Vi2 U ... Vin, = X,
and Vii n Vik = 0 for 1 :::; j :::; nl, and 1 :::; k :::; nl, andj =F k, and nl :::; n/2.
Step 2. For each subset Vii(l :::; i :::; nl) find the nearest subset Vij to it and connect the
two nodes for which the distance between the subsets is minimal.
This step will add at least nl/2 edges to the MST and the total number of subtrees n2
will be less than or equal to nd2. If all nodes of set X are connected, then the resulting
tree is the M ST of the set X, otherwise Step 2 needs to be repeated for the subsets
Vkl , Vk2 , ... , Vknk (k = 2,3, ... :::; log2 n) until all nodes of X are connected.
Now, we will assume that the nodes X6,XI , ... ,Xn of an MST are independent p-
dimensional random variables with the same probability density f(·), and Yi, 12,· .. , Yn
are vectors defined in the following way
1j =11 XO,Xj II,
where \I . II is the Euclidean distance in Rp. By rearranging Yi, Y2, . .. , Yn in increasing
order we obtain the sequence DI :::; D2 :::; ... :::; Dn.
Definition: Dk is the kth order distance variable [5}.
Here, we define the expected value of the length of an MST, E(L(MST)), as a
linear combination of the expected values E(DI)' E(D2)' ... ' E(Dko) and the unknown
nonnegative integer numbers TJI, TJ2, ... , TJko.
ko
E(L(MST)) = L TJkE(Dk)'
k=l
where: ko :::; n/2, and n/2 :::; TJl < n, and 0 :::; TJk(k = 2,3, ... , ko), and L~~l TJk = n - 1.
By taking the coefficients TJk generated by Wroclaw Taxonomy algorithm and using E(Zk)
instead of E(Dk)' we define the following measure
ko ko
L*(MST) = L 2k: 1/ 2E(Zk) = n 1/ 2 LT k E(Zk) (ko = log2n),
k=l k=l
which can be used as an estimate of E(L(MST)).
Because in the bivariate uniform distribution on [0,1]2 the first moment of Zk is:
E(Z ) = Tl -1/2 ~ fU - 1/2)
k 7r 6 f(·)
3=1 J
(k = 1,2, ... ),
233
we have k
L*(MST) = Vn LTk E(Zk) ~ y'n (c = 0.71).
j=1
The normal distribution is under investigation now. For example, if f(x, y) = (27r tIe _~22-y2
then E(Zk) of the first few random variables Zk are: E(ZI) = (27r)1/2, E(Z2) = 1.5E(ZI)
and E(Z3) = 1.875E(ZI).
The two above distributions are considered in our Monte Carlo experiments: the
uniform distribution on [0,1]2 and the bivariate normal distribution with different location
parameters and different correlation coefficients. The program written in Pascal calculates
the length L(M ST) of the M ST and the frequencies 7]1, 7]2, ... ,7] k o of the first order,
second order, ... and the k~h order edges respectively.
Since any edge (Xi, X j) connects two nodes Xi and X j, the program counts the lowest
order edge, e.g. if the order of edge (Xi, Xj) is r for node Xi and s for node Xj, then the
order of the edge is k = min( r, s).
Analyzing the simulation results summarized in Table 1 and Table 2, we conclude that
the proportion of 7]1,7]2, ... , 7]k of the distances d1 , d2 , • •• , dk in an M ST does not depend
on the probability distribution of its nodes. But, the average distance depends on the
probability distribution.
Empirical distributions
n 7]1 7]2 7]3 7]4 7]5 7]6 L(MST) L*(MST)
10 8 0 1 1.8400 1.7986
25 17 6 0 1 3.3819 3.2000
50 36 5 4 2 2 4.8265 4.7542
100 74 14 6 3 1 1 6.8508 6.8899
250 167 62 19 1 10.4676 11.0365
500 331 127 31 8 2 14.7704 15.7080
1000 689 221 61 25 3 20.8907 22.2861
Table 1: Bivariate uniform distribution on [0,1]2.
234
Empirical distributions
n 'f/1 'f/2 'f/3 'f/4 'f/s 'rJ6 L(MST)
10 8 1 5.82755
25 19 2 2 1 1 10.5230
50 37 8 3 0 1 16.2485
100 71 19 8 0 0 1 27.2729
250 172 61 13 2 1 42.4054
500 347 107 38 6 4 62.7728
1000 687 222 73 13 3 92.7595
Table 2: Bivariate normal distribution(Ex = Ey = 0, ox = Oy = 1, P = 0).
References
[1] Florek, K., 1. Lukasiewicz, J. Perkal, and H. Steinhaus. Sur la liaison et la Division
des Point d'un Ensemble Fini. Colloq. Math, 2:282-285, 1951.
[2] Karp, R. M. The probabilistic analysis of some combinatorial search algorithms. In
Algorithms and Complexity: New Directions and Recent Results, J. F. Traub(Ed.),
1-19. Academic Press, New York, 1976.
[3] Kruskal, J. B. On the Shortest Spanning Subtree of a Graph and the Traveling
Salesman Problem. In Proceedings of the American Mathematical Society, 7:48-50,
1956.
[4] Prim, R. C. Shortest Connection Matrix and some generalizations. Bell Systems
Technical Journal, 36:1389-1401, 1957.
[5] Trybus, G. Zastosowania Matematyki, 14:237-244, 1974.
Genetic Algorithms: A New Approach to the
Timetable Problem
Alberto Colorni, Marco Dorigo and Vittorio Maniezzo *
In this paper we present the results of a research relative to the ascertainment of limits
and potentials of genetic algorithms [4, 3, 6] in addressing highly constrained optimization
problems, where a minimal change to a feasible solution is very likely to yield an infeasible
one. As a test problem, we have chosen the timetable problem (TTP), a problem that is
known to be NP-hard [5], which has been intensively investigated for its practical relevance
[2,1].
The problem instance we faced was the construction of a class timetable for an Italian
highschool. The problem may be decomposed into formulation of several interrelated
timetables, one for each pair of sections of the school considered. A pair of sections can
be in fact processed as an "atomic unit," not further decomposable given its high internal
dependencies, but relatively isolated from the other pairs of sections.
Given these premises, the problem is described by:
• a list of the teachers of the pair of sections (20 in our case),
• a list of the classes involved (10 for the pair of sections),
• a list of the weekly teaching hours for each class(30),
• a curriculum of each class, that is the list of the frequencies of the teachers working
in that class,
• some external conditions, for example the hours that the teachers use to teach in
other sections.
The formal representation of the TTP is as follows: given the 5-tuple< H, T, A, R, f >
where
T is a finite set {Tb T 2 , ••• , T;, ... , Tm} of resources (teachers),
'Politecnico di Milano, Dipartimento di Elettronica, Milano, Italy.
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgiil et al.
© Springer-Verlag Berlin Heidelberg 1992
236
H is a finite set {Hi, H2' ... ' Hj, ... , Hn} of time intervals (hours),
A is a set of jobs to be accomplished (lessons to be taught),
R is an m * n matrix of rij E A (a time table),
f is an objective function to be maximized, f : R ~ JR,
We want to compute max f( (1,.0., n, II) where
(1 is the number of superimposition, that is the situations where more that one teacher
is present in the same class at the same time. (this is controlled by a parameter
that may be different than zero for reasons of efficiency),
.0. is the set of didactic goals (e.g., having the hours of the same subject spread over the
whole week),
n is a set of organizational goals (e.g., for each hour of the week having two teachers
available for possible temporary teaching posts),
II is a set of personal goals (such as for each teacher to have a class-free day).
Every solution (timetable) generated by our algorithm is feasible if it satisfies the
following constraints:
• Every teacher and every class must be present in the timetable in a predefined
number of hours.
• There may not be more than one teacher in the same class in the same hour.
• No teacher can be present in two different classes in the same hour.
• There is no "uncovered hours" ( that is, hours when no teacher has been assigned
to a class).
The approach we followed for the solution of this problem is the following. We decided
to choose as the alphabet, the set A of the jobs that the teachers have to perform whose
elements include the lessons to be taught and other activities. This alphabet allows us
to represent the pr<?blem as a matrix R (an m * n matrix of rij E A) where each row
corresponds to a teacher and each column to an hour. Every element rij of the matrix R
is a gene: its allelic value may vary on the subset of A specific to the teacher corresponding
to the row containing the gene. The constraints are managed as follows:
237
(i) by the genetic operators, so that the set of hours to be taught by each teacher,
allocated in the initialization phase, can not be changed by the application of the
genetic operators (which have been specifically redefined for this purpose);
(ii) by a filtering algorithm, so that the superimpositions caused by the application of
the genetic operators be totally or partially eliminated by filtering;
(iii) by the objective function, so that the selective pressure is used to limit the number
of individuals with infeasibilities.
We decided to manage the impositions by means of both filtering (the so called genetic
repair [7] and fitness function penalties to gain a greater degree of freedom in moving
through the search space. This choice has been caused by the difficulty of the problem.
In our application, in fact, every teacher represents a TSP-like problem ( consisting of the
analysis of the permutations of a predefined symbol set).
The objective function, which provides the genetic algorithm with feedback from the
environment, is structured hierarchically and is built so as to measure a generalized cost,
which represents the distance existing between the current timetable and the needs of the
school. The hierarchical structure has been chosen in order to acknowledge the different
relevance of several groups of problem conditions. The difference is reflected in the weights,
which have different orders of magnitudes. In our application, the following structure has
been chosen:
level 1: feasibility conditions (weight values ~ 1000);
level 2: global management conditions (weight values ~ 100);
level 3: single teacher conditions(weight values ~ 10).
At levell, we handle possible superimpositions of teachers (two or more teachers in
the same classroom at the same hour) and uncovered hours for the classes (hours when a
class is not covered by any teacher). At level 2, we consider the didactic, organizational
and teacher requirement topologies. At level 3, we consider the preferences expressed
by each teacher for his/her specific timetable. Each teacher assesses his/her personal
requirements. These assessments are then normalized so that each teacher takes part
with a specific quota to the determination of the weight regarding the requirements of the
whole teaching staff (a value specified at level 2). The operators used are the following:
Reproduction: This is the classical reproduction operator that promotes individuals
with an above average value of the fitness function.
238
Mutation of order k: This operator takes k contiguous genes and swaps them with
other k contiguous nonoverlapping ones belonging to the same row. This operator
cannot be applied when among the genes to be mutated, there are some special
characters that have been allocated during the initialization phase to unconvertible
acti vi ties.
Day mutation: This operator takes one day and swaps it with another day of the same
row. The ith day, in a teacher timetable, is a substring containing genes that codify
five contiguous hours, from the first to the fifth of one day. It is a.special case of
mutation of order k. This operator has been introduced for efficiency reasons (with
special reference to free-day allocation).
Crossover: The objective in defining this operator is that of recombining building blocks
efficiently so that given two parents, it is possible to generate two sons having better
fitness function (f.f.) values (or at least one of them with a significant increase of
f.f. value). the building blocks are computed on the basis of a local fitness function
(1.£.£), i.e. the part of the fitness function due only to characteristics specific to each
teacher.
Filtering: The filtering operator takes as input an infeasible solution and returns as out-
put a feasible one. It is possible to distinguish between two kinds of constraints:
rows and columns. Row constraints are incorporated in the genetic operators and
are, therefore, always satisfied; column constraints (infeasibilities due to superim-
positions) are managed by means of a combination of fitness function and genetic
repair. The goal of filtering will, therefore, be that of recovering column, hence
global, feasibility for any given timetable.
The algorithm has been coded in C language on an IBM-PC with standard config-
uration. The model and the program have been tested in defining the timetable for a
large highschool in Milan. We have cooperated with the teachers that usually define
the timetable. This cooperation has been continuous from the design phase until the
validation phase which is still going on with promising results.
The features of our model that can be generalized are:
• The definition of genetic operators that minimize generalized cost functions (which
penalize the possible infeasibility of the generated solution)
• The distribution over genetic operators, fitness function and genetic repair for the
management of the infeasibilities
239
• The hierarchical structuring (in our case on three levels) of the U. in order to
allow an easy and effective definition of the relevance of the different criteria and
objectives used
• The run-time adaptation of f.f. weights.
With reference to specific applications, we have, moreover, defined a menu-based inter-
face that allows the user to interactively build an objective function suited to his specific
needs and a filtering algorithm capable of recovering the infeasibilities of a timetable and
converting it into a feasible one that is near the first according to a specific metric.
References
[1] Chahal, N., and D. deWerra. An interactive system for constructing timetables on
a PC. European Journal of Operational Research, 40(1),1989.
[2] Davis, L., and F. Ritter. Schedule optimization with probabilistic search. Proc. 3rd
IEEE Conference on Artificial Intelligence Applications, Orlando, Florida, February
1987.
[3] De Jong, K. A.,and W. M. Spears. Using genetic algorithms to solve NP-complete
problems. Proc. 3rd Int. Conference on Genetic Algorithms and Their Applications,
George Mason University, June 1989.
[4] M. Dorigo. Genetic algorithms:The state of the art and some research proposal.
Technical Report No. 89-058, Polytecnico di Milano, Italy, 1989.
[5] Even, S., A. Itai, and A. Shamir. On the complexity of timetable and multi com-
modity flow problems. SIAM Journal of Computing, 5(4), December 1976.
[6] D. E. Goldberg. Genetic Algorithms in Search, Optimization and Machine Learning,
Addison-Wesley, 1989.
[7] H. Miihlenbein. Parallel genetic algorithms, population genetics and combinatorial
optimization. Proc. 3rd Int. Conference on Genetic Algorithms an Their Applica-
tion, George Mason University, June 1989.
A New Approximation Technique for Hypergraph
Partitioning Problem
Scott W. Hadley *
The partitioning of the nodes of a hypergraph arises in many different design/layout
applications. In particular, the netlist partitioning problem in VLSI design, c.f. [2, 5, 9],
can be represented as a hypergraph partitioning problem. The modules in the netlist
correspond to nodes of the hypergraph and the nets correspond to the hyperedges.
The netlist partitioning problem can be described as follows: Given a hypergraph
(netlist) H, partition the n nodes into I< distinct subsets Vi, V;, ... , VK of cardinality
mI, m2, ... , mK, respectively, such that the number of hyperedges with nodes in more
than one subset is minimized. This problem is known to be NP-hard [6], even in the case
when H is a graph.
The most common heuristics for netlist partitioning are called interchange heuristics,
c.f. [4, 7, 10]. These heuristics are based on starting with an initial partition of the
nodes and obtaining better partitions by moving or interchanging a node (or set of nodes)
between subsets until no further moves generate an improvement. The quality of the final
solution often depends on the quality of the intitial partition.
In the case where H is a graph, Barnes [1] efficiently finds an intital partition that is
based on the eigenvalues of the incidence matrix of H. It has been empirically verified,
c.f. [3], that the partition obtained by this heuristic usually places most of the nodes in
the correct subsets. The purpose of this paper is to obtain a graph approximation of the
hypergraph H so that we can use the technique of Barnes to obtain initial partitions.
Vannelli and Hadley [11] introduced a hypergraph approximation technique based
on underestimating cuts in H. We discuss under what conditions it is useful to obtain
an underestimation and observe that in many cases finding an underestimation has no
advantages over finding a general approximation.
Let CG(P) denote the combined weight of the edges of G that are cut by partition
P. Let CH(P) denote the number of the hyperedges of H that are cut by P. The set of
'Universty of Waterloo, Department of Electrical and Computer Engineering, Waterloo, Ontario,
Canada N2L 3G 1.
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgtil et al.
© Springer-Verlag Berlin Heidelberg 1992
242
partitions satisfying a specified property will be denoted by F.
Consider the following optimization problem
(1)
(i.e. minimize the number of hyperedges cut in H over all partitions in F), and the related
problem
min{CG{P) : P E F}, (2)
(i.e. minimize the weight of cut edges in G over all partitions in F).
If we can solve (2) exactly and if G underestimates H then the optimal solution to
(2) provides a lower bound for (1). This lower bound can be very helpful in determining
the quality of any solution to (I). This was the motivation for the graph approximation
introduced by Vannelli and Hadley [11]. Unfortunately, if we can not solve (2) exactly, the
bounding property of a heuristic solution does not hold. Since many partitioning problems
are NP-hard even in the graph case, we usually can not expect to solve (2). In these cases,
we may wish to find the best approximation rather than the best underestimation as was
done in [11, 12].
We introduce a new technique to obtain weights for the edges of G. The graph obtained
is the best l2-approximation to H.
As in [11, 12], the edge set of G is obtained by considering each hyperedge in turn
and generating the clique on the nodes of the hyperedge. We then assign weights to each
edge in the clique. Notice that when we are considering only one hyperedge at a time,
(and so only a subset of the nodes of H), a k-partition of the nodes of H may correspond
to at-partition, t :::; k, of the nodes of the clique. After considering all hyperedges, we
have generated a weighted graph which, in general, has multiple edges. We obtain G
by replacing each set of multiple edges by a single edge whose weight is the sum of the
weights of all edges in the set.
Since cuts in G should approximate cuts in H, we want CG(P) = CH{P) for all P E F.
If we consider each hyperedge in turn, this implies we want to assign weights to the edges
of the corresponding clique, say G' , such that CGI(P) = 1 if P cuts the hyperedge under
consideration, -and CGI(P) = 0 if P does not cut the hyperedge. Obviously, if P does not
cut the hyperedge then P does not cut any edges of G' , i.e. CGI(P) = 0 We assume the
hyperedge under consideration contains n nodes. So, ideally, we wish to find a solution
to the following linear system
LS CGI(P) = 1 for all P E F. (3)
In general, LS will not have an exact solution. The best approxiamtion to a solution in
the lrsense is the least squares solution of LS.
243
If we let X(i-I}n+j denote the weight assigned to edge (i,j) in G' , and A = (a uv ) denote
the matrix where a uv = 1 if edge v is cut by partition u, (a uv = a otherwise), then LS
can be written as:
Ax = 1, (4)
where 1 denotes the vector of ones (of the appropriate dimension). The least squares
solution to (4) is given by the normal equations
(5)
or equivalently
It is interesting to note that the matrix A defined above is well defined given n, (the
number of nodes in the hyperedge), and the class of partitions, P, that we are considering.
We derive a closed form characterization of the solution of (5). One should note
that matrix A grows very quickly, e.g. in the case n = 12, k = 4 matrix A will have over
700, 000 rows, necessitating an efficient method for solving (5) without explicitly obtaining
matrix A.
This approximation technique can be extended to approximate more than one hyper-
edge at a time. Assume we wish to approximate the set GE = {gl, ... ,gj} of hyperedges.
We do this by generating the clique on all nodes incident with at least one hyperedge in
G E. In this case, (4) becomes
Ax = b, (6)
where bi (the ith entry of b) denotes the number of hyperedges of G E that are cut by
partition i. Again, since the number of rows of A (and b) gets very large for even small
examples, it is important to find an efficient way to solve the least squares problem
Qbtained from (6). The solution is given by the solution to the linear system
(7)
We give a characterization of AtA and Atb thus enabling an efficient solution to the linear
system (7).
Preliminary testing indicates that, in general, the intitial partitions obtained using
this new approximation lead to better partitions than those obtained using the underes-
timation approximation found in [11, 12].
244
References
[1] E. R. Barnes. An algorithm for partitioning the nodes of a graph. SIAM J. on
Algebraic and Discrete Methods, 3(4):541-550, 1982.
[2] Brayton, R. K., G. Hactel, K. Mullen, and A. Sangiovanni-Vincentelli. Logic Min-
imization Algorithms for VLSI Synthesis, Kluwer Academic Publishers, Boston,
1984.
[3] Y. Chi. On partitioning and allocation problems for multiprocessor simulation.
unpublished M.A.Sc. thesis, Department of Electrical and Computer Engineering,
University of Waterloo, Ontario, 1989.
[4] Fiduccia, C. M., and R. M. Mattheyses. A linear-time heuristic for improving
network partitions. 19th Design Automation Conference, 175-181, 1982.
[5] Friedman A. D., and P. R. Menon. Theory and Design of Switching Circuits, Bell
Telephone Laboratories and Computer Sciences Press, Rockville, MD, 1975.
[6] Garey, M. R., and D. S. Johnson. Computers and Intractability, Freeman, San
Francisco, CA., 1979.
[7] Kernighan, B. W., and S. Lin. An efficient heuristic procedure for partitioning
graphs. Bell Syst. Tech. J., 49(2):291-307, 1970.
[8] Kusiak, A., A. Vanelli, and K. R. Kumar. Clustering analysis: Models and algo-
rithms. Control and Cybernetics, 15(2):139-154, 1986.
[9] Russo, R. 1., P. H. Oden, and P. K. Wolf, Sr. A heuristic procedure for the parti-
tioning and mapping of computer logic blocks to modules. IEEE Trans. Comput.,
C-20:1455-1462,1971.
[10] 1. A. Sanchis. Multiple-way network partitioning. IEEE Trans. on Comput.,
38(1 ):62-81, 1989.
[11] Vanelli, A., and S. W. Hadley. "A Gomory-Hu cut tree approach for partitioning
netlist". IEEE Trans. on Circuits and Systems, to appear July, 1990.
[12] Vanelli, A., and S. W. Hadley. "An efficient procedure for partitioning the nodes of
a hypergraph". Under review SIAM J. on Discrete Mathematics, Jan., 1990.
Optimal Location of Concentrators in a
Centralized Teleprocessing N etwor k
M.G. de Oliveira *
Centralized networks with a tree structure generally arise in the context of the design of
local access networks, which are linked to a large distributed netwo~k at gateway backbone
nodes. In some cases, however, even large-scale networks are centralized systems. The
terminals are linked together in groups sharing a multidrop line and connected to the
host computer or to the backbone switch through a concentrator. By assuming that the
terminals are identical in terms of the amount of data flow they may generate, line and
concentrator capacities may then be expressed as the maximum number of terminals they
can accommodate. The problem to solve consists of determining the optimal number and
location of the concentrators and of linking the facilities in tree hierarchical structure at
minimum cost, without exceeding line and concentrator capacities.
The complexity and dimension of the problem rule out a true optimization method
based on a global model. A heuristic approach is followed instead, comprising three
stages: a) a terminal clustering procedure, which aglomerates and partitions the large
number of terminals into clusters weighted by the number of terminals they represent
and ideally "located" at the respective centers of mass COM's; b) a concentrator location
problem to determine the optimal number and locations of the concentrators from a given
set of potential concentration nodes; and c) a terminal layout problem to decide for each
partition, how the terminals are actually linked together. Our contribution in this paper
is to propose both an exact and an approximate method for the concentrator location
problem, which we claim is an attractive alternative to the commonly used ADD and
DROP heuristics.
The network representation for the concentrator location problem is as shown in the
figure. The nodes represent the facilities, and the edges represent the links connecting
them.
In the node set of the directed graph N = {s, W, T}, node s represents the host
of central computer, the set of intermediate nodes W = {WI,.'" Wi, ... , W m } represent
*Departament de Matematica, Fac. Ciencias e Tecnologia, Universidade Nova de Lisboa.
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgiil et al.
© Springer-Verlag Berlin Heidelberg 1992
246
[-d,l
the potential concentration points and those in set T = {t l , . .. ,tj, . .. ,t n } stand for the
clusters of terminals, as determined by the clustering algorithm. The network design
model formulation for the concentrator location problem is the following:
P: minimize z = cx + f Y (1)
subject to: Ax b (2)
0 < x5:.h (3)
0 < xw 5:. hw (4)
(hW)i hsiYi (5)
A is the node-arc incidence matix of the directed graph. c, x and h are the vectors of
line costs, flows and capacities respectively. The vector b, associated with node set N has
entries of D, 0 and -d j corresponding to the subsets {s}, Wand T respectively. dj is
the number of terminals in cluster j and D is the total number of terminals. The design
feature of the network is accounted for by the presence in the model of the vectors of
binary variables Y and fixed costs f associated with the set liV of potential concentrator
sites.
For the given set of m potential concentration points, the number of possible concen-
trator sites combinations is 2m . Our method basically consists of a cycle of simplification
tests embedded in a Branch and Bound (B & B) procedure, by which an attempt is made
to fix the binary variables to one of their feasible values with the aim of reducing the
size of the B & B tree to consider. At each node of the B & B tree and for any feasible
247
combination of the binary variables, the subproblem to solve is a Minimum Cost Network
Flow problem (M C N F). For the calculation of the parameters used in the simplification
tests, a set of M C N F problems is solved. However, in this case, a problem generally
differs from the previously solved one by only a few changes in the cost vector, and opti-
mality is easily recovered after a few primal simplex iterations. Obviously, a network flow
code having the reoptimizing capability is vital to this kind of approach. We used the
RNET system of FORTRAN subroutines by Grigoriadis and Hsu. These considerations
also suggests the use of the LIFO (Last-in-Firts-Out) rule for selecting the node of the B
& B tree to be processed next.
Some notation is needed. N[S) denotes the MCNF problem for subset SeW, and
z[S) is the optimal value for that problem. However, throughout the procedure, the graph
structure remains unchanged. To "exclude" a node Wi from set vl', the cost of arc(s, Wi)
is set to a relatively high value. After reoptimization, the associated variables obviously
remain non basic at zero level. Let L be the index set of the binary variables. At the
node of the B & B tree considered, set L is partitioned into three subsets:
L1 = {i E L : Yi = I}; Lo = {i E L : Yi = O}; L2 = L - L1 U Lo.
The cycle of simplifications consists of four tests:
Sl: i E L2, calculate 8(i) = Z[L1 U L2 - {i}). If 8(i) ~ Ii, then Yi = l.
S2: Let !£ be a lower bound on the best solution that can be obtained from the current
node of the B & B tree and calculated as follows:
!£ = Z[L1 U L2) + F[L1), where F[L1J = 2:iELl Ii; and let z* be the value of the best
solution found so far. If!£ + Ii ~ z* then Yi = o.
S3: i E L2, calculate a(i) = Z[L1)- Z[L1 U {i}J. If a(i):::; Ii, then Yi = o.
S4: Let i be also a lower bound calculated as follows:
i = Z[L1J + F[L1J- A[L2) + F[L2), where A[L2J = 2:iEL2 a(i). If i + a(i) - Ii ~ z',
then Yi = l.
The cycle of simplification tests is repeated iteratively and it terminates as soon as
no more variables can be fixed to 1 or 0 by the tests. The branching decision variable is
then selected and for that purpose the following rule is adopted: Select the free variable
Yk such that:
a(k) - Ik = maxiEL2{a(i) - j;}.
In the optimization method described above, a branching decision variable is chosen
each time the cycle of simplificati0J?- tests terminates. With all the other things being
equal, if only the open branch is processed and the closed branch is omitted, a heuristic
procedure results which can be seen as a combination of the ADD and DROP heuristics.
248
Since the values of the 8(i)'s are non decreasing, and those of the o:(i)'s are non increasing,
the solution obtained by the method is clearly suboptimal. Thus, for large instances of
the model we have an approximate procedure, which becomes an attractive alternative to
the other heuristics for the concentrator location problem, from the point of view of both
computing time and the results obtained.
A Column Generation Algorithm for the Vehicle
Routing Problem with Time Windows
Martin Desrochers *
Jacques Desrosiers t
Marius Solomon t
Let G = (N, A) be a network where A is the set of arcs or route segments and N
is the set of nodes or customers. Associated with each arc (i,j) E A there is a cost Cij
and a duration tij. We assume that the service time of customer i is included in the
duration of each arc (i,j). In this paper, the cost is taken to be the distance between i
and j. The vehicle routing problem with time windows (VRPTW) involves the design of
a set of minimum cost routes originating and terminating at a central depot, d, for a fleet
of vehicles which services a set of customers with known demands qi. Each customer is
serviced exactly once. The service of a customer, involving pick-up (delivery) of goods or
services, can begin at Ti within the time window defined by the earliest time, ai, and the
latest time, bi , when the customer will permit the start of service. If a vehicle arrives at a
customer too early, it will wait. In addition, due dates cannot be violated. Furthermore,
all the customers must be assigned to vehicles such that the vehicle capacities, Q, are not
exceeded.
The VRPTW can be formulated as a set partitioning problem. However, for im-
plementation reasons, we are not going to use this model directly. Instead, for column
generation we are going to use a set covering type model as the subproblem can generate
routes containing cycles. Furthermore, the linear relaxation of the set covering type model
is numerically far more stable than that of the set partitioning model.
To introduce the set covering type model, let now 'ir be a constant taking an integer
value if route r E R visits customer i E N\ {d} and 0 otherwise. The constant 'ir indicates
that a customer i can be visited more than once by route r. Let also Cr to be the cost
of route r and take Xr as a binary variable equaling 1 if route r is used and 0 otherwise.
*GERAD and Ecole Poly technique, Montreal, Canada
tGERAD and Ecole des Hautes Etudes Commerciales, Montreal,Canada
tNortheastern University, Boston, Massachusetts, U.S.A.
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgtil et al.
© Springer-Verlag Berlin Heidelberg 1992
250
Finally, let Xd and Xc be two additional integer variables. The variable Xd is defined as
the number of routes, while the variable Xc represents the total distance traveled. Note
that Xc is integer only if Cr is integer for all r, r E R. We satisfy this condition by taking
Cij to be integer for (i, j) E A. We can now present our set covering type model formally.
The mathematical formulation is:
minimize LCrXr (1)
rER
subject to: L rirXr > 1 (Vi E N\ {d}) (2)
rER
LXr -Xd a (3)
rER
LCrX r -Xc a (4)
rER
Xr E {a,I} (r E R) (5)
Xd,X c > a integer. (6)
The objective function and constraints (2) and (5) form a set covering problem which
selects a minimal cost (distance) set of routes such that for each customer i there is at
least one route visiting that customer. Constraints (3) and (4) ensure that the number of
routes and the total distance traveled, respectively, are integers.
The simplex method is used to solve the linear relaxation of the set covering type
problem. It gives the dual variables 7ri, i E N\ {d},7rd and 7rc associated with constraints
(2),(3) and (4) respectively, necessary for the solution to the subproblem. It also enables
easy reoptimization each time new columns are generated from the subproblem. There-
fore, from the linear programming theory, we obtain that the marginal cost cr of a route
r is given by:
Cr = Cr - L 7ririr - 7rd - 7rc C r
iEN\{d}
Since the cost of a route is defined as I:f=o Cik,i k +1 for a route (io, iI, ... , i K , i K +I), cr
becomes
K K K
Cr = LCik,ik+1 - L7rik -7rd-7rcLCik,ik+l
k=O k=l k=O
K
cr = L[(1- 7r c)Ci k ,ik +1 - 7rikJ as io = d.
k=O
Therefore, we can define the marginal cost cij of an arc (i, j) as cij = (1 - 7rc )Cij -
7ri, V(i,j) E A.
251
Solving the LP relaxation is accelerated by generating several columns simultaneously.
This is possible as the one-time solution of a subproblem by dynamic programming not
only produces the minimum marginal cost column but also many other columns of negative
marginal cost. The routes (columns) selected to be added to the LP relaxation are almost
disjoint as this enhances the efficient discovery of integer solutions. Furthermore, as
set covering characterizations of vehicle routing applications exhibit high degeneracy, we
further accelerate the convergence of the simplex method by a perturbation strategy on
the right hand side of constraints (2).
A primal-dual algorithm developed for the resolution of the shortest path problem
with resource constraints (Desrochers, 1988) computes the marginal cost for a route by
progressive refinement of lower and upper bounds on its value. The algorithm requires
the creation of two sets of labels at each node. The first set includes the labels associated
with feasible paths and defines primal solutions which provide an upper bound on the
cost of efficient solutions at each node. The second set includes the labels associated
with lower bounds on the cost of a path ending at node j with a given state value. The
algorithm modifies the sets of labels until the optimal solution is obtained. For a set
S of states at node j, labels are created through a "pulling" process. Labels associated
with feasible paths from the origin to node j are obtained by extending all feasible paths
from the origin to node i for which the addition of arc (i, j) allows arrival at node j in a
state belonging to S. All the new labels at a given iteration are created at a single node,
in contrast with other dynamic programming approaches requiring updating at several
nodes per iteration. A 2-cycle elimination procedure first proposed by Houck et aI. (1980)
has also been incorporated in the shortest path calculation.
When the subproblem does not generate any more negative marginal cost columns,
the simplex algorithm provides the optimal solution of the linear relaxation of the set
covering type formulation. If the solution is integer and each customer is covered exactly
once, the solution is also optimal for the VRPTW. Otherwise, the linear relaxation of the
set covering type model may be fractional or some customers may be covered more than
once. In either case, a. branch and bound tree must be explored, and additional columns
might be generated at each branch. In practice, however, over-covering has never arisen.
Furthermore, this condition is guaranteed not to occur if the cost matrix satisfies the strict
triangle inequality. The branching strategies designed for the VRPTW can be separated
in two categories. At the first level, the integrality of the number of vehicles used and the
total distance traveled is required. This branching level involves all the columns of the
set covering formulation. At the next level, branching decisions are taken locally on the
arcs of the network. This second branching level involves only very few columns.
252
To analyze the behavior of the VRPTW optimization algorithm described in the pre-
vious paragraphs, this algorithm was programmed in FORTRAN using the GENCOL
software for column generation (Sans6 et al., 1990). Our results indicate that this algo-
rithm proved to be very successful on a variety of practical size benchmark VRPTW test
problems (Solomon 1987). The algorithm was capable of optimally solving 100-customer
problems. This problem size is six times larger than any reported to date by other published
research. The results also reveal that the LP relaxation of the set covering type model
provides an excellent primal lower bound which in turn allows the efficient derivation of
an optimal solution by branch and bound. For 27 problems out of the 87 attempted, the
lower bound is equal to the optimal value. For the other problems, the average integrality
gap is 1.5% (maximum gap is 12.1%.) Therefore, solving the subproblem with integer
variables results in a small integrality gap, allowing the implicit exploration of a part of
the integrality gap of the set covering type model. Detailed results analysis is presented
in (Desrochers, Desrosiers and Solomon 1990).
The approach presented in this paper can be easily extended to even more difficult
problem variants. Multiple time windows per customer can be modeled by using as many
nodes as time windows for each customer. Only one node will be visited among the nodes
associated with a given customer. Two other extensions have been solved in Haouari et
al. (1990). These are the heterogeneous fleet problem and the multiple depot problem.
References
[1] Desrochers, M. 1988. An Algorithm for the Shortest Path Problem with Resource
Constraints. Cahiers du GERAD G-88-27, Ecole des H.E.C., Montreal, Canada,
H3T 1V6
[2] Desrochers, M., Desrosiers, J. and Solomon, M. 1990. A new optimization algo-
rithm for the vehicle routing problem with time windows. To appear in Operations
Research.
[3] Haouari, M., Dejax, P. and Desrochers, M. 1990. Modelling and Solving Complex
Vehicle Routing Problems using Column Generation. Working Paper, LEIS, Ecole
Centrale, Paris.
[4] Houck Jr, D.J., Picard, J.-C., Queyranne, M. and VEMUGANTI,R.R. 1980. The
Travelling Salesman Problem as a Constrained Shortest Path Problem: Theory and
Computational Experience. Opsearch 17, 93-109.
[5] Sans6, B., Desrochers, M., Desrosiers, J., Dumas, Y. and SOUMIS, F. 1990. Mod-
eling and Solving Routing and Scheduling Problems: GENCOL Reference Book,
GERAD, 5255 Decelles, Montreal, Canada, H3T 1V6.
[6] Solomon, M.M. 1987 Algorithms for the Vehicle Routing and Scheduling Problem
with Time Window Constraints, Operations Research 35, 254-265.
The Linear Complementary Problem, Sufficient
Matrices and the Criss-Cross Method
D. den Hertog, C. Roos and T. Terlaky *
1 Introduction
Linear Complementarity Problems (LOP) will be considered in this paper. Let us start
with fixing our notations. Let q be an n-dimensional vector, M be an n x n square matrix.
The pair (q, M) defines the LOP as follows:
-Mz+w=q, z~o, w~o, zTw=O. (1)
Variables Wi and Zi for i = 1, ... , n are called complementary variables. The coefficient in
row i and column j of matrix M will be denoted by mij. The solvability of LOP depends
on the special properties of the coefficient matrix M.
Linear complementarity problems are one of the most widely studied problems of mathe-
matical programming. Several effective methods were developed for solving LOPs in the
last decades (see Murty [5]).
Recently, the class of (column, row) sufficient matrices was identified by Cottle, Pang &
Venkateswaran [1] soon after Cottle [2] generalized the principal pivoting method (with a
tricky modification) for row sufficient LOPs.
Linear complementarity problems with sufficient matrices and their solution by the criss-
cross method (see Klafszky & Terlaky [4]) are examined in this paper. The criss-cross
method is a simple and purely combinatorial method, so the subject of this paper is
to characterize a matrix class by the finiteness of a combinatorial method. The defini-
tion of (column, row) sufficient matrices relies essentially on sign properties, so this is a
combinatorial definition of matrix classes.
Definition 1 A matrix M is called row sufficient if X MT x ::; 0 implies X MT x = 0
for every vector x. (i.e. if xi(MT X)i ::; 0 for all i ) then xi(MT X)i = 0 for every i.) A
'Delft University of Technology, Faculty of Technical Mathematics and Informatics, P.O. Box 356,
2600 AJ Delft, Netherlands.
NATO AS\ Series. Vol. F 82
Combinatorial Optimization
Edited by M. Akgul et al.
© Springer-Verlag Berlin Heidelberg 1992
254
matrix 111 is called column sufficient if XMx ::; 0 implies XMx = 0 for all vector x.
A matrix M is called sufficient if it is both row and column sufficient. (Here capital X
denotes the diagonal matrix, which has Xi in its diagonal.)
It has been proved that P matrices and PSD matrices are (row, column) sufficient ma-
trices, but there are sufficient matrices that are neither P, nor P S D matrices. The basic
properties of (column, row) sufficient matrices can be found in [1] and [2].
2 The criss-cross method for LCPs
Let an LOP be given. For simplicity, the actual coefficient matrix of any complemen-
tary basic tableau will be also denoted by (-M). Note that the nonbasic part of any
complementary basic tableau is a principal pivot transform of the original matrix (-M).
The criss-cross method can be defined as follows.
The criss-cross method
Initialization:
Let the starting base be defined by wand let w = q, Z = 0 be the initial solution.
The initial basic tableau is given by [-M, E, q].
Pivot Rule:
We have a complementary base and the corresponding basic tableau.
Leaving Variable Selection
Let k := min {i : Wi < 0 or Zi < O}.
If there is no k, then STOP, a feasible complementary solution has been found.
(Without loss of generality we may assume that Wk < 0.)
Entering Variable selection
Diagonal Pivot
If -mkk < 0, then make a diagonal pivot and repeat the procedure.
(Here Wk leaves and Zk enters the base.)
Exchange Pivot
We know that mkk = 0 in this case. Let l' := min{j : -mkj < 0 or - mjk > O}.
If there is no 1', then STOP, LOP is infeasible.
If there is an 1', then make an exchange pivot on (1', k) and repeat.
(Here Wk and ZT leave and Zk and W T enter the base.)
First of all, note that if the algorithm stops, then LOP is really solved. The algorithm
is initialized with a complementary solution, and since it performs only diagonal and
255
exchange pivots, complementarity is obviously preserved. If there is no leaving variable
(there is no k), then the actual solution solves the LCP since it is nqnnegative and com-
plementary. This property is independent from the special properties of matrix M. It is
also easy to see that if there is no entering variable, then there is no solution for the LC P.
Sufficient and necessary properties for the finiteness of the criss-cross method
The following properties are required for any principal transform of the coefficient matrix.
Property 1 The diagonal elements of (-M) are nonpositive.
Property 2 If for an index k the diagonal element is zero (-mkk = 0), then -mkj < 0
iff -mjk > 0 for any j.
Property 3 For an LCP the situations AB, CD, AC and BD of Figure 1 are exclusive
for any distinguished index indicated as infeasible in the basic tableaus if only complemen-
tary basic solutions are taken into account.
3 Results
Using these properties, the orthogonality property of basic tableaus and sufficiency prop-
erties of the coefficient matrix, the following theorems can be proved.
Theorem 1 Let M E M be a member of a class of matrices. The criss-cross method
solves LCP in a finite number of pivot steps for any member of the matrix class with any
parameter vector q E Rn iff Properties 1, 2 and 3 hold for any member of this matrix
class, and the matrix class is closed for these properties.
Theorem 2 Let the LCP be defined by an arbitrary vector q and by a sufficient matl'ix
M. Then, the criss-cross method solves LCP(M,q) in a finite number of steps.
Theorem 3 If an LCP is defined by a not column sufficient matrix M, and assume that
the desired criss-cross pivot exists at any situation, then there is a parameter vector q that
the criss-cross method may cycle on this LC P(M, q).
Conjecture: Let M be a column sufficient but not row sufficient matrix. Then there is
a vector q E Rm and a complementary base of LCP(M, q) where there is no criss-c1'OsS
pivot, but LC P( M, q) is feasible. (The criss-method fails to solve the problem')
There is a possible extension of our work to generalize our results and the definition of
sufficiency to oriented matroids, oriented matroid programming. This is the subject of
another paper (see Fukuda & Terlaky [3]).
256
A: EB B: EB
EB EB
- EB
EB
EB
EB -
c: 8 D: 8
EB .0 EB EB. . EB- -
8 8
8 +
8
EB. . EB- EB. .0 EB -
8
+ 8
Figure 1.
257
References
[1] Cottle, R. W., J. S. Pang, and V. Venkateswaran. Sufficient Matrices and the Linear
Complementarity Problem. Technical Report SOL 87-15, 1987.
[2] R. W. Cottle. The principal pivoting method revisited. Technical Report SOL 89-3,
1989.
[3] Fukuda, K., and T. Terlaky. Linear complementarity and oriented matroids. Preprint,
1990.
[4] Klafszky, E., and T. Terlaky. Some generalizations of the Criss-Cross Method for
Quadratic Programming. Linear Algebra and Its Applications, to appear.
[5] K. G. Murty. Linear complementarity, linear and nonlinear programming. Sigma
Series in Applied Mathematics, 3, Heldermann Verlag, Berlin, 1988.
A Characterization of Lifted-Cover Facets of
Knapsack Polytope with GDB Constraints
George L. Nernhauser, Gabriele Sigismondi and Pamela Vance *
Facet-defining inequalities lifted from minimal covers are used as strong cutting planes
in algorithms for solving 0-1 integer programming problems. We extend the results of
Balas [1) and Balas and Zemel [2) by giving a set of inequalities that determines the
lifting coefficients of facet-defining inequalities for any ordering of the variables to be
lifted. We further extend these results to obtain facet-defining inequalities for the 0-1
knapsack problem with mutually exclusive generalized upper bound (GUB) constraints.
Suppose a 0-1 program contains the inequality
L ajXj ~ b with Xj E {O, I} for all j EN.
jEN
Without loss of generality, since 0-1 variables can be complemented, assume aj > 0
for all j E N and suppose that a1 ~ a2 ~ ... ~ an > 0 and a1 ~ b.
The set e ~ N is called cover if 2.:: jE G aj > b. The cover e is minimal if for each
k E e, 2.:: jE G aj - ak ~ b.
A lifted cover inequality (LeI) is an inequality of the form
L Xj + L O:jXj ~ r - 1
JEG JED
where e is a minimal cover, 0 = N \ e, and r = lei. The lifting coefficients O:j are
nonnegative integers. For each j E 0, O:j must be as large as possible while still main-
taining validity for the lifted cover inequality to define a facet. Given a minimal cover
e = {jl,jz, ... ,jr} with ajl ~ aj2 ~ ... ~ ajr' let 0 = N \ e, /10 = 0, /1h = 2.::7=1 aj,
for h = 1, ... , r, /1t = /1r for t > r, and ,\ = /1r - b > O. For k E 0, let (3k = h if
/1h ~ ak ~ /1h+l-l. For Q ~ 0, we define (3(Q) = 2.::iEQ((3i + 1).
We say that S ~ 0 is an independent set if for all nonempty Q ~ S,
L ai > /1{3(Q) - A.
iEQ
'School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, Georgia
30332, USA.
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgtil e1 al.
© Springer-Verlag Berlin Heidelberg 1992
260
Balas and Zemel have shown that D:j :::; (3j + I for all j E N is a necessary condition for
LeI to be valid, and D:j ~ (3j is a necessary condition for LeI to define a facet. Moreover,
they have shown that D:j :::; (3j is a necessary condition for validity if D:j :::; Ji{3j - A. Here
we completely characterize facet-defining LeI's in terms of maximal independent sets.
Theorem 1 I:jEc Xj + I:jEc D:jXj :::; r - I defines a facet if and only if D:j = (3j + I for
all j in a maximal independent set Sand D:j = (3j for all j E C \ S.
The proof of the Theorem follows from the results of Balas and Zemel and close
examination of the value of
Gc(v) = max L Xj
JEC
LajXj < v
JEC
Xj E {O, I} j E O.
Now, consider the 0-1 knapsack with mutually exclusive GUB constraints,
"" a'x' < bL.t J J
jEN
L Xj < I ViE I, Si nSj = 0 Vi i= j.
Xj E {O,l} j E N.
Define 0 ~ N to be a cover if I:jEc aj > b and if no two members of the cover are in
the same GUB set.
Given a minimal cover 0 = {j1,h, ... ,jT}' define C, Jih, (3k, and (3(Q) as before. For
Q E C, let
{j EO: :3 k E Q and i E I such that j, k E Si}
{j E 01(Q): aj:::; ajl3(Ql+I-I C2(Q)I}'
02( Q) is easily determined by considering sequentially the smallest elements of 01( Q).
We call a set S ~ C an independent set if for all nonempty Q ~ S
L ai > Ji{3(Q)-IC2 (Q)I + L aj - A
iEQ jEC2(Q)
or if the inequality is violated for some subset Q, then at least two members of Q are in
the same GUB set.
261
Theorem 2 'EjEc Xj + 'EjEG Cl:jXj :::; l' - 1 defines a facet of the 0-1 knapsack problem
with GUB sets if and only if Cl:j = {3j + 1 for all j in a maximal independent set Sand
(Xj = {3j for all j E C \ S.
The proof is similar to that of Theorem 1. The upper bound on Cl:j is determined by
considering lifting Xj first, the lower bound by lifting Xj last. Note that Theorem 2 with
ISil = 1 for all i E I reduces to Theorem 1.
The two theorems presented characterize facets, but the computation necessary to find
a maximal independent set can be time consuming since the number of inequalities that
must be checked grows exponentially with the cardinality of the set. However, simply
identifying independent sets may be useful for obtaining strong LeI's.
References
[1] E. Balas, Facets of the Knapsack polytope, Mathematical Programming 8, 146-164,
1975.
[2] E. Balas and E. Zemel, Facets of the Knapsack polytope from minimal covers, SIAM
Journal on Applied Mathematics 34, 119-148, 1978.
On Pleasant Knapsack Problems
Bela Vizvari *
1 Introduction
In this paper the following knapsack problems will be considered
(1)
The maximization problem is MAXKP and the other one is MINKP. The constraints are
formally the same. We assume that the following regularity conditions are satisfied.
The order of variables is in accordance with their weights and relative weights, i.e.
and
(4a)
in the case of MAXKP and
( 4b)
in the case of the MINKP. These latter constraints will be referred to as (4). Assumptions
(3) and (4) restrict the knapsack problems to be considered. The restriction
(5)
ensures the feasibility of the greedy solutions. The constraints (2)-(5) are called regularity
conditions.
Then the greedy solution is defined by
*Bilkent University, POB 8, 06572 Maltepe, Ankara, Thrkey, VIZVARI@TRBILUN.BITNET
NATO ASI Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgiil et al.
(6)
© Springer-Verlag Berlin Heidelberg 1992
264
The greedy solution is considered as the function of the right-hand side and the number
of variables as sometimes greedy solutions of problems having only first k(~ n) variables.
The vector xg(k, b) is considered as an n-dimensional vector with x~(k, b) = 0, j =
k + 1, ... , n. A particular optimal solution of the problem will be denoted by x* (n, b).
The optimal value of the objective function is f(n, b) and the greedy value is g(n, b), i.e.
g(n, b) = 2:';=1 cjx;(n, b). The problem Pk is the subproblem of the first k variables.
Definition 1 Let u and v be values of the objective function belonging to different solu-
tions. The value u is better or at least as good, resp.,as the value v if
{ u > V or u 2: v, resp., in the case of the MAXKP
u < v or u ~ v, resp., in the case of the MINKP
Definition 2 A right-hand side b of a parametric knapsack problem of type MAXJ(P or
MINKP is called pleasant, if the greedy solution of it is optimal, i.e. f(n, b) = g(n, b).
The problem is called pleasant if the greedy solution is optimal for every right-hand side.
The problem is completely pleasant if Yk(1 ~ k ~ n) the subproblem Pk is pleasant, i. e.
Y k Y b f(k, b) = g(k, b). Finally a problem is called relatively pleasant if it is pleasant
but not completely pleasant.
This contribution reports on a research which connects and generalizes results achieved
mainly in number theory ([1], [2], [4], [10], [l1]) and partially in operations research ([3])
and furthermore some new questions closely connected with this area are investigated
([7], [8], [9]). Only special cases of the MINKP problem have been investigated in number
theory until now. Exception is ([4]) which give a version of Theorem 1 for MAXKP.
2 Completely Pleasant Problems
The easiest way of testing the pleasantness of a knapsack problem is based on the following
theorem. This test gives a positive answer only if the problem is completely pleasant and it
consists of n - 2 consecutive applications of the theorem. This test is an O(n 2 ) algorithm.
Theorem 1 ([IJ, [3J, !4J and [6J) Assume that the regularity conditions are satisfied and
the subproblem P n - 1 is pleasant. Let the integers sand t be defined as follows.
Then the following two statements are equivalent: (i) the problem is pleasant, (ii) Cn +
g(n,t) is at least as good as SCn-l.
265
3 Error Analysis
The error of the greedy method can be defined as the difference between the optimal and
the greedy values, i.e.
err(n, b) = { f(n, b) - g(n, b)
g(n, b) - f(n, b)
in the case of the MAXKP
in the case of the MINKP
The error analysis is based on the following observation.
Theorem 2 ([3J, [6]) Assume that the regularity conditions hold and the problem Pn is
pleasant. Let i be an arbitrary positive integer and
Let b an integer of the interval [ian, (i + l)a n ), such that x~(n, b) = 0. Then
err(n,b) :::; err(n,sian_l)
This theorem reduces the set of cases of the right-hand sides potentially giving the
maximal error, but the remaining set is still infinite. Through some further lemmas we
obtain the following statement where we use the previous notations.
Theorem 3 ([3J, [6]) Assume that the regularity conditions hold and the problem Pn lS
pleasant. Let d = g.c.d.(an_l, an). Then
where
max{err(n, b): bE Z+} = max{err(n, sian-I) : i = 1, ... , (an/d -I}
{max{O,siCn_1 - g(n,sian_l)}resp., in the case of the MAXJ(P
max{O,g(n, Sian-I) - Sicn_l)}resp., in the case of the MINJ(P
4 Necessary and Sufficient Conditions of the Pleas-
antness
The following two theorems provide the conditions for a finite set such that if all of the
right-hand sides belonging to this set are pleasant then the problem is pleasant.
266
Theorem 4 ([llJ, [6]) Assume that the regularity conditions are satisfied and n > 3.
Then the problem is pleasant if and only if for every y E Z, such that
we have
f(n,y) = g(n,y)
Theorem 5 Assume that the regularity conditions are satisfied and Pn is not pleasant.
Let m be the smallest nonpleasant right-hand side. Let the indices k and p be defined on
the following way
k = max{j : xj(n, m) > O} and p = max{j : 3 x*(n, m), xj(n, m) > O}.
Then there are two indices q and r, such that p 2: q > r 2: 1, x~(p, ak) > 0, and
m = a q + Ej=q ajxj(p, ak).
In [8] a polynomial algorithm of O(n4) is discussed, which can check the pleasantness
of a problem in all of the cases.
5 The Relatively Pleasant Problems with n=4
It is interesting to determine the relatively pleasant problems with the smallest possible
size. Until now only the case with Cl = Cz = ... = Cn = 1. of MINKP has been investigated
In this case the smallest possible n is 5 according to a theorem of [10]. In [1] the case
Cl = 1, Cz = 2, C3 = a, C4 = a + 1, Cs = 2a, where a 2: 4 is a parameter.
It is easy to see ([7]) that if the regularity conditions are satisfied then any problem
with n = 1 or 2 is pleasant. An immediate consequence of this is that if n = 3
and the regularity conditions are satisfied and the knapsack problem is pleasant, then it
is completely pleasant. Thus the smallest possible size of a relatively pleasant knapsack
problem is 4.
Theorem 6 Assume that the regularity conditions are satisfied in an relatively pleasant
problem of 4 variables. Then the following conditions are satisfied
az is not a divisor of a3, (7)
a3 < a4 < r~l a- a2 2, (8)
(r;;1)cz is better than C3 + (r;;1az - a3)cl (9)
C4 + (r;;1az - a4)cl is at least as good as r~1Cz (10)
267
where (1) and (8) are consequences of (9) and (10).
Theorem 7 Assume that the regularity conditions hold for a problem of 4 variables. The
problem is relatively pleasant if and only if (9), (10) and the following conditions are
satisfied
C4 + g(2, 2a3 - a4) is at least as good as 2C3, (11)
C4 + (a3 + a2 - a4)cI is is at least as good as C3 + C2' (12)
Examples. It is easy to see that the conditions of Theorem 4 are satisfied in the
following two examples, i.e. relatively pleasant problems with 4 variables exist for both
MAXKP and MINKP.
and
References
max lXI + (a + 2)X2 + (a + 4)X3 + (2a + 6)X4
Xl + aX2 + (a + 1 )X3 + 2ax4 = b
mm lXI + (a - 2)X2 + (a - 2)X3 + (2a - 6)X4
Xl + aX2 + (a + 1)X3 + 2ax4 = b
[1] M. Djawadi, Kennzeichung von Mengen mit einer additiven Minimaleigenschaft,
Dissertation, Math. Inst., Joh. Gutenberg University, Mainz, 1974.
[2] S. C. Johnson, and Kerninghan, B. W., Making change with a minimum number of
coins, manuscript, undated.
[3] M. J. Magazine, Nemhauser, G. 1., and Trotter, L. E., When the greedy solution
solves a class of knapsack problems, Operations Research 23: 207-217 1975.
[4] O. Marstrander, On a problem of Frobenius, Math. Scand. 58 161-175 1986; MR
87k:11030; Zb1.607.10038.
[5] E. S. Selmer, The local postage stamp problem, I, Research monograph, Dept. of
Math., Univ. of Bergen, Norway. No. 42-04-15-86 1986.
[6] B. Vizvari, On the Optimality of the Greedy Solutions of the General Knapsack
Problem, Computer and Automation Institute of the Hungarian Academy of Sci-
ences, Report 5: 1-23 1989.
[7] On Some Generalization of the Greedy Method, Bilkent University, Department of
Industrial Engineering, Res. Rep. IEOR-9001, 1-7 1990.
268
[8] B. Vizvari, A Polynomial Algorithm to Check the Pleasantness of a Knapsack Prob-
lem, Bilkent University, Department of Industrial Engineering, Res. Rep. IEOR-
9013, 1-14 1990.
[9] Vizvari, On Pleasant But Not Completely Pleasant Knapsack Problems, Bilkent
University, Department of Industrial Engineering, Res. Rep. IEOR-9017, 1-7 1990.
[10] Zoellner, Uber Mengen natiirlicher Zahlen, fiir die jede euklidische Darstellung eine
minimale Koeffizientensumme besitzt, Diplomarbeit, Mainz 1974.
[11] J. Zoellner, Uber angenehme Mengen, Mainzer Seminarberichte in additiver Zahlen
Theorie, 1: 53-71 1983.
Extensions of Efficient Exact Solution Procedures
to Bicriterion Optimization
Yasemin Aksoy *
There exists a wide variety of decision making situations in which trading off one
objective against another is involved, such as time versus cost. Bicriterion mathemat-
ical programming has been successfully applied in such situations. Examples of such
works include nutrition planning, portfolio analysis, resource allocation, transportation
and assignment problems, project scheduling, production planning, etc. In this paper
an interactive algorithm is given for a bicriterion problem in the most general sense,
namely, the bicriterion nonconvex mixed integer programming problem that maximizes
two objective functions over a compact nonempty set.
The algorithm finds an optimal solution to the problem in the sense that maximizes
the utility of the decision maker (DM). Since it is not easy to assess a value function to
represent the preferences of the DM, here we assume that it exists, but cannot be stated
explicitly. Using a branch and bound scheme, the algorithm searches among only the set
of nondominated solutions since one of them is a most preferred solution that maximizes
the overall value function of the DM over the set of achievable solutions.
The interactive branch and bound algorithm only requires pairwise preference com-
parisons from the DM. Based on the DM's responses, the algorithm reduces the set of
nondominated solutions, and terminates with a most preferred nondominated solution,
given that the DM's preferences are consistent, transitive, and invariant over the course
of optimization.
Compared to dual based algorithms for bicriterion nonlinear mixed integer program-
ming problem, the interactive branch and bound algorithm has the advantage of yielding a
most preferred solution of the DM independent of the existence of a duality gap. However,
it requires solving single objective nonlinear mixed integer programming subproblems at
each iteration. While algorithms do exist for the solution of certain classes of nonlinear
integer programming problems, most have not been tested computationally and those that
have are effective only for relatively small size problems. Naturally, the computational
'Tulane University A. B. Freeman School of Business
NATO AS! Series. Vol. F 82
Combinatorial Optimization
Edited by M. Akgiil et al.
© Springer-Verlag Berlin Heidelberg 1992
270
efficiency of the interactive branch and bound algorithm is dependent on the optimiza-
tion method used to solve the subproblems at each iteration. Therefore, use of efficient
methods that are specifically designed for solving the subproblem at hand would result
in significant computational savings.
An advantage of the algorithm is that if the original problem has a special structure,
then the subproblems will have a similar structure. For example, if the original problem
is a bicriterion convex integer programming problem, then all the subproblems are also
convex integer programming problems. This is not only important in achieving compu-
tational efficiency but also in motivating further research in extending efficient solution
methodologies for single objective models to the cases where it is more realistic to include
two objectives.
Computational savings may also be achieved, if an optimization method that relies on
an initial feasible solution is used while solving the subproblems. By a proper selection
of one of the nondominated solutions that was generated in an earlier iteration, a good
initial feasible solution can be found.
In most practical applications, only a small number of interactive iterations can actu-
ally be carried out. One of the advantages of the interactive branch and bound algorithm
is that the algorithm generates nondominated solutions with objective function values
that are well spread. This helps the DM get a better idea about the shape of the set of
nondominated solutions at early iterations of the optimization.
Combinatorial Aspects in Single Junction Control
Optimization
Giuseppe Bruno and Gennaro Improta*
In recent years the evaluation by mathematical programming techniques of optimal
values for junction control variables (green timing, green scheduling and cycle time) has
been carried out by various methods. These methods adopt as possible objectives of opti-
mization: the capacity factor maximization (linear); the total rate of delay minimization
(non linear but convex or linear using piecewise linearization of delay functions); the cycle
time minimization (linear).
It is possible to distinguish two classes of mathematical programming methodologies
of different complexity.
In the first class, the flows and saturation flows of the streams being noted, the stage
matrix is preliminarily fixed and the optimal green times for the stages calculated. This
is called a Stage-based approach. These methods use mathematical models in which all
constraints are linear and the variables of optimization are real. The stage-based approach
has the disadvantage of requiring in advance the specification of sequence of stages and
the duration of the transition periods. Furthermore, no direct method is available within
these methods to constrain the duration of the effective red and green periods of the
groups themselves.
In the second class, the flows and saturation flows are still assumed known and the
optimal timing is evaluated from the knowledge of the crossing compatibility of the dif-
ferent streams. It is possible to divide the methods belonging to the second class (see
Allsop, 1983) into two subclasses.
The first subclass is based on the analysis of the possible sequences of compatibility
cliques and can be regarded as a natural evolution of the stage-based approach. Any
clique is a possible stage, any sequence of cliques respecting some imposed constraints is
a possible stage succession. The optimization problem consists of calculating the optimal
clique sequence, the green time for each clique and the cycle time (Soffers, 1968; Zuzarte
'Universita' di Napoli Federico II, Dipartimento di Informatica e Sistemistica, Cattedra di Ricerca
Operativa, via Claudio 21 - 80125 Napoli.
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgul et al.
© Springer-Verlag Berlin Heidelberg 1992
272
Tully, 1977; Zuzarte Tully and Murchland, 1978; Cantarella and Improta, 1981).
In the second subclass an endpoint and the green time for each stream are used to
determine the optimal cycle time, green timing and green scheduling. This is called phase-
based approach. In this alternative approach one starts directly from the durations of the
signal indications rather than through the intermediary of stages. Furthermore, the stage
sequence and the structure of the transition periods haven't to be specified in advance of
optimization.
Two methods, belonging to this approach, have been proposed in recent years.
In the first method (Gallivan and Heydecker, 1983; Heydecker and Dudgeon, 1987;
Moller, 1987) the green timing and the scheduling are represented in the complex plane,
by two variables for each group, one representing the start and the other the duration of
the green interval. In other words the cycle time can be represented by a circumference
and the green times of the groups by circular arcs duly located.
The other method (also called: single junction control system design) is based on a
mathematical programming model by which control variables (cycle time, green timing
and scheduling) are calculated simultaneously. This model is formulated by representing
on a time axis the control variables, measured in time unit (Improta and Cantarella, 1982;
Improta and Cantarella, 1984) or defined as a proportion of the cycle time (Cantarella and
Improta, 1988). All the constraints are linear and the non-overlapping condition among
incompatible groups is expressed using binary variables. An alternative formulation usili*g
only real variables requires non-linear constraints (Cantarella and Improta, 1983). The
model can be solved by usual Mathematical programming techniques. A problem ori-
ented algorithm can be adopted for capacity factor maximization and critical cycle time
evaluation (Cantarella and Improta, 1988).
The phase-based approach represents an interesting evolution in the field of the isolated
junction control optimization. It requires, however, a preliminary decision about the
composition of the streams and the assignment of the lanes; the flow ratios of the streams,
indeed, have to be assumed constant during the cycle.
Starting from the binary linear mathematical model used by Cantarella and Improta
(1988), a proposal to remove these last hypothesis is indicated. It is based on a model
describing the behavior of the users belonging to streams coming from the same access.
A numerical example using a simple test junction is shown; the obtained results are
compared to the ones deriving from a standard phase-based method.
273
References
[1] R. E. Allsop, Optimization of Timings of Traffic Signals. Proc. 1983 AIRO Conf.
Guida Editoria, Napoli, 103-120, 1983.
[2] G. E. Cantarella, and Improta, G., Una Metodolgia per il Progetto Globale di
Intersezioni Semaforizzate. Atti delle Giornate di lavoro AIRO 1981, Torino, 33-48,
1981.
[3] G. E. Cantarella, and Improta, G., A Non-Linear Model for Control System Design
of an Individual Signalized Junction. Proc. 1983 AIRO Conf. Guida Editori. Napoli,
709-722, 1983.
[4] G. E. Cantarella, and Improta, G., Capacity Factor and Cycle Time Optimization:
A Graph Theory Approach, Transpn Res. 22B, 1-23, 1988.
[5] S. Gallivan, and Heydecker, B. D., Optimizing the Control Performance of Traffic
Signals at a Single Junction. 15th Ann. Conf. Universities Transport Study Group,
Imperial ColI. London, 1983.
[6] B. G. Heydecker, and Dudgeon, I. W., Calculation of Signal Settings to Minimize
Delay at a Junction. On Transportation and Traffic Theory. N. H. Gartner and N.
H. M. Wilson Eds., Elsevier Science Publisher Co., Inc., 159-178, 1987.
[7] G. Improta, and Cantarella, G. E., Signalized Junction Control System DeSIgn.
EURO - TIMS XXV, Lausanne, July 1982.
[8] G. Improta, and Cantarella, G. E., Control System Design for an Individual Signal-
ized Junction. Transpn. Res., 18B, 147-167, 1984.
[9] K. Moller, Calculation of Optimum Fixed- Time Signal Programs. On Transportation
and Traffic Theory. N. H. Gartner and N. H. M. Wilson Eds., Elsevier Science
Publisher Co., Inc. 179-198, 1987.
[10] K. E. Stoffers, Scheduling of Traffic Lights - A New Approach. Transpn. Res., 2,
199-234, 1968.
[11] I. M. Zuzarte Tully, Synthesis of Sequences for Traffic Signal Controllers Using
Techniques of the Theory of Graphs, Ph.D. Thesis, University of Oxford Engineering
Laboratory Report OUEL, 1189/77,1977.
[12] I. M. Zuzarte Tully, and Murchland, J. D., Calculation and Use of the Critical
Cycle Time for a Single Traffic Controller. PTRC Summer Annual Meet, Proc.,
PTRC-PI52, 96-112, 1978.
Approximation Algorithms for Constrained
Scheduling
Leslie A. Hall *
David B. Shmoys t
In this paper we consider several constrained machine scheduling problems. The first
model consists of n jobs and m identical machines that operate in parallel. Each job j
has a processing time Pj and must be processed without interruption for time Pj on any
one of the m machines. In addition, each job j has a release date rj, when it becomes
available for processing, and a delivery time qj. Each job's delivery begins immediately
after its processing has been completed, and all jobs may be delivered simultaneously. A
schedule consists of a set of starting times 0"1, ••• ,O"n with O"j ~ rj, j = 1, ... , n, and an
assignment of jobs to machines so that each machine processes at most one job at a time.
For a given schedule, we define Cj := O"j + Pj + qj to be the completion time of job j, and
the object of the problem is to minimize, over all possible schedules, Cma:c := maxj Cj .
The problem as stated is equivalent to that with release dates and due dates, dj , rather
than delivery times, in which case the objective is to minimize the maximum lateness,
Lj = O"j + Pj - dj, of any job j. When considering the performance of approximation
algorithms, the delivery-time model is preferable (see [6,4]). Because of this equivalence,
we shall denote the problem as PIriILma:c, using the notation of Graham et al. [I]. The
special case of a single machine is denoted llrjILma:c.
The second model that we consider is a two-machine flow shop with release dates,
in which each of n two-task jobs must be processed first on machine Ml and then on
machine M 2 • No preemption is allowed. A job j becomes available for processing on
Ml at its release date rj; and, it cannot begin processing on M2 until it has completed
processing on Ml (although there can be time between when a job completes processing
on Ml and begins processing on M2)' The object is to minimize the time at which M2
finishes processing all jobs. In the notation of Graham et al. [1], this model is denoted
F21riICma :c.
·Princeton University, Princeton, N. J.
tCornell University, Ithaca, N. Y.
NATO ASI Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgiil et al.
© Springer-Verlag Berlin Heidelberg 1992
276
All of these problems are strongly N P-hard. For such problems, it is natural to search
for a p-approximation algorithm, an algorithm guaranteed to deliver a solution with value
within a factor p of the optimal value. The best result that we can hope to obtain for
strongly N P-hard problems is a polynomial approximation scheme, namely, a family of
(1 + E)-approximation algorithms whose running times can depend in an arbitrary manner
on l/E.
We present the first polynomial approximation schemes for the problems under con-
sideration. This work appears in its complete form in two extended abstracts [3, 5] and
two papers [2, 4]. Our approximation algorithms are based on the notion of an outline
scheme. The following simple idea motivates this technique. For combinatorial optimiza-
tion problems, often knowing some information about the optimal solution allows us to
construct a near-optimal (or even optimal) solution in polynomial time. A strategy for
solving the problem in general would then be to test every possibility for this information.
Since at least one of the possibilities corresponds to an optimal solution, we are then guar-
anteed to find a near-optimal (or optimal) solution ourselves. Of course, the drawback
to this strategy is that, in general, any set of information large enough to be useful has
an exponential number of possible values. Our challenge is therefore to devise a set of
information that is at once detailed enough to be useful but small enough that we can
test every possible value for the information in polynomial time. The formal definitions
follow.
Definition. An outline scheme for a problem is a labeling of feasible solutions such
that, for each feasible solution x, the associated label, called an outline, provides concise
information about x.
For example, one outline scheme for a scheduling problem labels each job with its starting
time in the schedule. Of course, this outline is particularly useful because with it, we
could essentially reconstruct the original schedule, x. Most outlines reveal only partial
information about their corresponding schedule or schedules; even so, this partial infor-
mation might still be enough to construct a schedule that is "close to" , or almost as good
as, the outline's corresponding schedule or schedules.
Definition. For E > 0, a (1 + E)-optimal outline scheme for a problem is an outline scheme
for the problem and an algorithm A with the following property: given any outline w for
any instance I, Algorithm A delivers a feasible solution to I of value at most (1 + E) times
the value of any feasible solution of I that is labeled with w.
In particular, if the number of possible distinct outlines for I is polynomial in the size of
I, then by running Algorithm A on every outline, we obtain in polynomial time a feasible
277
solution guaranteed to be within a factor (1 + t) of the optimal solution.
In addition to the scheduling models already mentioned, we have been interested in
studying precedence-constrained scheduling problems. Job precedence constraints arise in
a number of practical scheduling problems, and one important area of tesearch in schedul-
ing theory has been to study under what conditions a precedence-constrained problem
is computationally harder than its counterpart with independent jobs. We consider this
question with respect to finding provably near-optimal solutions for strongly NP-hard
problems; for a particular problem, what is the best relative error that can be guaranteed
by a polynomial-time algorithm, or can any performance guarantee be achieved? Typi-
cally, precedence constraints make a problem provably harder. For example, scheduling
unit-length jobs on identical parallel machines to minimize the maximum completion time
is a trivial problem; just balance the number of jobs assigned to each machine as much as
possible. On the other hand, for the same problem except with a precedence relation con-
straining feasible schedules, Lenstra & Rinnooy Kan [7] have shown that no polynomial-
time algorithm can guarantee a relative error better than 1/3, unless P = N P. We
give the first example of a precedence-constrained scheduling problem that is (strongly)
N P-hard, and yet any fixed positive relative error can be guaranteed in polynomial time.
Specifically, we present a polynomial approximation scheme for the precedence-constrained
version of IhILma:e, which we denote llrj,precIL ma :e' It is based on extending the tech-
niques used for the case of independent jobs.
The new result highlights an interesting distinction between IhlLma:e and its gener-
alization that allows each job to be processed on one of m parallel identical machines.
Our earlier results include a polynomial approximation scheme for PhILma:e; however,
if precedence constraints are added to this parallel machine problem, one can prove that
no p-approximation algorithm exists for any p < 4/3 (unless P = NP) [7]. Thus, the
new result provides evidence that the parallel version is harder than the single-machine
problem.
References
[1] R. 1. Graham, E. 1. Lawler, J. K. Lenstra, and A. H. G. Rinnooy Kan. Optimization
and approximation in deterministic sequencing and scheduling: a survey. Annals of
Discrete Mathematics, 5:287-326, 1979.
[2] 1. A. Hall. A polynomial approximation scheme for a constrained flow-shop schedul-
ing problem. Working paper, February 1990, revised October 1990.
[3] 1. A. Hall and D. B. Shmoys. Approximation algorithms for constrained scheduling
problems. In Proc. IEEE 30th Annual Symp. Foundations of Computer Science,
pages 134-141, 1989.
278
[4] 1. A. Hall and D. B. Shmoys. Jackson's rule for single-machine scheduling: making
a good heuristic better. Math. of O.R., to appear.
[5] 1. A. Hall and D. B. Shmoys. Near-optimal sequencing with precedence constraints.
In Ravi Kannan and W. R. Pulleyblank, editors, Proc. Math. Prog. Soc. Conference
on Integer Programming and Combinatorial Optimization, pages 249-260, 1990.
[6] H. Kise, T. Ibaraki, and H. Mine. Performance analysis of six approximation algo-
rithms for the one-machine maximum lateness schedulng problem with ready times.
Journal of the Operations Research Society of Japan, 22(3):205-224, 1979.
[7] J. K. Lenstra and A. H. G. Rinnooy Kan. Complexity of scheduling under precedence
constraints. Operations Research, 26:22-25, 1978.
An Analogue of Hoffman's Circulation Conditions
for Max-Balanced Flows
Mark Hartmann *
Michael H. Schneider t
Let G = (V, A) be a directed graph. A real-valued vector x defined on the arc set A
is a max-balanced flow for G if for every cut W the maximum weight over arcs leaving
W equals the maximum weight over arcs entering W. This instance of algebraic flows,
which have been studied by Hamacher [2] in the more general setting of regular matroids,
was re-introduced by Schneider and Schneider [3] under the name max-balanced graphs.
For vectors I :::; u defined on A, an analogue of Hoffman's circulation conditions holds for
max-balanced flows: There exists a max-balanced flow x satisfying I :::; x :::; u if and only
if max aE5+(W) la :::; max aE6-(W) U a for all 0 eWe V. Both of these are equivalent to the
existence of an (l, u) cycle coverfor G, which is a collection of directed circuits {Ca : a E A}
such that a E Ca and U e ;::: la for all e E Ca. Analogous results are shown to hold for
algebraic flows in oriented regular matroids in [2], where the cutsets 8+(W) and 8-(W)
are replaced by the positive and negative parts D+ and D- of the cocircuits D of the
oriented regular matroid and a directed circuit is any circuit C with C = C+. In the case of
max-balanced flows, these characterizations can easily be shown to hold for any oriented
matroid using an extension of Minty's painting lemma [1, Proposition 3.4]. We give a
simple graph-theoretic proof for directed graphs, which yields other equivalent conditions
that point to a relationship between max-balanced graphs and bottleneck paths.
In directed graphs, the cycle cover condition can be used as the basis of an O(IAI2)
algorithm which either finds a max-balanced flow x satisfying I :::; x :::; u or a subset
oeWe V with max aE6+(W) la > max aE6-(W) U a (see [2], p. 62). We show, however, that
minimizing a linear function cT x over the set mbf(l, u) of max-balanced flows x satisfying
I :::; x :::; u is NP-hard by reducing the Steiner Tree in Graphs problem to the following
special case, which we call Max-Balanced Subgraph: Given a directed graph G = (V, A),
a subset B s::; A and a positive integer ]( :::; IAI - t, is there a max-balanced subgraph of
'Department of Operations Research, Chapel Hill, NC 27559
tDepartment of Mathematical Sciences, The Johns Hopkins University, Baltimore, MD 21218
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgiil et al.
© Springer-Verlag Berlin Heidelberg 1992
280
G that includes all of the arcs in B and no more than K arcs? On the other hand, we
show that the problem of determining a minimum cardinality set E C V X V such that
the digraph (V,AU E) is max-balanced can be solved in O(IVI+IAI) time. (A digraph
is max-balanced if and only if it is the union of its strongly connected components.) We
give an O(IVIIAI) algorithm for computing the greatest element in mbf(l, u) under the
usual coordinate partial order, which first solves the all-pairs bottleneck paths problem
in G with capacities u. This allows us to solve the problem of finding a max-balanced
flow x which minimizes Ilx - zlloo in polynomial time, where z is arbitrary. By way of
contrast, we show that if the 100 norm is replaced by the h norm, the problem becomes
N'P-hard. The algorithm for finding a maximal max-balanced flow also allows us to solve
problems of the form minxEmbf(l,u) maXo.EA( Co.Xo. +d o. ) and maxxEmbf(l,u) mino.EA( Co.Xo. +do. )
in O( IV IIAllog IVI) time using binary search.
In the special case of minimizing or maximizing Xo. over x in mbf(l,u), reducing op-
timization to feasibility is the approach of relations (3.14) and (3.15) in [2]. However,
when studying the minimal elements in mbf(l, u), it is more convenient to view this in
another way. We say that b is a forcing arc for a, written b E force( a), if h ~ lo. and every
directed circuit C which contains b and has U e ~ h for all e E C also contains a. Then
minxEmbf(l,u) Xo. = maXbEforce(o.) h, which can be computed in O(IVIIAI) time using bot-
tleneck paths. We give a related O(1V121AI) algorithm which finds a minimal element in
mbf(l, u) by sequentially minimizing Xo. one arc at a time and updating the upper bounds.
It is not hard to show that if Y is a minimal element of mbf(l, u), then if we sequentially
minimize Xo. and the arcs are chosen in increasing order of Yo., then the result is y. These
results concerning minimal elements can be shown to hold for oriented matroids, but we
do not have an algorithm for computing the minimum values in this case.
There will in general be many other orders of the arcs for which sequentially minimizing
Xo. yielUs y, but to see this we first need to derive additional structural properties of the
minimal elements in mbf(l, u). We show that in an oriented regular matroid, if y is
a minimal element in mbf(l, u) and C is a circuit, then there exists a E C such that
lo. = Yo. = mineEc Yeo This gives an analogue for max-balanced flows of the fact that in a
directed graph, if Y is an extreme point of the set of circulations x satisfying 1 :::; x :::; u,
then the set of basic arcs of y forms a tree. This allows us to show that for x, y E mbf (l, u)
and y minimal, if x i= y then there exists a with 10. = Yo. < Xo. such that Yo. < Ye for every e
such that Xe < Yeo As a corollary, we find that if x and yare minimal elements of mbf(l, u)
with Xe = le whenever Ye = Ie then x = Y, which gives an analogue for max-balanced flows
of the fact that in a directed graph, if Y is an extreme point of the set of circulations x
satisfying 1 :::; x :::; u, then y is determined by its values on non-basic arcs. We also have
281
that if y is a minimal element in mbf (l, u) and we sequentially minimize Xa where the arcs
are chosen so that whenever la < Ya, all arcs e with Ie = Ye < Ya occur before a, then the
result is y. Finally, we show that counting the number of minimal elements in mbf (l, u)
is #P-hard.
An interesting open question raised by some of the conditions in our analogue of
Hoffman's circulation conditions for max-balanced flows in directed graphs is the following:
Let G = (V, A) be a directed graph, and let x be a non-negative real-valued vector defined
on the arc set A. We say that x is flow-symmetric if for every pair sand t of nodes, the
maximum value of an (s, t) flow in G with capacities x is equal to the maximum value of
a (t, s) flow in G with capacities x. Clearly every circulation is flow-symmetric, but is the
converse true? (The analogous result is true for max-balanced flows, with "flow" replaced
by "bottleneck path.")
References
[1] R.G. Bland and M. Las Vergnas. Orientability of Matroids. Journal of Combinatorial
Theory, Series B 24, 254-265 (1978).
[2] H. Hamacher. Flows in Regular Matroids. Oelgeschlager, Gunn (3 Hain, Cambridge
(1981).
[3] H. Schneider and M.H. Schneider. Max-balancing weighted directed graphs. To ap-
pear in Mathematics of Operations Research, 16 (1) (1991).
Some Telecommunications Network Design
Problems and the Bi-Steiner Problem
Geir Dahl *
We present two network design problems arising in telecommunications. First we
discuss the subscriber network extensions problem which consists in designing a telephone
network connecting certain subscribers to some switching point. Next we mention a
similar problem in the design of cable television networks. Finally we briefly discuss the
bi-Steiner problem, which is a generalization of the Steiner problem in directed graphs in
which directed two-connectedness to each terminal node is required.
In telecommunications the design of subscriber network extensions (SNE) in a tele-
phone network is an important planning problem where large investments are involved.
Basically the problem can be described as follows (see [5]).
We are given certain subscribers, each to be connected to some supply point (several
alternatives are available). The final network is hierarchical. Each subscriber shall be
connected either to a distributor or directly to a cross connector. The distributors are
connected to the cross connectors, which again are connected to some supply point. Each
supply point is either a cable from a subscriber switch or an RSU (remote switching unit).
All connections are made by cables (selected among different types). A cable contains a
certain number of copper wire pairs. Each subscriber has a demand for a certain amount
of such pairs, and these pairs are dedicated to the use of this subscriber in the final
network. Furthermore all cables are placed in trenches. Ther~ are costs and capacities
associated with cables, distributors, cross connectors, and finally one has costs of digging
trenches.
The (SNE) problem consists of designing a network at minimum cost that satisfies
the requirements described above. The planning tool ABONETT has been developed
by Norwegian Telecom,' Norway for solving the (SNE) problem (see [5]). It is currently
used by regional planners, with average economic savings (compared to manual plans
developed by experienced planners) of 10-15 %. The core of ABONETT is an integer
linear programming model, involving both a 'trench problem' and a 'cabling problem'.
'Norwegian Telecom, Research Dept., P.o.box 83, N-2007 Kjeller, Norway
NATO AS! Series. Vol. F 82
Combinatorial Optimization
Edited by M. Akgiil et al.
© Springer-Verlag Berlin Heidelberg 1992
284
The trench problem (where to dig the trenches) is essentially solved separately as an
undirected Steiner problem. In the model of the cabling problem there are variables that
describe whether a cable of a certain capacity is to be installed from node i to node j.
The technology requires that all traffic through a cable must be in the same direction (the
network structure is tree-like directed away from the supply point). Thus a directed model
was chosen, meaning that the cost of terminal equipment (cross connector, distributor)
can be included in the cost of each of the ingoing arcs of the node. (This is valid since the
final solution will contain at most one ingoing arc of each node). Essentially the model
is a directed Steiner problem ('where to place cables') with certain additional constraints
reflecting the capacity requirements. This problem is solved in ABONETT by relaxing the
capacity requirements in a Lagrangian fashion. The Lagrangian subproblems are directed
Steiner problems which are solved by some fast heuristic methods, while the Lagrangian
dual problem is solved by a subgradient algorithm (see [5]).
Currently there is increasing interest in being able to offer the subscribers better
service quality in the sense of a certain survivability against cable cuts etc. This can be
accomplished by e.g. 'dual homing' which means that a subscriber is connected to the
supply point via two (arc-) disjoint paths. It is therefore of interest to be able to find
minimum cost solutions which contain two arc-disjoint directed paths from the supply
point to certain subscribers.
Next we mention a closely related problem in telecommunications that occur in the
planning of local area cable television networks (CTV), (see [3]). The problem consists of
finding a minimum cost cable network which connects a receiver of signals (from e.g. a
satelite), called the Dl-head, to certain subscribers within a (reasonably small) geographic
area. The network consists of subnetworks (D2-network, D3-networks and subscriber
networks) organized in a hierarchical manner. Each subnetwork is a directed tree with a
certain root node, and it consists of cables and nodes containing equipment like splitter,
amplifier and tap. Thus the path from the Dl-head to each subscriber will pass through
the D2-network, then some D3-network and finally some subscriber network. By definition
an amplifier (tap) is located in the root node of each D3-network (subscriber network).
The purpose of the cable network is to deliver a signal to each subscriber at a prescribed
(single) level (dBmV). Throughout the network one has attenuation (signal loss) in both
cables and equipment (the magnitude of this loss is type- determined). However the
amplifiers are used to increase level wherever needed. Again cables must be placed in
trenches, and one has to decide where to dig these trenches.
The problem has been formulated as a mixed integer linear programming problem,
and a solution method has been suggested, see [3]. The method consists in solving first
285
'the trench problem' (as for (SNE)), then the 'cabling problem'. The trench problem
is an undirected Steiner problem, and usually the trench costs accounts for about 70%
of the total costs. The solution method for the cabling problem consists in solving a
sequence of linear programs and heuristically fixing certain variables between consecutive
LP problems. By using this model/solution method one can solve rather small (CTV)
problems, and we are now trying to improve these results (see [3]).
As mentioned above, it is of interest to solve Steiner problems with connectivity con-
straints. In particular one can consider the directed bi-Steiner (DBS) problem defined
as follows ([1]). Consider a directed graph with a fixed node, called the root node, and
certain terminal nodes (e.g. subscribers). Furthermore there is given a non-negative cost
function defined on the set of arcs. The bi-Steiner problem consists of finding a subset
of the arcs of minimum total cost such that this subgraph contains, for each terminal
node t, two arc-disjoint directed paths from the root node to t ('bi- connected'). More
generally we can have a non-negative integral connectivity parameter k( v) associated with
each node v and require that the subgraph contains k(v) arc-disjoint directed paths from
the root node to v. This problem, called the directed Steiner problem with connectivity
constraints (DSCC), is discussed in [2). The corresponding problem in undirected graphs
is studied in [4], [6).
Two interesting special cases of (DBS) are (i) the case of one terminal node, and (ii)
the case where all nodes (except the root) are terminal nodes. Both these problems can
be solved in polynomial time, see [2). One can formulate an integer linear programming
model for (DBS) by introducing one O/l-variable per are, and stating constraints by
applying Menger's theorem (the special case of the max-flow min-cut theorem). A rather
simple cutting plane algorithm for solving (DBS) (and also (DSCC)) instances has been
implemented. The separation problems consists of solving max- flow problems. The
computational results demonstrate that the linear programming relaxation is very strong,
in fact one usually gets integral optimum solutions from this relaxation (so branch and
bound is not needed). Thus for solving many real world problems the challenge lies in
being able to separate fast the Steiner cut inequalities. Here several heuruistic methods
(e.g. based on breadth-first- search) can be used with success. For a more detailed
presentation of (DBS) and (DSCC), see [1), [2).
286
References
[1] G. Dahl, The bi-Steiner problem, Research Document TF Nll/90, Norwegian Tele-
com (1990).
[2] G. Dahl, Directed Steiner problems with connectivity constraints, to appear in
Discrete Applied Mathematics.
[3] G .Dahl, O. Eriksen, R. Lorentzen, R. Nielsen and L. Skar, Cable-TV - an optimiza-
tion tool for design of cable television networks, Research Document TF Nl/90,
Norwegian Telecom (1990).
[4] M. Grotchel, C.L. Monma and M. Stoer, Polyhedral approaches to network surviv-
ability, Report No. 189, Universitat Augsburg (1989).
[5] R. Lorentzen and H. Moseby, Mathematical models and algorithms used in the
subscriber network planning tool ABONETT, TF-report 66/89, Norwegian Telecom,
Research Dept. (1989).
[6] C.L. Monma and D.F. Shallcross, Methods for designing communications networks
with certain two-connected survivability constrai~ts, Operations Research 37 (1989)
531-541.
Parallel Machine Scheduling to Minimize Costs
for Earliness and Number of Tardy Jobs
H. G. Kahlbacher*
T. C. E. Chengt
We discuss the problem of scheduling a set N of n independent jobs on m parallel machines
to minimize costs for earliness, due date assignment and weighted number of tardy jobs.
We restrict the due dates to the common due date case and distinguish between two
different models, namely an externally given common due date or an adjustable common
due date. The latter model usually includes due date assignment costs since moderate due
dates represent a high level of service quality. An extensive survey of scheduling research
involving the due date assignment decision has been conducted by Cheng & Gupta[2],
and a survey of scheduling research involving both due date models has been written by
Baker & Scudder[l].
In this paper we combine the constant tardiness penalties with earliness costs. This
approach has its natural applications in the Just-In- Time production systems since earli-
ness may cause inventory costs and the philosophy of Just-In-Time mainly tries to reduce
these inventory costs without violating the job due dates. As for the tardy jobs, they must
be delivered on an individual basis on completion. This will result in additional delivery
costs for tardy jobs which are functions of the number of tardy jobs with respect to a
given due date (or delivery schedule). We concentrate on the common due date version
of the problem, i.e. we wish to complete all jobs in the job set at the same time. This
restriction has been motivated by applications in Just-In- Time production and by the
difficulty of the general due date problem. At this point, we mention that the common
due date approach may be used as a heuristic for the general problem by solving a series
of common due date subproblems generated by cluster analysis of the due dates. Since
this paper gives an insight in the complexity of the common due date scheduling problem,
it may help to classify the general due date problem.
·University of Kaiserslautern, Faculty of Mathematics, D-6750 Kaiserslautern, FRG.
tUniversity of Manitoba, Department of Actuarial and Management Sciences, Winnipeg, Manitoba,
Canada R3T 2N2.
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgiil et al.
© Springer-Verlag Berlin Heidelberg 1992
288
As for the objective function, we use the function defined below, but we may specify
some of the parameters in each section. Let c(j) denote the completion time of job j in a
feasible schedule S and let d denote the common due date. E denotes the set of early jobs
defined by: E = {j I c(j) ::; d}, and T denotes the set of tardy jobs, T = N - E. Now,
if f(·) and g(.) denote real-valued and nondecreasing functions defined on the real and
non-negative numbers so that f(O) and g(O) are normalized to 0 without loss of generality,
and if w(j) denotes a tardiness penalty for job j, the objective function can be written
as:
F(S,d) = Lf(d-c(j))+ Lw(j)+g(d).
jEE JET
The first part of the sum F(S, d) is called the earliness costs, the middle part is called
the tardiness costs and the last part is called the due date assignment costs.
For the single machine problem with an adjustable common due date, we develop an
O( n log n) algorithm for the problem with equal tardiness penalties (w( i) = w(j), V jobs i
and j). The theoretical basis ofthis algorithm has been discussed in Cheng & Kahlbacher[3].
Then we state the problem with arbitrary tardiness penalties and give a reduction of the
Knapsack Problem to the general problem which shows that the problem is NP-hard.
Nevertheless, a special case, the problem with linear due date assignment costs but with-
out earliness costs, can be solved with a linear time algorithm.
We first note that the single machine problem with an externally given common due
date is N P-hard for arbitrary tardiness penalties even if the earliness costs are neglected.
This result is due to Lawler & Moore[7] who proved an equivalence between the Scheduling
Problem and the Knapsack Problem. We, therefore, restrict the problem to the equal
tardiness penalties case. We distinguish between two similar models. In the first model,
it is possible to have idle periods on the machine, and in the second one this is forbidden.
Please, note that an idle period during the process is of course of no use, but an idle
period at the beginning may delay all of the early jobs and decrease the earliness costs
without changing the tardiness costs. If idle times are allowed, the problem can be solved
in O( n log n) time; for the case that idle times are forbidden, we present an approach that
solves the problem in pseudopolynomial time, O(nd), using dynamic programming. The
latter approach is strongly influenced by the method developed in Kahlbacher[6].
We next discuss the multiple machine problem with an adjustable common due date
and equal tardiness penalties since, even with this restriction, most of the problems are
N P-hard. In this section, we shall give a proof that the problem is N P-hard even if we
do not consider earliness costs. This is shown by reducing the Partition Problem to the
2-parallel machine Scheduling Problem with constant tardiness penalties and linear due
date assignment costs. Motivated by this result, we then neglect the due date assignment
289
costs, but we include earliness costs; of course, without earliness and due date assignment
costs the problem becomes trivial (assigning all jobs a sufficient large due date, e.g. the
sum of all processing times, guarantees that none of the jobs will be tardy). It turns
out that the problem with earliness costs where machine idle times are forbidden as well
as the problem with machine idle times allowed is NP-hard. For the reduction, we use
the Partition Problem and the Even-Odd Partition Problem respectively. But, if we
consider linear earliness costs, the problem with idle times is solvable in O( n log n) time.
Then, it is shown that even for linear earliness costs, adding linear due date assignment
costs changes the problem complexity: it becomes NP-hard (again we use the Even-Odd
Partition Problem for the reduction).
The last model discussed is the multiple machine problem with an externally given
common due date. We prove that the problem is NP-hard even for linear earliness costs
and equal tardiness penalties. This is true regardless of machine idle times being allowed
or forbidden. For this reduction, we use again the Even-Odd Partition Problem stated in
Garey, Tarjan & Wilfong[5].
References
[1] Baker, K. R., and G. D. Scudder. Sequencing with earliness and tardiness penalties:
A review. Operations Research, 38(1):22-36, 1990.
[2] Cheng, T. C. E., and M. C. Gupta. Survey of scheduling research involving due
date determination decisions. European Journal of Operational Research, 38:156-
166, 1989.
[3] Cheng, T. C. E., and H. G. Kahlbacher. Single-machine scheduling to minimize
earliness and number of tardy jobs. Preprint No. 179, Universitiit Kaiserslautern,
FB Mathematik, 1990.
[4] Garey, M. R., and D. S. Johnson. Computers and Intractability - A Guide to the
Theory of NP-Completeness, Freeman, 1979.
[5] Garey, M. R., R. E. Tarjan, and G. T. Wilfong. One-processor scheduling with
symmetric earliness and tardiness penalties. Mathematics of Operations Research,
13(2):330-348, 1988.
[6] H. G. Kahlbacher. Scheduling with monotonous earliness and tardiness penalties.
In Proceedings of the Second International Workshop on Project Management and
Scheduling, 330-349, Compiegne 1990.
[7] Lawler, E. L., and J. M. Moore. A functional equation and its application to resource
allocations and sequencing problems. Management Science, 16(1):77-84, 1969.
Exact Solution of Multiple Traveling Salesman
Problems
J. Gromicho, J. Paixao and 1. Bronco*
This paper presents a method developed for the multiple traveling salesman problem (m-
TSP), which is a generalization of the well known TSP [6]. In the m-TSP, there are
m salesmen who are required to visit n customers in such a way that all customers are
visited exactly once by exactly one of the salesmen. Hence, each salesman leaves from
and returns to the same point, the depot, and each one of them completes a tour visiting
a subset of the customers.
The m-T S P has been studied by several authors [3, 5]. If the problem includes simple
capacity constraints associated with the salesmen, it becomes the Capacitated Vehicle
Routing Problem, see [7].
The present paper is mainly concerned with the asymmetric version of the problem
with no capacity constraints. Also, we are interested in dealing with 'almost-symmetric'
instances where only a small portion of the costs is not symmetric. In fact, these types
of instances have proved to be very hard to solve by the available algorithms.
In this paper, we deal with the following mathematical formulation for the m-T S P:
Minimize LLCijXij (1)
iEV jEV
subject to: LX''J 1 ('VjEV-{n+l}) (2)
iEV
LXij 1 ('Vi E V - {n + I}) (3)
jEV
LX"'J m (j=n+l) (4)
iEV
LXij m (i=n+l) (5)
jEV
LLXij > I ('VS c V : S i- 0, S i- V) (6)
iES j~S
x··'J E {a,l} ('Vi,j E V) (7)
'University of Lisbon, DEIOC-FCUL and CEAUL-INIC, Av. 24 de Julho, 134 - 5£., 1300 Lisboa,
Portugal.
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgul et al.
© Springer-Verlag Berlin Heidelberg 1992
292
Although constraints (3) and (5) are redundant, we keep them in order to produce a
quasi-assignment relaxation( QA-relaxation) by dropping out constraints (6).
Polynomial algorithms are known for the quasi-assignment problem [8, 1]; so one can
produce lower bounds on the optimal value for the m-T SP and, feasible solutions can be
efficiently derived from the quasi-assignment solutions by 'patching' heuristics [4].
In order to strengthen the lower bounds obtained from the QA- relaxation, we apply
an additive bounding procedure following the techniques presented in [2].
All of these techniques - QA-relaxation, 'patching' heuristics and additive bounding
method - are embedded in a tree-search procedure using a branching rule based on
penalties computed for the subtours. Computational experience carried out on PCs (MS-
DOS) has proved to be successful for a wide range of randomly generated problems with
up to 120 customers. It also shows the effectiveness of the QA bound in purely asymmetric
instances with or without the triangular inequality. The quality of the bound produced
by the additive approach only seems to compensate the extra amount of computational
effort needed to compute it in symmetric or 'almost-symmetric' cases where it is highly
effective.
References
[1] Branco, 1. M. Algoritmos para modelos matematicos de quasi-afecta<;ao e extensoes.
Tese de Doutoramento, DEIOC-FCUL-University of Lisbon, Portugal, 1989.
[2] Fischetti, M., and P. Toth. An additive bounding procedure for combinatorial
optimization problems. Operations Research Society of America, 37(2):319- 328,
1989.
[3] Gavish, B., and S. Graves. The Traveling Salesman Problem and related problems.
Working paper 7906, Graduate School of Management, University of Rochester,
Rochester, NY, 1979.
[4] Gromicho, J. Limites e enumera<;ao para 0 problema do Caixiro Viajante Multiplo
Assimetrico. Tese de Mestrado, DEIOC-FCUL-University of Lisbon, Portugal, 1990.
[5] Laporte, G., H. Mercure, and Y.Nobert. An exact algorithm for the Asymmetrical
Capacitated, Vehicle Routing Problem. Networks, 16:33-46, 1986.
[6] Lawler, E. L., J. K. Lenstra, A. H. G. Rinnooy Kan, and D. B. Shmoys (eds.). The
Traveling Salesman Problem: A Guided Tour of Combinatorial Optimization. Wiley,
N.Y., 1985.
[7] Lenstra, J. K., and A. H. G. Rinnooy Kan. On general routing problems. Networks,
6:273-280, 1976.
[8] Paixao, J. Pj' and 1. M. Branco. A quasi-assignment algorithm for bus scheduling.
Networks, U:249-269, 1987.
A Nonlinear Two-Stage Cutting Stock Problem
J. M. Valerio de Carvalho and A. J. Guimaraes Rodrigues*
In this communication, we present a two-stage cutting stock system that arIses III
a make-to-order steel industry. Orders are accepted on a monthly basis, and planning
and production must be completed within a month. Client specifications include weight,
width, thickness, hardness, etc. According to the characteristics, the orders are split into
different groups of raw materials. For each final product there is a well-defined sequence
of operations.
Besides the cutting operations, the material must also be processed to comply with the
desired specifications. Processing includes, among other operations, reducing thickness in
a roll mill and softening the material to the required hardness. Every month, there are
several different cutting stock problems, one for each group. The width of the initial rolls
vary from 932 mm to 1040 mm, and it is fixed for each group. The width of the orders
vary tipically from 7.0 mm to 500 mm.
A peculiarity of the system is that the rolls must be cut in two distinct cutting phases,
the rolls being processed between the cutting operations. In the first phase, the raw ma-
terial rolls are cut into intermediate rolls. Each intermediate roll comprises one or more
final rolls and an extra width for scrap (typically 5 to 6 mm). This scrap is necessary
to guarantee the quality of the final product. The number of final rolls within an inter-
mediate roll is constrained not only by the cutting capacity of the second phase cutting
machines but also by the width of the roll mills and other processing machines. Final rolls
are associated in intermediate rolls for two reasons: first, the first cutting machine has
cutting capacity restrictions; second, the processing work depends heavily on the number
of intermediate rolls generated. After processing, the rolls are finally cut (second cutting
phase) to the required final width.
The demand characteristics vary substantially within each group. We define two
different strategies to solve the different problems. For groups where there is a small
number of orders and the quantities ordered are quite high, which makes the problem
amenable to a linear programming approach, we introduce a model to deal with the non-
linearities of the system. For other groups (see below), heuristics provide better solutions.
'Dept. de Produc;ao e Sistemas, Universidade do Minho, 4719 Braga Codex, Portugal.
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgiil et al.
© Springer-Verlag Berlin Heidelberg 1992
294
The linear programming formulation objective is to minimize a cost function that
weights the total trim loss (generated in both cutting phases) and the number of in-
termediate rolls generated. A delayed pattern generation technique, a variation of the
Gilmore-Gomory approach [1] was adopted to tackle this problem. The coeficients of the
A matrix of the main LP problem express the number of final rolls in the intermediate roll.
The coeficients of the knapsack subproblem constraint are the widths of the intermediate
rolls. Due to the extra scrap, these widths are non-linear with respect to the number of
final rolls. Therefore, the knapsack subproblem must consider all possible intermediate
rolls. The necessary conditions for a cutting pattern to be a candidate column are derived.
Martello and Toth [3] provide a good algorithm to deal with this knapsack problem if the
number of possible intermediate widths is not too big. The LP solution can be easily
massaged to get integrality. The non-linearity described makes the previous LP approach
of Haessler [2] not applicable to this problem.
For groups with a large number of orders of small volume, the following heuristic
procedure provides good quality solutions. The heuristic is divided in two main steps.
First, a combination of intermediate rolls that sub sums the required number of final rolls
is sought. These combinations take into account the minimization of the number of inter-
mediate rolls generated. Secondly, the intermediate rolls are packed in the useful width
of the initial rolls. This phase consists of a greedy heuristic that uses a lexicographically
decreasing enumeration process. The cutting patterns are generated as an aspiration level
(based in trim loss) is obeyed. If the aspiration level is not attained in the last cutting
patterns, a local optimization is applied.
References
[1] P.C. Gilmore and R.E. Gomory. A linear programming approach to the cutting
stock problem - part ii. Operations Research, Vol 11, p. 863-888, 1963.
[2] R. Haessler. Solving the two-stage cutting stock problem. Omega, The Intl.JZ. of
Mgmt Sci., Vol 7, p. 145-171, 1979.
[3] S. Martello and P. Toth. An exact algorithm for large unbounded knapsack prob-
lems. Operations Research Letters, Vol 9, p. 15-20, 1990.
The Probabilistic Behavior of the
Generalized HARMONIC Algorithm for the
On-Line Multi-Dimensional Bin Packing
J. Csirik and E. MatC§*
1 Introduction
In the classical one-dimensional bin-packing problem, we are given a list of numbers
(items) in the interval (0,1]' which must be packed into a minimum number of unit-
capacity bins (i.e. bins that can contain items totalling at most 1). It is well known
that this problem is NP-hard, and accordingly a number of approximations have been
developed for its solution. These algorithms can be analysed from the worst-case and the
probabilistic points of view. One such algorithm is the HARMONIC T algorithm. In this
case the items are classified into r categories, according to how many items of the same
size may be packed into a bin. The r-th, i.e. the last category will contain all the items
which are less than or equal to l/r. This algorithm is efficient from both points of view,
and it has the further important advantage that it is an O(n)-time and O(l)-space on-
line algorithm. The worst-case behaviour of this algorithm was analysed in [3], and some
improvements are given in [4] and [5]. The probabilistic analysis of the one-dimensional
case was performed in [1] and [2].
This summary analyses the probabilistic behaviour of an algorithm which is a gen-
eralization of the one-dimensional HARMONIC T to solve the d-dimensional bin-packing
problem.
2 Definitions and results
Let us suppose that the items of the list L= (AI, A 2 , • •• , An) are independent and uni-
formly distributed in the d-dimensional interval (0, l]d. An item A = (aI, a2, ... , ad) will
• J6zsef Attila University, of Szeged, Hungary, Arpad ter 2
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgiil et al.
© Springer-Verlag Berlin Heidelberg 1992
296
be called an (ir,i 2 , ••• ,id)-type item if aj Eli) (j = 1,2, ... ,r), where Ii = (l/(i + 1),1/i}
if i = 1,2, ... , r -1, and IT = (0, l/r} if i = r. Each bin will contain only the same type of
items. A bin B intended to pack (ir, i 2 , ••• , id)-type items will be called a B i1 .i2. ....id bin.
Clearly, a B;"i 2 , •.•• id bin has room for exactly i1 . i2 ..... id items (of type (iI, i 2, ... , i d ))
if iI, i 2, ... , id < r. If i j = r for some j-s, we still pack only i l . iz ..... id items into
a bin, even if we could pack more. Now, if the next item from the list L is given, we
pack it into the last bin of the same type. If this bin is full, or if there is not yet such a
bin, we open a new empty bin for the item to be packed. Let ki1,iz, ...•id be the number of
h, i z, . .. , id)-type items. d
E (k- - -) - n rr If. I21,12, ... ,'Id - f.)
j=1
(IIil is the length of Ii) because of the uniform distribution and of the independence of the
conditions for the dimensions. We can pack n:=l i j such items into a bin, so the expected
value of Hd.r(n)/n is
n rrd l~i)1
t-
}=l J
where R = {1, 2, ... , r}. This equation is only approximately true because of rounding
errors, but the difference between the sides is less than rd /n since the number of different
bin types is rd. Clearly the right side is equal to
because of
T-l (1 1 1) 1= L -:z - "7 + Tl + rZ
3=1 J J J
?r 2
6-1.
Since we need at least n/2d bins (this is the expected volume of the n items) in the
optimal packing of the list L, the generalized HARMONICT algorithm has an asymptotic
efficiency of
297
References
[1] Csirik, J., J. B. G. Frenk, A. M. Frieze, G. Galambos, and A. H. G. Rinnooy Kan.
A probabilistic analysis of the Next Fit Decreasing bin-packing heuristic. Gp. Res.
Leiters, 5: 233-236, 1986.
[2] Csirik, J., and E. Mate. The probabilistic behaviour of the NFD bin-packing algo-
rithm. Acta Cybernetica, 7: 241, 1986.
[3] Lee, C. C., and D. T. Lee. A simple on-line bin-packing algorithm. J. Assoc.
Comput. Mach., 32: 562-572, 1985.
[4] Ramanan, P., D. J. Brown, C. C. Lee, and D. T. Lee. On-line bin packing in linear
time. J. of Algorithms, 10: 305-326, 1989.
[5] Richey, M. B. Improved bounds for refined harmonic bin packing. Manuscript.
Efficient Labelling Algorithms for the Maximum
Noncrossing Matching Problem
Federico Malucelli and Daniele Pretolani *
1 Introduction
Consider a bipartite graph G = (0, D, E) where 0 and D are origin and destination node
sets respectively (I 0 I = ID I = n), and E is a set of edges (i, j), i E 0 and JED (I E 1= m).
Suppose we draw the origin nodes and the destination nodes arranged in two columns
and the edges as straight line segments between origins and destinations. A noncrossing
matching is a subset of edges M ~ E such that no two edges of M intersect (including
intersections at nodes). The Maximum Non Crossing Matching (MNCM) is the problem
of finding the noncrossing matching of maximum cardinality. Let p denote the cardinality
of the MNCM. Problems arising in several fields can be modelled as MNCM: for example
the 3-Side Switch Box in VLSI design has been presented in [2] where an O(n 2 ) time
algorithm is proposed. The problem can be reduced to the one of finding the longest
increasing subsequence in a permutation of size m [1]. An algorithm for this problem has
been proposed by Fredman and slightly improved by Widmayer and Wong[7]; in our case
the algorithm complexity is O( m + (m - p) log p). In this paper some labeling algorithms
for the MNCM, which work directly on the bipartite graph, will be proposed. The overall
complexity is improved to Oem log log n) or to Oem +min{ np, (m - p) log p}). Finally the
weighted case is considered.
2 The labelling algorithm
We identify both origin nodes and destination nodes with numbers in the set 1,2, ... , n;
the nodes are numbered in increasing order from the top to the bottom, hence two edges
(i,j) and (h, k) cross iff (i :s: hand j ~ k) or (i ~ hand j :s: k).
*Dipartimento di Informatica, Universit' di Pisa, Corso Italia 40, 56125 Pisa, Italy.
NATO ASI Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgiil et al.
© Springer-Verlag Berlin Heidelberg 1992
300
The algorithm is organized in two phases: a first phase during which labels are assigned
to all the edges of E; a second phase during which the edges of the MNCM are selected.
The label L(i,j) corresponds to the cardinality of the partial MNCM that includes (i,j)
and lies entirely above it (i.e. it includes only edges (h, k) such that (h < i and k < j).
The value of the maximum assigned label gives the cardinality of the MNCM for G as can
be easily proved [2]. The selection of the edges in an MNCM can be carried out easily
in Oem) time provided that the edges are arranged in a suitable data structure (i.e. a
bucket list).
3 Implementation and complexity
In our algorithm, the techniques described in [1, 7] are modified to work on general
bipartite graphs. We assign a label LN(j) to each destination node j, where LN(j) is
the maximum label assigned to an edge incident to j. During the algorithm, we maintain
the sequence P = P(D), P(l), ... , P(K), where K is the maximum label L(e) currently
assigned; P(k) is the smallest destination j such that LN(j) = k. It is easy to see that
for each edge (i,j):
L(i,j) = 1 + max{LN(k) : k < n = 1 + max{k : P(k) < n. (1)
The complexity of our algorithm depends on the implementation of the max operation in
(1). If P is implemented as a vector, we can perform a binary search on P for each edge
(i,j); the resulting complexity is Oem + min{np, (m - p) logp}).
We can label each edge in less than logarithmic time using a bounded dictionary [5] or
the priority queue defined in [6]. For each edge (i,j), the max operation and the updating
of P can be carried out in O(log log n) time; the overall complexity of the algorithm
becomes O(mlog log n) with an Oem) space bound.
4 The weighted case
Let wij be a real number associated with each edge (i,j) E E. The Maximum Weight
Noncrossing Matching (MWNCM) is defined as the non crossing matching M with the
maximum sum of wij over (i,j) E M. Note that the MWNCM is not necessarily the
MNCM. The basic structure of the algorithm remains unchanged, and each label L(i,j)
gives the weight of the partial MWNCM which includes edge (i,j) and lies entirely above
(i,j). Consequently, the labelling operation becomes:
L(i,j) := wij + max{LN(k) : k < n.
301
In order to compute max{LN(k), k < j} efficiently, a data structure that allows multi-
dimensional search is required, such as the priority search tree(PST) [4]. The resulting
complexity of the algorithm is D( m log n). The space requirement for the PST is D( n).
Another stimulating problem is the one of finding the Maximal NCM of minimum weight,
which seems to be more difficult than the MWNCM: it is possible to devise a trivial D( m 2 )
algorithm; the possibility to improve this complexity deserves further investigations.
References
[1] M.L. Fredman. On computing the length of longest increasing subsequences. Dis-
crete Mathematics, 11(1):29-35, 1975.
[2] Kajitami, Y., and T. Takahashi. The non cross matching and applications to the
3-side switch box routing in VLSI layout design. Proc. International Symposium on
Circuits and Systems, 776-779, 1986.
[3] E. Lawler. Combinatorial Optimization: Networks and Matroids, Holt, Rinehart
and Winston, (1976).
[4] E. M. McCreight. Priority search trees. Siam Journal on Computing, 14(2}:257-276,
1985.
[5] Melhorn, K., and S. N"aher. Bounded ordered dictionaries in O(log log N) Time
and D(n) space. Information Processing Letters, 35:183-189, 1990.
[6] P. van Emde Boas. Preserving order in a forest in less than logarithmic time and
linear space. Information Processing Letters, 6(3):80-82, 1977.
[7] Widmayer, P., and C.K. Wong. An optimal algorithm for the maximum alignment
of terminals. Information Processing Letters, 10:75-82, 1985.
A Phase I That Solves Transportation Problems
Konstantinos Paparrizos *
1 Introduction
Recently, a number of exterior point simplex algorithms for assignment problems have
been developed [1,2,6, 7]. Preliminary computational results on assignment problems [2]
indicate that the algorithms are efficient in practice. These algorithms are initialized with
a dual feasible basis. Then dual feasibility is destroyed to be restored again at optimality.
In this paper we extend the results in [7] to TPs. The original problem is modified by
introducing an artificial variable to every equality constraint of the original problem and
the algorithm is then applied to the modified problem. The initial basic solution consists
of all the artificial variables. The absolute values of two artificial variables are reduced by
an equal amount at every nondegenerate iteration. Primal and dual feasibility of the TP
is reached simultaneously.
Except for the determination of the incoming arc, the algorithm can be seen as the
network simplex algorithm [4] applied to a specially structured minimum cost network
flow problem. However, the reduced cost of the incoming arc is unrestricted in sign.
Similarities with primal-dual approaches also exist.
Familiarity with the results in [7] is assumed.
2 Preliminaries
Given a directed bipartite graph G = (A, B, N), where A and B are node sets containing
m and n nodes respectively and N is the set of arcs (i,j) directed from a node i E A to
a node j E B, the TP is:
·Department of Mathematics, Democritus University of Thrace, Xanthi 67100, GREECE
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgiil et al.
© Springer-Verlag Berlin Heidelberg 1992
304
mm 2:(i,j)EN CijXij
S.t. 2:(i,j)EN Xij = (Xi, i E A
2:(i,j)EN Xij = f3j, j E B
Xij ~ 0, (i,j) E N,
where ai and bj are positive integers, IAI = m, IBI = nand
La; = Lbj = S.
iEA jEB
A basic solution to TP is a tree T of the graph G. Associated with T are the primal
variables x;j(T) computed by setting xij(T) = 0, (i,j) EN, the dual variables ui(T), i E
A, and vj(T),j E B, computed by setting ui(T) - vj(T) - Cij = 0, (i,j) E N and the
reduced costs wij(T) = ui(T) - vAT) - Cij, (i, j) E N. T is primal (dual) feasible if
x;j(T) ~ °(wij(T) ~ 0), V(i,j) E N. A tree that is both primal and dual feasible is
an optimal solution to (TP). T + (g, h), (g, h) tj. T, contains a unique cycle Cgh(T). By
'deleting (k,1) E Cgh(T) a new tree T + (g, h) - (k, 1) is constructed. The interchange
of (g, h) and (k,l) corresponds to a pivot operation of the simplex method. The arcs in
Cgh(T) directed opposite to (g, h) are called backward arcs. The set of backward arcs is
denoted by Bgh(T).
3 The Algorithm
An artificial intermediate node °with bo = °and zero-cost artificial arcs (i, 0), i E
A, and (O,i), j E B, are introduced to construct a new graph E = {R,M} with node
set R = A + B + {a} and arc set M containing all the original and artificial arcs. The
modified problem (MP) is the minimum cost flow problem defined on E. The algorithm
is applied to MP.
Every tree is rooted at the node 0. The in-tree arcs directed away from (towards) the
root are called downward (upward) arcs. Deleting from T an arc (i,O), i E A, ((O,j),j E
B) a subtree Ii (Tj ), called a distinguished tree, is cut off from the root. The union of all
the distinguished trees Ti (Tj) is a forest S(T) (D(T)) called the surplus (deficit) forest.
Finally, we set N(S,D) = ((i,j) EN: i E S(T), j E D(T)}.
The algorithm is formally stated as follows.
STEP 0. Start with the tree T containing all the artificial arcs.
305
STEP 1. If N(S, D) = 0 or SeT) = 0, STOP. Otherwise, compute w;j(T)
and set wgh(T) = max{w;j(T) : (i,j) E N(S,D)}.
(i,j) EN
STEP 2. Compute x;j(T) and set Xkl(T) = min{x;j(T) : (i,j) E Bgh(T)}. In case of
ties (k,l) is the first backward arc met when Cgh(T) is traced in the direction of
(g, h) starting from the root.
STEP 3. Set T f- T + (g, h) - (k, I) and go to step 1.
Observe that the arc to be deleted is chosen with Cunningham's [4] pivoting rule.
4 Correctness and Complexity
Given a tree T, an arc (i,j) E T is degenerate if xij(T) = O. A pivot operation that
deletes a degenerate arc is called a degenerate pivot. A primal feasible rooted tree is
called a strong basis if every degenerate arc is downward (see [3],[4]).
It is easily seen that the initial tree is a strong basis. As the arc to be deleted is chosen
by Cunningham's [4] pivoting rule, every tree generated by the algorithm is a strong basis.
It is also easily seen that Cgh(T) contains the root and, hence, two backward artificial
arcs. As a result, if a pivot is nondegenerate, two artificial variables are reduced by
an integer. If a degenerate pivot is applied to T producing the adjacent tree T*, the
arc (k, I) E T - D(T) and, hence, SeT) E S(T*). As there are at most m nodes in the
surplus forest after at most m -1 degenerate pivots a nondegenerate pivot is applied. The
maximum number of consecutive degenerate pivots together with the next nondegenerate
one is called a stage. Therefore, in at most S stages every artificial variable becomes zero.
Simultaneously, the surplus forest becomes void and the algorithm stops.
The work in a single iteration required to determine the arc to be deleted and update
the tree and the variables is CJ (m + n). All this work in a stage is CJ (m( m + n)). Using
Fibonacci heaps [5], it can be shown that the work in a stage required to determine the
incoming arcs is CJ (INllogm). Therefore we showed
Theorem 1 The algorithm is well defined. It stops in at most mS iterations and in at
most 0 (S(m(m + n) + INllogm)) elementary operations.
In order to show correctness we observe first that the algorithm minimizes the sum of
the in-tree artificial variables. Hence, if N(S, D) = 0, TP is not feasible.
Assume now that the algorithm stops because SeT) = 0. We have already shown that
each artificial variable is zero. Hence, the optimal solution to MP is feasible to TP. The
set of the primal (dual) variables of TP is a proper subset of the set of primal (dual)
306
variables associated with MP. As the optimal solution to MP is basic, the complementary
slackness conditions associated with TP are satisfied.
In [7] it is shown that the optimal solution to MP is dual feasible to TP thus estab-
lishing the correctness of the algorithm.
5 Concluding Remarks
In implementing the algorithm there is no need to construct MP. It suffices to update the
two forests and the values of the artificial variables. Updating forests seems to be cheaper
than updating trees. As there is no initialization procedure (the initial forest contains
no arc) and the algorithm is very simple, it might be efficient in practice. Regardless of
whether the algorithm is efficient or not, its generalization to linear problems seems to be
a valuable practical result.
References
[1] Achatz, H., Kleinschmidt, P. and Paparrizos, K. A dual forest algorithm for assign-
ment problems, Applied Geometry and Discrete Mathematics, to appear.
[2] Akgul, M. and Ekin, O. A dual forest algorithm for the linear assignment problem,
RAIRO Operations Research, to appear.
[3] Barr, R., Glover, F. and Klingman, D. The alternating path basis algorithm for the
assignment problem, Math. Prog. 13: 1-13,1977.
[4] Cunningham, W. H. A network simplex algorithm, Math. Prog. 11: 105-116 1976.
[5] Fredman, M. 1. and Tarjan, R. E. Fibonacci heaps and their uses in improved
network optimization algorithms, Journal of the ACM 34: 596-615 1987.
[6] Paparrizos, K. An infeasible (exterior point) simplex algorithm for assignment prob-
lems, Math. Prog., to appear.
[7] Paparrizos, K. A simplex type algorithm for assignment problems initialized with a
solution that is neither primal nor dual feasible, manuscript 1990.
A Polynomially Bounded Dual Simplex Algorithm
for Capacitated Minimum Cost Flow Problem
Canan A. Sepil and Ay§egiil Altaban*
In this paper, we present a dual simplex algorithm for solving the capacitated minimum
cost flow problem.
Given a graph GI = (VI, EI) with VI = Vs U Vs and V s n Vs = 0, an extended graph
G = (V, E) with V = VI U {a} and E = EI U {(a,j),j E VB} is constructed. Here V s is
the set of supply nodes in G I • An initial dual feasible solution is obtained by solving the
shortest path problem using the costs as the weights for each arc. The negative of the
dual variable associated with each node is taken to be equal to the length of the shortest
path to that node.
The basic underlying idea of the algorithm is the notion of "updated flows". The main
purpose of using this concept is to find those arcs whose primal feasibility violations are not
affected by other arcs' primal infeasibilities. Given a basic primal flow Xi corresponding
to a dual feasible tree, we define the updated flow x~ of ei, as the net primal flow on arc
ei and determine it as follows.
Initially for a given dual feasible tree T, we set x; to be equal to Xi for all arcs ei E T.
Then starting from tip nodes in T, all arcs ei with x; < a or xi > Ui are eliminated.
Once an arc is eliminated, the xj values in remaining arcs of T are updated using a flow
value set equal to a or Ui for the eliminated arc. Mathematically, Xl can be represented
as follows:
where
OR(j) = +1 or -1 depending on the orientation of ej,
Z} = {ei: ei E Ep n Ej,j -=J i,x; < a}, ZJ = {ei: ei E Ep n Ej,j -=J i,x~ > Ui},
ZJ = {ei: ei E EB n Ej,j -=J i,X: < a}, ZJ = {ei: ei E EB n Ej,j -=J i,x~ > Ui},
and Ej is the set of arcs in the subtree Tl obtained from T by dropping arc ej and taking
'Middle East Technical University, Industrial Engineering Department, Ankara, Turkey.
NATO ASl Series, Yol. F 82
Combinatorial Optimization
Edited by M. Akgiil et at.
© Springer·Yerlag Berlin Heidelberg 1992
308
the subtree which does not contain the root node.
Let Q = Ql U Q2 U Q3 U Q4, where
Ql is the set of arcs ei E EF with x~ < 0,
Q2 is the set of arcs ei E EF with x~ > Ui,
Q3 is the set of arcs ei E EB with x~ < 0,
Q4 is the set of arcs ei E EB with x~ > Ui.
Here, EF is the set of forward arcs, and EB is the set of backward arcs on a given tree
of G = (V, E). We use the term" arc ei leaving the tree" when the corresponding dual
slack varible enters the dual basis. The leaving arc el will be selected in such a way that
if el E Qi, then there cannot be any other arc in the subtree induced by arc el which
is an element of Q - Qi, and el should be the minimum-depth arc in the corresponding
subtree. The selection of the entering arc depends upon the orientation and type of primal
infeasibility of the leaving arc.
After a particular pivot, if the total deficiency in the subtree Tl decreases, this re-
duction can affect the deficiency of the arcs (ei E Pg ) and can cause an increase in the
deficiencies of these arcs, where Pg is the unique path from the arc ej to the root in the
tree. A polynomial bound can be obtained if the deficiency reduction in Tl is greater than
or equal to the deficiency increase of those arcs in Pg •
The following lemma forms the basis for the proof of convergence of the algorithm and
the computational bound which is established in Theorem 1.
Lemma 1: If none of the arcs e a E Pg have the same type of violation as the leaving arc
or are not at their bounds for the same violation type, then the total deficiency of the tree
reduces by at least one after the pivot. On the other hand if all the infeasible arcs have
the same type of violation, then the deficiency may remain the same.
The computational bound of the algorithm is given in the following theorem.
Theorem 1: The algorithm requires at most (I V 12 M) pivots, where M is the total
supply value.
We have developed a scaled version of the algorithm where the supplies, demands
and the arc capacities are scaled in a way similar to Edmond and Karp's approach. The
following lemma presents the computational bound of the scaled version.
Lemma 2: DZ, the total deficiency at the beginning of subtree z (z ~ 1), is bounded by
IVI+IEI·
The computational bound with scaling may be given as follows.
Theorem 2: The total number of pivots in the scaling algorithm is bounded by
(7 + 1)(1 V 1+ 1El)I V 12 where 7 is the scaling parameter.
Formulation and a Lagrangean Relaxation
Procedure for Solving Part Scheduling and Tool
Loading Problems in FMS
Kannan Sethuraman*
Marshall L. Fisher t
Alexander H.G. Rinnooy Kan:j:
1 Introduction
The growth of the metal-working industry spawned improvements in computer integrated
manufacturing technology. One result of such improvements was the development of Flex-
ible Manufacturing Systems (FMS). A Flexible Manufacturing System typically consists of
versatile numerically controlled machines, connected by an automated material handling
system, all under a central computer control. The FMS's achieve the enviable results of
effectively combining the efficiency of transfer lines and flexibility of job shops by eliminat-
ing or reducing set-up or change over times between manufacturing operations. Although
FMS's can offer a wide variety of benefits, the flexibility of these systems has resulted in
making the design and subsequent operation of FMS's very complex.
The tool loading problem and scheduling of jobs in an FMS are addressed in this
paper. The problem deals with the assignment of tools and jobs to a set of available
machines and the determination of a schedule for each of those machines. In the past,
researchers have opted for sequential decision-making where the tool loading problem is
considered prior to the scheduling problem. Although such an approach may result in a
more tractable problem, it is not clear how much has been sacrificed. In this paper, we
develop a model in which loading and scheduling decisions are made simultaneously. In
'The Wharton School, University of Pennsylvania.
tThe Wharton School, University of Pennsylvania.
lErasmus University, Rotterdam.
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. AkgOl et al.
© Springer-Verlag Berlin Heidelberg 1992
310
the following section, we briefly review the relevant literature and in section 3, we describe
the characteristics of the problem addressed in this paper.
2 Literature Review
A set of production planning problems that must be solved for efficient use of an FMS was
discussed by Stecke [3]. It addressed specifically the grouping and loading problems. She
formulated them as nonlinear 0-1 mixed integer programs and provided solution method-
ologies for these problems. Rajagopalan [2] presented a mixed integer linear programming
formulation for grouping part types with various operations on an FMS with some con-
straints on tool magazine capacities. Kusiak [1] has developed a mathematical program
for loading problems (i.e. allocating batches to machines). Kusiak combines elements
of the generalized assignment problem and the generalized transportation problem. A
recent paper by Van Vliet and Van Wassenhove [4] reviews the application of Operational
Research Techniques to the planning of Flexible Manufacturing Systems.
Although there have been several hundreds of papers published in the area of schedul-
ing, there has been a dearth of literature that deals directly with the loading and schedul-
ing of jobs in an FMS. In this paper, we attempt to provide a model which integrates
these two decisions.
3 Problem Description
In this model, we need to schedule a set of part-types on a given set of versatile machines.
Each of these machines are equipped with a tool magazine. In flexible manufacturing sys-
tems, tool magazines significantly increase the efficiency of the tooling activities. Without
the tool magazine, the tool setup would involve transferring a tool from a tool storage
room to the spindle and this would incur a considerable amount of time. Here, the tool
magazine acts as a secondary storage and tools may be exchanged between the main stor-
age room and the magazine leading to a magazine setup, or between the magazine and
the spindle resulting in a spindle setup. Automatic tool exchange mechanism in FMS's
enables us to perform spindle setups in negligible amount of time. Instead of having only
one tool available to each machine, a flexible machine has available a finite set of tools.
The tool magazine capacity restricts the number of tools that can be mounted at any
instant of time.
The problem characteristics and some of the basic assumptions of the model are described
below:
311
• In our problem, we are dealing with a job shop consisting of M machines, each of
them capable of handling a variety of operations. The restriction imposed is that
each machine can perform only one operation at a time.
• There are n jobs in the system and all of them are available at time zero for imme-
diate processing. Each job j consists of Ii operations which must be performed in
a specified order: 1,2, ... , fj.
• The machine on which operation i of job j must be performed is a limited choice.
The machines on which the operations need to be performed are not pre-determined.
• No preemption of task performance is allowed. Once a task ij is started, it must be
processed until its completion.
• The processing time for operation i of job j on machine m is given by Pjim. The
processing time is assumed to depend on the machine on which the operation is
performed.
• Each job has a due date dj associated with it. If the job j is late then a penalty of
Wj is imposed for each delayed day.
4 Methodology
In this paper, We formulate the integrated loading and scheduling problem as a zero-
one integer program. This integer program will load jobs onto the machines, determine
the tools to be mounted on these machines at different time instants and it will also
provide a schedule for each of these machines. Since the number of integer variables
required to obtain an optimal schedule is quite large for even a small problem, we provide
a "Lagrangean relaxation approach for obtaining lower bounds on the optimal solution of
the problem. The details of the formulation and the relaxation procedure are discussed
in [5].
References
[1] Kusiak, A., Loading models in FMS, in Raouf, A. and Ahmad, S.l. (Eds.), Recent
Developments in FMS, Robotics, CAD/CAM, 119-131.
[2] Rajagopalan, S.K., Formulations and heuristic so lutions for parts grouping and
tool loading in FMS, in K.E. Stecke and R.J.Suri (eds.): FMS, Operations Research
Models and Applications, 119-131.
312
[3] Stecke, K.E., Formulation and solution of nonlinear integer production planning
problems for flexible manufacturing systems, Management Science, Vol. 29, No.3,
273-280.
[4] Van Vliet M. and van Wasenhove L.N., Operational Research techniques for analyz-
ing flexible manufacturing systems, working paper no. 8932/ A, Erasmus University,
Rotterdam.
[5] Sethuraman K., Fisher, M.L. and Rinnooy Kan A.H.G., For mulation and a La-
grangean Relaxation Procedure for Solving Part Scheduling and Tool Loading Prob-
lems in FMS, in preparation, 1991.
Euclidean Steiner Minimal Trees with Obstacles
and Steiner Visibility Graphs
Pawel Winter *
Suppose that we are given a set Z = {ZI' Z2, •.. , zp} of terminals in the plane together
with a set of disjoint polygonal obstacles n = {WI, W2, ... , wd. The Euclidean Steiner t7'ee
problem with obstacles (ESTPO) is to determine the shortest network spanning Z while
avoiding all obstacles. Such a network is called the obstacle-avoiding Euclidean Steiner
minimal tree (ESMTO).
ESTPO is NP-hard even in the simplest case with n = 0. This obstacle-free version is
known as the Euclidean Steiner tree problem (ESTP), and its optimal solution is referred to
as the Euclidean Steiner minimal tree (ESMT). Rather than developing exact algorithms
for the ESTP (ESTPO), one needs to look for good heuristics to solve problem instances
of even relatively small size (say 30 terminals).
One of the heuristics for the ESTP is based on the observation that ESMTs tend
to break down into unions of ESMTs for small subsets of terminals. This observation
also seems to be valid for the ESMTOs. In particular, for randomly generated problem
instances of the ESTP (ESTPO), it is extremely rare that their optimal solutions involve
subnetworks with more than five terminals.
The general idea behind the heuristic for the ESTP, which also carries over to the
ESTPO, is as follows.
• Select subsets of 2, 3, and 4 terminals which are most likely to appear near each
other in the ESMT (ESMTO).
• Construct ESMTs (ESMTOs) for each of these subsets.
• Concatenate ESMTs (ESMTOs) in some greedy fashion to a tree spanning all ter-
minals.
We address one of the major subproblems which needs to be resolved in order to obtain
an efficient version of this heuristic for the ESTPO. It is the problem of constructing
ESMTOs for subsets of 3 and 4 terminals avoiding n.
·University of Copenhagen, Dept. of Computer Science, DK-2100 Copenhagen 0, Denmark.
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgiil et al.
© Springer-Verlag Berlin Heidelberg 1992
314
The selected subsets of 3 and 4 terminals will in general not be disjoint. Particular
locations of the terminals in the subset often make it possible to prune away some (but
usually not all) of the obstacles. When more than one obstacle remain, brute enumeration
is the only exact method available at this time. More specifically, to solve the ESTPO with
three terminals {Zl' Z2, Z3} and an arbitrary number of polygonal obstacles, we proceed
as follows. For each triple of points {a, b, c} taken from among the three terminals and
extreme points V of not pruned obstacles, we
• construct the ESMT T(a,b,c),
• check for the penetration,
• connect the three terminals {Zl,Z2,Z3} to T(a,b,c) by shortest paths (possibly of
zero length),
and among the surviving (i.e., nonpenetrating) solutions select the shortest one. The
ESTPO with four terminals is solved in the analogous way.
Solving the three (four) terminals problem involves construction of many ESMTs, each
followed by the penetration check. Since subsets of terminals are not disjoint and several
small subproblems may involve the same obstacles, many ESMTs will be constructed
more than once. Consequently, it is worthwhile to preprocess terminals and obstacles so
that the penetration checks can be carried out as efficiently as possible. We introduce a
family of geometric constructs, called Steiner visibility graphs, which make it possible to
achieve this objective and discuss several algorithms for their construction.
Let a and b denote two points anywhere in the plane but inside any obstacle. We say
that b is Steiner-visible from a iff it is possible to place a point s somewhere on the 2; -arc
;;:b of the circle circumscribing the equilateral triangle (with ab as one of its sides and the
third, so-called equilateral point eab to the right of the line through a and b) such that a
as well as b are visible from s.
When b is Steiner-visible from a, there is usually more than just one feasible location
for s on ;;:b. Fea~ible locations of s on ;;:b form one or several subarcs of ;;:b. The set of
these disjoint sub arcs will be denoted by Sn(a, b).
When n contains only one convex polygonal obstacle then Sn(a, b) can be determined
in O(1og n) time where n denotes the maximum number of boundary edges on any of the
obstacles. The algorithm achieving this bound is basically using the information about
the relative positions of 4 supporting half-lines from a and b to w. When n contains k
convex polygonal obstacles, the one-obstacle algorithm can be used to determine Sn (a, b)
in O( k log k + k log n) time. This algorithm can be easily modified to cover the case with
315
non-convex polygonal obstacles (by the triangulation of obstacles). Its complexity then
becomes O(mlogm) where m = IVI.
Steiner visibility between all pairs of points in Z U V can be conveniently represented
by a directed Steiner visibility graph of first order, denoted by G 1 (Z, !1), with Z u V as its
vertex set. G 1 (Z,!1) has an oriented arc [a, b] iff b is Steiner-visible from a. Non-empty
Steiner visibility regions Sn (a, b) can be associated with the arc [a, b].
The simplest way of obtaining Gl(Z,!1) is to determine Sn(a, b) for each ordered pair
of points in Z U V. Such algorithm determines G1 (Z,!1) in 0(M 2 mlogm) time using
0(M2m) space, where M = m + p.
The Steiner visibility from one point to all other points can be determined in one sweep
while looking at one boundary edge at a time (rather than one obstacle at a time). One
of the advantages of this second approach is that the triangulation is not needed when
the obstacles are not convex. This algorithm determines G1 (Z,!1) in 0(M 2 mlogm) time
and 0(M2m) space.
Rather than scanning all boundary edges for a given a E Z U V, we can first prune
some of them away if they have no influence on the Steiner visibility from a. Suppose
that a beam of light originating from a is rotated all the way around. As the beam hits
the first obstacle on its way, it is reflected back with the information which boundary
edge caused this reflection. We can avoid the situation where the beam is not reflected in
some direction by placing two horizontal and two vertical slabs enclosing all Z-points and
all obstacles. Consequently, the rotation of the beam around a results in what basically
is a polygon Pa of the reflecting segments of boundary edges. Sn(a, b) for any point
b E Z U V, b "I a, can only contain subarcs of ;;:b which are inside Pa (and Pb). Pa
can be easily obtained as a byproduct of the well known 0(M2) time and O(M) space
plane-sweep algorithm for the visibility graphs.
Using this third approach, G1 (Z,!1) can be determined in 0(M 2 m) time and space
using 0(M2) preprocessing. This bound is overly pessimistic on the average since the
number of segments on Pa and H will usually be much smaller than m (worst case), and
it is not necessary to walk all the way around Pa and H to determine Sn (a, b).
Suppose that the Steiner visibility graph of i-th order, i 2: 1, denoted by Gi(Z, !1), is
given. Let Ei denote its arcs. The Steiner visibility graph of (i+l)-st order, denoted by
G i +1(Z,!1), is defined as follows. Its vertex set consists of ZUV together with equilateral
points corresponding to arcs in E i • G i +1(Z,!1) has an oriented arc [a, b] iff [a, b] E Ei or
it is possible to place an S-point s on ;;:b such that
• If a E Ei , then Sa E Sn(al,a2), where Sa is the intersection of sa with.iVi2, and Sa
is visible from s. If a E Z U V, then a must be visible from s.
316
.If bEE;, then Sb E Sn(bI, b2 ), where Sb is the intersection of sb with iJ;;, and Sb is
visible from s. If b E Z U V, then b must be visible from s.
We are only interested in Steiner visibility graphs which are needed to construct ESM-
TOs for 3 and 4 terminals from Z U V. It can be shown that we only need G2 (Z, !1).
Furthermore, when constructing G2 (Z, !1), we can restrict our attention to the cases when
either a or b belongs to Z U V.
If a E El or b EEl, then Sn(a, b) can initially be narrowed due to the non-existence
of the line-segment sa (resp. sb) intersecting aili2 (resp. iJ;;) or due to the intersection
occuring at Steiner-invisible points on aili2 (resp. iJ;;). Once this initial restriction of ;b
is obtained, the approach which is almost the same as for G l (Z,!1) is applied to obtain
the final Sn(a, b).
Assume that the restricted G2(Z,!1) is given. Suppose that a Steiner tree T(a,b,c)
exists with s E ;b. The following conditions are necessary and sufficient for T(a, b, c) to
be nonpenetrating any of the obstacles in !1:
• b is Steiner-visible from a, and s belongs to one of the intervals in Sn(a, b),
• c is Steiner-visible from b, and s belongs to one of the intervals in Sn(b, c),
Hence, the penetration check for T(a, b, c) can be carried out in O(q+log m) time where
O(q) is the time needed to establish whether [a, b] E G2 (Z, !1). If obstacles are convex,
this reduces to O(q + log k). Using similar approach, the existence of nonpenetrating
Steiner tree for 4 points can be determined in O( q + log m) time.
It should be pointed out that Steiner visibility graphs could be pruned for even more
arcs if the distances between points were taken into account. Such distance tests proved
to be very useful in pruning infeasible topologies when looking for the exact solution to
the ESTP.
A Set Covering Formulation of the Matrix
Equipartition Problem
S. Nicoloso and P. Nobili *
This paper is concerned with a certain N P-hard matrix decomposition problem (Matrix
Equipartition). Given a (0, I)-matrix M with a row set R, Matrix Equipartition consists
of finding two equicardinality subsets RI and R2 of R with maximum size such that every
row of RI is disjoint from every row of R 2 • Call R3 the set of the remaining rows.
In addition to its theoretical significance, the problem arises also in applicative contexts
like, for example, the design of Very Large Scale Integrated circuits (P LA block-folding
problem) and Flexible Manufacturing Systems (two machines loading problem).
We propose two different formulations admitted by Matrix Equipartition. The first
one is the following:
minimize I:iER Xli + I:iER X3i
subject to: I:iER Xli I:iER X2i
Xli + X2i + X3i I ('Vi E R)
Xlh + X2k < 1 ('Vh, k E R: hand k non-disjoint)
Xl, X2, X3 E {O,I}R
where Xsi has value 1 when the ith row is assigned to set Rs. The first constraint imposes
the equality on the sizes of the two sets RI and R 2 • The second one is an assignment
c~nstraint: any row must be in either of the three subsets of rows. As for the third one,
'topological constraint', it states that two non-disjoint rows hand k cannot be assigned
to the subsets RI and R2 respectively; that is, the two rows can be either assigned to a
same subset Rk, k = 1,2,3 or to two different subsets, one of which must indeed be R3 .
Notice that the number of constraints of this formulation is polynomially bounded in the
number of variables.
*Instituto di Analisi dei Sistemi ed Informatica del CNR, Viale Manzoni 30, 00185 Roma, Italy.
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgiil et al.
© Springer-Verlag Berlin Heidelberg 1992
318
For introducing the second formulation we need the following:
Definition 1. A subset R2 of R is said to be solution-inducing if there exist two subsets
Rl and R3 such that < R1 , R 2 , R3 > is a tripartition of R solution to Matrix Equipartition.
Any such solution is said to be induced by R 2 •
Since for solving the problem, it is obviously sufficient to determine a solution-inducing
set R2 , we can write:
(F)
mInImIZe
subject to: I:'ES Yi > 1 (\:I minimal bad subset S ~ R)
Yi E {O,I}R
where y, has value °or 1 depending on whether the ith row does or does not belong to
R2 , and a 'bad' subset S of R is a subset for which 2 1 S 1 + 1 N(S) 12::1 R 1 +1, where
N(S), the neighbour rows set, is the set of rows not belonging to S which are not disjoint
from some row of S. Among all the bad subsets S of R, it is sufficient to consider only
the minimal ones, i. e. those which do not properly contain a bad subset.
It is possible to prove the following:
Theorem 1. A (0, I)-vector y satisfies the constraints of (F) if and only if the corr'e-
sponding set R2 is solution-inducing.
Observe that (F) is a Set Covering formulation. By the Set Covering theory, one can
show that most of the costraints of (F) usually define facets of the polyhedron whose
extreme points are the incidence vectors of the solution-inducing sets. In contrast with
the first formulation, the number of constraints of (F) is, in the worst case, exponential
in 1R I. This fact in practice limits to very small instances the possibility of solving (F)
by explicitly listing all of its constraints. Also, observe that every minimal bad subset
S constitutes a minimal nonsolution in the sense that any subset obtained from S by
dropping one element induces a solution to Matrix Equipartition.
We implemented an incremental algorithm for solving Matrix Equipartition based on
a Branch & Bound technique which starts with a feasible heuristic solution. During the
process, the algorithm maintains a relaxed description of the set of feasible solutions,
that is a (small) subset of the constraints of (F). Whenever a new solution is found,
which satisfies the current partial formulation, it is handed to a Feasibility Oracle, which,
given a (0, I)-vector, either verifies that it is the incidence vector of a solution-inducing
set or produces a number of violated constraints which are added to the current partial
formulation.
319
We coded the algorithm in Pascal, and we run it on a DEC Vax 6310 on several
test matrices either taken from the literature or randomly generated. For comparison
reasons, we also run a standard Branch & Bound based package (ZOOM) on the same
test problems using the first formulation proposed.
Numerical results show that our approach is to be preferred over the traditional one
in the instances whose density is greater than 20%, roughly, and indeed, the performance
advantage improves for increasing densities. This fact can be explained considering that
the cardinality of the bad subsets decreases as the density increases. Consequently, the
associated Set Covering constraints become stronger and stronger.
We think that the addition of a procedure for generating valid cuts from the fractional
solutions obtained by the LP relaxations solved during the bounding procedure could
improve the performance of the algorithm. For the same purpose, we think also that
it is worth to try a strategy which uses also some of the constraints of the first given
formulation.
Maximizing a Submodular Function by Integer
Programming: A Polyhedral Approach
Heesang Lee and George L. Nemhauser *
Let N = {I, 2, ... ,n} be a finite set. A real-valued function 1 whose domain is all
of the subsets of N is said to be submodular if 1(S) + 1(T) ;::: 1(S U T) + 1(S n T)
for all S, T S;;; N. The problem of maximizing a submodular function includes many
N P-hard combinatorial optimization problems, for example the max-cut problem, the
uncapacitated facility location problem and some network design problems. Thus, this
research is motivated by the opportunity of providing a unified approach to many N P-
hard combinatorial optimization problems whose underlying structure is submodular.
We can formulate the problem of maximizing a submodular function as the integer
program:
max{7]: (7],x) E Qsub(J)},
where Qsub(J) = ((7],x) E JRl * Bn: 7]:::; 1*(S) + L-jEN\SPj(S)Xj - L-jES kjxj, YS S;;; N},
and Bn is the set of n-dimensional binary vectors, and pj(S) = 1(SU {j}) - 1(S), and kj =
-pj{N\ {j}), and 1*(S) = 1(S) + L-jES kj . This is true because given (7], xT) E JRl * Bn,
(7], x) E Qsub(J) if and only if 7] :::; 1(T), where xT is the incidence vector of T. We study
the polyhedral structure of the convex hull of Qsub(J) and strong cutting plane algorithms
for solving the related integer program. First, we restrict 1 to be a submodular function
that is used to formulate the max-cut problem. Let Qsub(Jcut) be the corresponding
solution set when we restrict 1 to the submodular function for the max-cut problem. We
derive several classes of valid inequalities for the convex hull of Qsub(Jcut). They include
node packing related inequalities (clique, odd hole, odd antihole and web inequalities) and
some generalizations (t-clique-star inequalities). We give sufficient conditions for these
inequalities to be facet-defining, and when they are not facets, we show how to lift them
to obtain facets.
Also, we study the relationship between Qsub(Jcut) and the node packing polyhedron.
One result here is that we can find a class of valid inequalities for Qsub(Jcut) from any
• Georgia Institute of Technology, School of Industrial and Systems Engineering, Atlanta, GA 30332-
0205, U.S.A.
NATO ASl Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgiil et al.
© Springer-Verlag Berlin Heidelberg 1992
322
class of subgraph-induced valid inequalities for the node packing polyhedron. Moreover, if
the inequalities are facet-defining inequalities for the node packing polyhedron and satisfy
a simple condition, then the corresponding inequalities are facet-defining inequalities for
Qsub(fcut). This condition is always satisfied for any subgraph-induced inequalities of the
cardinality max-cut problem and always satisfied for the clique and odd-hole inequalities
of the weighted max-cut problem.
The separation problems for the node packing related inequalities of the cardinality
max~cut problem are the same as those for the corresponding inequalities of the node
packing problem. For the weighted max-cut problem, the separation problems are more
difficult, and we develop some heuristics for solving them.
We introduce a modification technique which transforms an arbitrary submodular
function and its maximization problem into a nonincreasing submodular function and the
maximization problem for the modified function. With this modification, almost all of
the polyhedral results (such as valid inequalities, facet conditions, lifting theorems and
separation problems) for Qsub(fcut) can be extended to Qsub(f') where f' is the modified
function.
When we solve the integer program by a cutting plane method, we begin with a
subset of the constraints and tighten the relaxation by adding violated valid inequalities
for Qsub(f') after solving each relaxed problem. We can solve the separation problem
for the original constraints of Qsub(f') in polynomial time when x E En. But, this
is not generally known to be true when x is fractional. The max-cut problem and the
uncapacitated facility location problem are special cases where we can solve the separation
problem for the original constraints in polynomial time. This advantage is possible since
the corresponding submodular function can be decomposed into a sum of submodular
functions, i.e. f(8) = I:i P(8), where each fi(8) is submodular. Another advantage
of this separable formulation is that we obtain a stronger linear program relaxation by
applying our polyhedral results to each submodular function fi.
We are developing strong cutting plane algorithms for some classes of the general
problem, includiI).g the max-cut problem and the uncapacitated facility location problem.
The algorithms use facet-defining inequalities and separation heuristics for identifying
such violated inequalities.
New Bounds for the Asymmetric Traveling
Salesman Problem
A.I. Barros *
P. Barcia t
Recently new techniques for producing lower bounds have appeared in the literature:
bound improvement sequences [1,2,3] and Lagrangean decomposition [6, 5].
The bound improving sequence technique consists of building a sequence of Lagrangean
duals that progressively reduces the duality gap. This is done by adding to the original
formulation a redundant constraint which will reduce the set of feasible solutions of the
Lagrangean dual. This way, we can expect to obtain better lower bounds.
In some problems, it is possible to find two sets of 'nice' constraints. For these problems
Jornsten and Nasberg [6] and Guignard and Kim [5] duplicate the variables thus enabling
a Lagrangean dual which will use both structures. Also, they have proved that this
approach yields stronger bounds.
Both techniques described seek to obtain stronger lower bounds. Barcia & Jornsten[4]
have developed an approach combining these two techniques which was applied with
success to the Generalized Assignment Problem. The main idea is to strengthen the La-
grangean decomposition, by adding to the original formulation a redundant constraint, in
order to obtain a lower bound which dominates the bounds generated by both techniques
individually.
In this work we follow this path applying the combined approach to the Asymmetric
Traveling Salesman Problem( AT S P).
We start by exploiting the r-Arborescence structure of the Asymmetric Traveling
Salesman Problem, and we show that the formulation obtained is as strong as the usual
one in the sense that its linear relaxation has the same set of feasible solutions.
Then, we apply the improved Lagrangean decomposition concept on this formulation
producing bounds which proved to be able to bridge the duality gap.
'University of Lisbon, Department of Statistics and Operation Research, Av. 24 de Julho 134-5, 1300
Lisboa, Portugal.
tThe New University of Lisbon, Department of Economics.
NATO AS! Series, Vol. F 82
Combinatorial Optimization
Edited by M. AkgUl et al.
© Springer-Verlag Berlin Heidelberg 1992
324
In order to speed up the computation of these bounds, we developed an extension
of the subgradient method which takes full advantage of the complete structure of the
formulation.
The extended version of the subgradient method allows the maximization of some non
concave functions, and its convergence was established under similar conditions of the
subgradient method.
Our limited computational experience shows that the use of the improved Lagrangean
decomposition technique will generally manage to bridge the duality gap without using
valid inequalities or branch and bound methods. We can expect to achieve the same kind
of results, or better ones, with other problems.
The extended version of the sub gradient method seems also to be a promising concept
and can be useful to extend the application of Lagrangean techniques to other optimization
problems.
References
[1] P. Barcia. The Bound Improving Sequence algorithm. Operation Research Letters,
4:27-30, 1985.
[2] P. Barcia. Constructive dual methods for Discrete Programming. Discrete Applied
Mathematics, 18:107-117, 1987.
[3] Barcia, P., and S. Holm. A revised Bound Improvement Sequence algorithm. Eu-
ropean Journal of Operational Research, 36:202- 206, 1988.
[4] Barcia, P., and K. Jornsten. Improved Lagrangean decomposition:An application
to the Generalized Assignment Problem. European Journal of Operation Research,
46(1 ):84- 92, 1990.
[5] Guignard, M., and S. Kim. Lagrangean decomposition: A model yielding stronger
Lagrangean bounds. Mathematical Programming, 6:62-88, 1987.
[6] Jornsten, K., and M. Nasberg. A new Lagrangean approach to the Generalized
Assignment Problem. European Journal of Operational Research, 27:313- 323, 1986.
A Lagrangean Heuristic for Set Covering
Problems
J .E. Beasley *
The set covering problem (SCP) is the problem of covering the rows of a m row, n column,
zero-one matrix by a subset of the columns at minimum cost.
It is well-known that the SCP is NP-complete and a number of optimal algorithms,
typically based upon tree search procedures, have been presented in the literature.
These algorithms have solved unicost problems involving up to 50 rows and 500
columns and non-unicost problems involving up to 400 rows and 4,000 columns (albeit at
considerable computational cost, particularly for non-unicost problems).
It is clear that, given the current situation with respect to optimal algorithms, there
is a place for a computationally effective heuristic algorithm capable of producing good
quality (near-optimal) solutions for the SCP.
A number of heuristic algorithms for the SCP have been given in the literature. Many
of these algorithms can, in general terms, be regarded as variations on the following
general procedure for the SCP.
1. Let S be the set of columns chosen to be in the solution to the SCP (initially S is
empty).
2. Add to S some column (chosen via a systematic rule) and repeat until S constitutes
a solution to the SCP.
3. Improve S by:
(a) removing columns which are redundant; and
(b) interchanging columns in S with columns not in S.
In this talk we illustrate how:
(a) generating a lower bound for the SCP via lagrangean relaxation; and
'The Management School, Imperial College, London SW7 2AZ, England.
NATO ASI Series, Vol. F 82
Combinatorial Optimization
Edited by M. Akgiil et 01.
© Springer-Verlag Berlin Heidelberg 1992
326
(b) attempting to maximise that lower bound via subgradient optimisation;
(c) thereby produces a sequence of lower bound solutions that can be modified, in a
computationally effective way, to produce a sequence of feasible solutions to the
SCP.
In addition we can, through use of the lower bound and the value of the best feasible
solution found, identify columns that can be removed from the problem.
Step (c) above is the innovative step - the rest of the algorithm is a standard approach.
The lagrange an heuristic presented in this talk was programmed in FORTRAN and
run on a Cray-1S (a vector processing supercomputer).
In order to compare the heuristic algorithm presented in this talk with other heuristics
we also coded in FORTRAN the BH heuristic of Balas and Ho, the SCHEURI heuristic
of Vasko and Wilson and the SCFUNC1 T07 heuristic of Vasko and Wilson.
96 test problems were considered (of varying sizes up to 1,000 rows and 10,000 columns).
For 76 of these test problems the optimal solution is known.
The following results were obtained:
1. The lagrangean heuristic gave a better (or equal) solution to BH or SCHEURI or
SCFUNC1 T07 for all but 3 of the 96 test problems.
2. For the 76 test problems for which the optimal solution is known:
(a) the optimal solution was found by BH 5 times, SCHEURI 13 times, SCFUNC1 T07
14 times and the lagrangean heuristic 44 times
(b) the average percentage deviation from optimal [100( solution value - optimal) loptim
was 7.416 % for BH, 3.311 % for SCHEURI, 6.428 % for SCFUNC1 T07 and
0.638 % for the lagrangean heuristic.
3. The ratio of computation times BH to SCHEURI to SCFUNC1T07 to lagrangean
heuristic was 1 to 9.3 to 12.8 to 14.8.
Computational results indicated that this heuristic gave better quality results than a
number of other heuristics, albeit at greater computational cost.
This trade-off between computational cost and quality of solution is common in heuris-
tic algorithms.
References
[1] J. E. Beasley. A lagrangian heuristic for set-covering problems. Naval Research
Logistics,37: 151-164, 1990.
Active node 42
Active schedule 65
Subject Index
Add and drop heuristic 249-251
Adjacent interchange property 155-161
Algorithms 41-48, 85-116
Alternating trees 90
Approximation algorithms 205
Arc deficiency 315-317
Assignment problem 85,311-314
Asymmetric TSP 333
Asymptotic efficiency 303-305
Augmenting Path 88
BUT property 178
Balinski-Gomory algorithm. 112-113
Base release time 67
Basis tree 286, 316
Bi-Steiner problem 289
Bicriteria mathematical programming 275
Bicriteria optimization 275-276
Bicriterion convex integer programming problems 275-276
Bin locations 191-193
Bin packing problem 303-305
Bipartite graph 216, 307-309, 311
Bivariate uniform distribution 236-337
Block angular structure 228
Block diagonal matrix 228
Block pivots 85-124
Block upper triangular matrix 176
Block-folding problem 327
Bottleneck path 287
Bound improvement sequences 333-334
Branch and bound algorithm 75, 205, 250, 255, 276, 328, 334
Bucket structure 39-62
Building evacuations 163-170
Capacitated minimum cost flow problem 315-317
Capacitated vehicle routing problem 297-299
Cardinality max-cut problem 331-332
Caressi and Sodini strategy 219-220
Characteristic function 132-133
Chebyshev metric 189, 197
Circulation flow 286
Clique inequalities 331-332
Clock heuristic 194
Collision of jobs 135-161
Column generation 253
Column sufficient matrices 260
Combinatorial optimization 135, 189, 199,211,261,331-332
Common due dates 293-295
Compatibility cliques 277
Compatible pair 215
Completely pleasant problems 270
Complexity analysis 85-116
Complexity issues 85-116,217
328
Computational complexity 85-116, 146, 158,204,261,295, 304, 307-309, 311-314,
315-316, 326, 331-332
Computational results 55-57, 134, 181-185,230-232,243,291,336
Computerized tomography 21
Concentrator location problem 249
Consecutive l's property 156
Constrained scheduling 281
Constraint decomposition 9
Contiguity constraints 139
Convergence properties 230
Convex hull 69
Connectivity constraints 291
Cooperative game theory 125, 131
Cost allocation 125
Cost operator algorithms 114-115
Costrained matching 211-221
Coupling constraints 228
Criss-cross algorithm 115-116,259
Cross decomposition 14
Crossover 242
Cut edge 41, 88
Cutting plane algorithm 66,290-291, 331-332
Cutting stock problem 301
Cycle cover 285
Cycle simplification 251 .
D-dimensional bin packing problem 304
Data structures 49-51,308
Decomposition 1-18
Dequeue 53
Digitized image 26-28
Dijkstra's algorithm 85
Directed bi-Steiner problem 290
Directed graphs 285
Directed Steiner problem 290
Disjunctive graph 202
Dispatch rule 207
Dual based algorithms 275-276
Dual methods for assignment problems 103-104
Dual simplex 315-317
Duality gap 331-332
Duality theory 1-18
Dynamic networks 169-172
Dynamic programming 319-322
Dynamic working basis 170-178
Eligible edge 42
Eligible path 42
Emergency evacuations 163-170
Enumeration schemes 205
Equality subgraph 87
Error analysis 271
Euclidean Steiner tree problem 323-325
Evacuation modeling 163-170
Excess flow 45
Excess sclaing algorithm 48
Exterior point algorithm 311-315
Extreme matching 214
FIFO/LIFO scheme 53
Facet defining inequalities 265, 331-332
Farka's lemma 2
Feasibility oracle 328
Feasibilty cuts 5
Feasible prefix 152
Filtering 242
Flexible manufacturing systems 319
Forest algorithms 107
Fundamental co-cycle 88
Fundamental cycle 88
GUB constraints 222,265
329
Gas processing system 125-128
Generalized HARMONIC algorithm 303
Generalized assignment problem 333-335
Genetic Algorithms 239
Genetic operator 242
Genetic repair 242
Grapgh 235
Greedy solutions 269
Heuristic procedure 239-242,249,293-295,301-302,331-332
Hierarchical structure 241
Highest label selection rule 45
Hoffman's circulation conditions 285
Hong-Rom algorithm 111-112
Hurricane evacuations 163-170
Hyperegde 245
Hypergraph partitioning problem 245
Hypomatchable set 214
Image reconstruction 21-23
Implementation 116,224,309
Independent jobs 293-295
Insertion sequence 195
Integer programming 202,254,290,331
Integer programming (0-1) 130, 133,265,319-322,331
Integrated loading and scheduling 319-322
Interchange heuristic 245
Investment problem 130
Iterative traveling salesman problem 189
Job shop scheduling 63-82, 135-162,201-210,319-322
Just-in-time inventory systems 293-295
K-best matching 211
330
Knapsack polytope 265
Knapsack problem 265,269,301-302,319-322
Kth order distance variable 236
L-2 approximations 246
Labeling algorithm 307-309
Lagrangean relaxation 290,336
Lagrangean heuristic 335
Lagrangean decomposition 333-334
Lagrangean dual 2, 223
Lagrangean multipliers 200, 290, 319-322
Lagrangean subproblems 290,319-322
Least squares 246
Level curve approach 195
Lexicographic linear programming 132
Lifted cover inequality 265
Lifted-cover facets 265
Linear complementarity problem 259
Linear inequalities 19
Linear programming model 301-302
Linear programming relaxation 66,329,331-332
Linear relaxation 255-256
Linear-quadratic penalty functions 227
Linked list 53
Location model 195
MINTO 76
Machine scheduling 293-295,319-322
Matching 87,211,307
Mathematical models 201
Mathematical programming 131-132, 277-278
Matrix equipartition 327
Max-balanced flow 285
Max-cut problem 331-332
Max-linear combinatorial optimization problem 189
Maximal independent set 267
Maximizing net present value 125-134
Maximum flow 39
Maximum noncrossing matching 307-309
Menu-based interface 243
Military application 230
Minimal cover 265
Minimal prefix 152
Minimum cost network flows 164-166,251,312,315
Mixed integer programming 66, 290
Modeling congestion 169-175
Modular decomposition 146
Monte Carlo simulation 237
Multi-drop line 249
Multicommodity networks 227
Multiple depot problem 253-256
Multiple time windows 253-256
Multiple traveling salesman problem 297-298
Murty and Lawler strategy 218-219
Mutation of order k 242
NP-complete problem 336
331
NP-hard problems 85,204,205,235,239,245,283,289-291,295,303,327,331-
332
Negative alternating cycle 214
Netlist partitioning 245
Nevwork 39, 85, 223
Nevwork design problem 331-332
Network flow problems with side constraints 166-168
Network simplex 166
Nondominated solutions 275-276
Nonpreemptive scheduling 65
Normal equations 247
Odd hole inequalities 331-332
Oil industry 125-129
Optimal location of concentrators 249
Optimal permutation of jobs 155
Optimal sequencing 135-162
Optimization 277
Optimization algorithms 201-205
Oriented regular matroids 263, 285-287
Orthogonal basic tableaus 260-261
Outer Approximation 4
Outline scheme 281-283
Parallel chains 140-143
Parallel composition 142
Parallel computing 227
Parallel machines 293-295
Parallel surrogate constraint method 31
Parametric knapsack problem 270
Partition strategies 218-220
Patching heuristic 297-299
Patient distribution system 230
Penalized nonlinear program 229
Penalty method 229
Perfect matching 87, 211
Phase I linear programming 19, 311
Phase I procedure 311
Phase-based green scheduling 278
Pivot rules 316-317
Pleasant knapsack problem 269
Polygonal obstacles 324-325
Polyhedral combinatorics 63, 331-332
Polyhedral methods 63, 331-332
Polynomial approximation scheme 281-283
Polynomial bounds 85, 116,308,315
Polynomial time algorithm 85, 283, 297-299
Polynomially solvable TSP 195
Polynomially solvable single-machine problem 205
Positron emission tomography 21
Precedence constraints 72, 140-150,281-283
Preference orders 135
332
Preflow 39-41
Preflow-push algorithms 41-42
Primal dual methods for assignment problems 85-124
Primal methods for assignment problems 112
Primal partitioning simplex 167-168
Primal simplex 97
Primary diagonal block 177-178
Printed circuit board assembly 186
Priority search 311-314
Probabilistic behavior 303
Probabilistic minimum spanning tree 235
Profit allocation game 133
Pseudonodes 215
Pseudopolyrtomial time 294
Pure network flows 164-166
Quadratic programming duality 2
Quasi assignment relaxation 297-299
Queue 53
Random network generator 55-56
Random variables 235-236
Ratio rule 135
Recursive methods for assignment problems 85
Regularity conditions 269
Relatively pleasant problems 272
Relaxation 66, 133-134, 228
Relaxation methods for assignment problems 96-97
Release times 63
Reproduction 242
Residual network 41
Restricted I-median problem 196
Restricted location problem 191
Restricted problem 217-219
Robot tour problem 189
Robotic assembly problem 189
Row sufficient matrices 260
SAP method 215
Scheduling 63-82, 135-162,201-210,293-295
School timetable 239
Separating hyperplane 73
Separation problem 76
Sequence inequalities 71
Sequencing problems 135-162
Sequential surrogate constraint method 30-31
Series composition 142
Series parallel precedence constraints 142
Set covering problem 254, 327-330, 335
Shapely value 132
Shortest augmenting path procedure 215
Side constraints 163-188,212
Signature methods for assignment problems 101-103, 105-107
Simplex method 311-314
Simplicial decomposition 230
Simulated annealing 206
Single junction control 277
333
Single machine scheduling problem 63-64, 204, 293-295
Sliding bottleneck heuristic 206
Smith's ratio rule 68, 135
Spanning tree 235-236
Spindle setup 319-322
Stack 53
Stage-based scheduling 277-278
Steiner tree problem with obstacles 323-325
Steiner trees 323
Steiner visibility graph 323-325
Storing networks 49
Strong cutting planes 265-268
Strong duality 2
Strong substitution property 150
Strongly compatible triple 215
Strongly connected components 286
Strongly feasible trees 89
Subgradient optimization 333, 336
Submodular functions 331
Subscriber network extension 289
Subset inequalities 70
Subtrees 316
Successive orthogonal projection method 20
Successive shortes path method 94-96
Summation inequalities 71
Surface graphs 216-217
Surface network 47
Surrogate constraints 19, 28, 32
Surrogate duality relaxation 205
Surrogate hyperplane 27-30
Symmetric flow 286-287
System design 278
Tardiness penalties 293-295
Telecommunication network design 289-290
Teleprocessing network 249
Hungarian algorithm 90-94
Minimal spanning tree 235
Time complexity 85
Time windows 253
Time-expanded networks 169-172
Timetable problem 239-244
Tool loading problem 319-322
Tool magazine setup, 319-322
Traffic control 277-278
Transportation problem 311-314
Traveling salesman problem 78,297, 333
Triangle inequality 65
Turnstile cost 173
Two machine loading problem 281,327
Two-machine flow shop 281-283
334
Two-stage cutting stock problem 301-302
Uncapacitated facility location problem 331-332
Underestimating cuts 245
Undirected Steiner problem 290
Utility function 276
VLSI design 245, 327
Valid inequalities 70,331-332
Value cuts 5
Variable decomposition 3
Vehicle routing problem 253
Visible Excess 47
Wave scaling algorithm 39-62
Weak duality 2, 75
Weighted completion time problem 137
Weighted noncrossing matching 308-309
Weighted tardiness 319-322
Working basis 167
Working basis inverse 167
Worst-case analysis 303-305
Wroc1aw taxanomy 235-236
NATO ASI Series F
Including Special Programmes on Sensory Systems for Robotic Control (ROB) and on Advanced
Educational Technology (AET)
Vol. 1: Issues in Acoustic Signal - Image Processing and Recognition. Edited by C. H. Chen.
VIII, 333 pages. 1983.
Vol. 2: Image Sequence Processing and Dynamic Scene Analysis. Edited by T. S. Huang. IX,
749 pages. 1983.
Vol. 3: Electronic Systems Effectiveness and Life Cycle Costing. Edited by J. K. Skwirzynski.
XVII, 732 pages. 1983.
Vol. 4: Pictorial Data Analysis. Edited by R. M. Haralick. VIII, 468 pages. 1983.
Vol. 5: International Calibration Study of Traffic Conflict Techniques. Edited by E. Asmussen.
VII, 229 pages. 1984.
Vol. 6: Information Technology and the Computer Network. Edited by K. G. Beauchamp. VIII,
271 pages. 1984.
Vol. 7: High-Speed Computation. Edited by J. S. Kowalik. IX, 441 pages. 1984.
Vol. 8: Program Transformation and Programming Environments. Report on a Workshop
directed by F. L. Bauer and H. Remus. Edited by P. Pepper. XIV, 378 pages. 1984.
Vol. 9: Computer Aided Analysis and Optimization of Mechanical System Dynamics. Edited by
E. J. Haug. XXII, 700 pages. 1984.
Vol. 10: Simulation and Model-Based Methodologies: An Integrative View. Edited by T. I. Oren,
B. P. Zeigler, M. S. Elzas. XIII, 651 pages. 1984.
Vol. 11: Robotics and Artificial Intelligence. Edited by M. Brady, L. A. Gerhardt, H. F. Davidson.
XVII, 693 pages. 1984.
Vol. 12: Combinatorial Algorithms on Words. Edited by A. Apostolico, Z. Galil. VIII, 361 pages.
1985.
Vol. 13: Logics and Models of Concurrent Systems. Edited by K. R. Apt. VIII, 498 pages. 1985.
Vol. 14: Control Flow and Data Flow: Concepts of Distributed Programming. Edited by M. Bray.
VIII, 525 pages. 1985.
Vol. 15: Computational Mathematical Programming. Edited by K. Schittkowski. VIII, 451 pages.
1-985.
Vol. 16: New Systems and Architectures for Automatic Speech Recognition and Synthesis.
Edited by R. De Mori, C.Y. Suen. XIII, 630 pages. 1985.
Vol. 17: Fundamental Algorithms for Computer Graphics. Edited by R. A. Earnshaw. XVI, 1042
pages. 1985.
Vol. 18: Computer Architectures for Spatially Distributed Data. Edited by H. Freeman and G. G.
Pieroni. VIII, 391 pages. 1985.
Vol. 19: Pictorial Information Systems in Medicine. Edited by K. H. Hahne. XII, 525 pages. 1986.
Vol. 20: Disordered Systems and Biological Organization. Edited by E. Bienenstock, F.
Fogelman Soulie, G. Weisbuch. XXI, 405 pages.1986.
Vol. 21: Intelligent Decision Support in Process Environments. Edited by E. Hollnagel, G.
Mancini, D. D. Woods. XV, 524 pages. 1986.
NATO ASI Series F
Vol. 22: Software System Design Methods. The Challenge of Advanced Computing Techno-
logy. Edited by J. K. Skwirzynski. XIII, 747 pages. 1986.
Vol. 23: Designing Computer-Based Learning Materials. Edited by H. Weinstock and A Bork.
IX, 285 pages. 1986.
Vol. 24: Database Machines. Modern Trends and Applications. Edited by A K. Sood and
AH. Qureshi. VIII, 570 pages. 1986.
Vol. 25: Pyramidal Systems for Computer Vision. Edited by V. Cantoni and S. Levialdi. VIII,
392 pages. 1986. (ROB)
Vol. 26: Modelling and Analysis in Arms Control. Edited by R. Avenhaus, R. K. Huber and
J.D. Kettelle. VIII, 488 pages. 1986.
Vol. 27: Computer Aided Optimal Design: Structural and Mechanical Systems. Edited by
C.A Mota Soares. XIII, 1029 pages. 1987.
Vol. 28: Distributed Operating Systems. Theory und Practice. Edited by Y. Paker, J.-P. Banatre
and M. Bozyigit. X, 379 pages. 1987.
Vol. 29: Languages for Sensor-Based Control in Robotics. Edited by U. Rembold and
K. Hormann. IX, 625 pages. 1987. (ROB)
Vol. 30: Pattern Recognition Theory and Applications. Edited by P.A Devijver and J. Kittler. XI,
543 pages. 1987.
Vol. 31: Decision Support Systems: Theory and Application. Edited by C. W. Holsapple and
A B. Whinston. X, 500 pages. 1987.
Vol. 32: Information Systems: Failure Analysis. Edited by J.A Wise and A Debons. XV, 338
pages. 1987.
Vol. 33: Machine Intelligence and Knowledge Engineering for Robotic Applications. Edited by
A K. C. Wong and A Pugh. XIV, 486 pages. 1987. (ROB)
Vol. 34: Modelling, Robustness and Sensitivity Reduction in Control Systems. Edited by
R. F. Curtain. IX, 492 pages. 1987.
Vol. 35: Expert Judgment and Expert Systems. Edited by J. L. Mumpower, L. D. Phillips, O. Renn
and V. R. R. Uppuluri. VIII, 361 pages. 1987.
Vol. 36: Logic of Programming and Calculi of Discrete Design. Edited by M. Broy. VII, 415
pages. 1987.
Vol. 37: Dynamics of Infinite Dimensional Systems. Edited by S.-N. Chow and J. K. Hale. IX, 514
pages. 1987.
Vol. 38: Flow Control of Congested Networks. Edited by A R. Odoni, L. Bianco and G. Szeg6.
XII, 355 pages. 1987.
Vol. 39: Mathematics and Computer Science in Medical Imaging. Edited by M. A Viergever and
A Todd-Pokropek. VIII, 546 pages. 1988.
Vol. 40: Theoretical Foundations of Computer Graphics and CAD. Edited by R.A Earnshaw.
XX, 1246 pages. 1988.
Vol. 41: Neural Computers. Edited by R. Eckmiller and Ch. v. d. Malsburg. XIII, 566 pages.
1988.
NATO ASI Series F
Vol. 42: Real-Time Object Measurement and Classification. Edited by A. K. Jain. VIII, 407 pages.
1988. (ROB)
Vol. 43: Sensors and Sensory Systems for Advanced Robots. Edited by P. Dario. XI, 597 pages.
1988. (ROB)
Vol. 44: Signal Processing and Pattern Recognition in Nondestructive Evaluation of Materials.
Edited by C. H. Chen. VIII, 344 pages. 1988. (ROB)
Vol. 45: Syntactic and Structural Pattern Recognition. Edited by G. Ferrate, T. Pavlidis, A. Sanfeliu
and H. Bunke. XVI, 467 pages. 1988. (ROB)
Vol. 46: Recent Advances in Speech Understanding and Dialog Systems. Edited by H. Niemann,
M. Lang and G. Sagerer. X, 521 pages. 1988.
Vol. 47: Advanced Compuling Concepts and Techniques in Control Engineering. Edited by
M.J. Denham and A.J. Laub. XI, 518 pages. 1988.
Vol. 48: Mathematical Models for Decision Support. Edited by G. Mitra. IX, 762 pages. 1988.
Vol. 49: Computer Integrated Manufacturing. Edited by I. B. Turksen. VIII, 568 pages. 1988.
Vol. 50: CAD Based Programming for Sensory Robots. Edited by B. Ravani. IX, 565 pages. 1988.
(ROB)
Vol. 51: Algorithms and Model Formulations in Mathematical Programming. Edited by S. W.
Wallace. IX, 190 pages. 1989.
Vol. 52: Sensor Devices and Systems for Robotics. Edited by A. Casals. IX, 362 pages. 1989.
(ROB)
Vol. 53: Advanced Information Technologies for Industrial Material Flow Systems. Edited by S. Y
Nof and C. L. Moodie. IX, 710 pages. 1989.
Vol. 54: A Reappraisal of the Efficiency of Financial Markets. Edited by R. M. C. Guimaraes, B. G.
Kingsman and S.J. Taylor. X, 804 pages. 1989.
Vol. 55: Constructive Methods in Computing Science. Edited by M. Broy. VII, 478 pages. 1989.
Vol. 56: Multiple Criteria Decision Making and Risk Analysis Using Microcomputers. Edited by
B. Karpak and S. Zionts. VII, 399 pages. 1989.
Vol. 57: Kinematics and Dynamic Issues in Sensor Based Control. Edited by G. E. Taylor. XI, 456
pages. 1990. (ROB)
Vol. 58: Highly Redundant Sensing in Robotic Systems. Edited by J. T. Tou and J. G. Balchen. X,
322 pages. 1990. (ROB)
Vol. 59: Superconducting Electronics. Edited by H. Weinstock and M. Nisenoff. X, 441 pages.
1989.
Vol. 60: 3D Imaging in Medicine. Algorithms, Systems, Applications. Edited by K. H. Hahne,
H. Fuchs and S. M. Pizer. IX, 460 pages. 1990.
Vol. 61: Knowledge, Data and Computer-Assisted Decisions. Edited by M. Schader and W. Gaul.
VIII, 421 pages. 1990.
Vol. 62: Supercomputing. Edited by J. S. Kowalik. X, 425 pages. 1990.
Vol. 63: Traditional and Non-Traditional Robotic Sensors. Edited by T. C. Henderson. VIII, 468
pages. 1990. (ROB)
Vol. 64: Sensory Robotics for the Handling of Limp Materials. Edited by P. M. Taylor. IX, 343 pages.
1990. (ROB)
Vol. 65: Mapping and Spatial Modelling for Navigation. Edited by L. F. Pau. VIII, 357 pages. 1990.
(ROB)
NATO ASI Series F
Vol. 66: Sensor-Based Robots: Algorithms and Architectures. Edited by C. S. G. Lee. X, 285
pages. 1991. (ROB)
Vol. 67: Designing Hypermedia for Learning. Edited by D. H. Jonassen and H. Mandl. XXV,457
pages. 1990. (AET)
Vol. 68: Neurocomputing. Algorithms, Architectures and Applications. Edited by F. Fogelman
Soulie and J. Herault. XI, 455 pages. 1990.
Vol. 69: Real-Time Integration Methods for Mechanical System Simulation. Edited by E. J. Haug
and R.C. Deyo. VIII, 352 pages. 1991.
Vol. 70: Numerical Linear Algebra, Digital Signal Processing and Parallel Algorithms. Edited by
G. H. Golub and P Van Dooren. XIII, 729 pages. 1991.
Vol. 71: Expert Systems and Robotics. Edited by T. Jordanides and B. Torby. XII, 744 pages.
1991.
Vol. 72: High-Capacity Local and Metropolitan Area Networks. Architecture and Performance
Issues. Edited by G. Pujolle. X, 536 pages. 1991.
Vol. 73: Automation and Systems Issues in Air Traffic Control. Edited by J. A. Wise, V. D. Hopkin
and M. L. Smith. XIX, 594 pages. 1991.
Vol. 74: Picture Archiving and Communication Systems (PACS) in Medicine. Edited by H. K.
Huang, O. Ratib, A. R. Bakker and G. Witte. XI, 438 pages. 1991.
Vol. 75: Speech Recognition and Understanding. Recent Advances, Trends and Applications.
Edited by P. Laface and Renato De MorL XI, 559 pages. 1991.
Multimedia Interface Design in Education. Edited by A. D. N. Edwards and S. Holland. XIV, 216
pages. 1992.
Vol. 77: Computer Algorithms for Solving Linear Algebraic Equations. The State of the Art. Edited
by E. Spedicato. VIII, 352 pages. 1991.
Vol. 78: Integrating Advanced Technology into Technology Education. Edited by M. Hacker, A.
Gordon and M. de Vries. VIII, 185 pages. 1991. (AET)
Vol. 79: Logic, Algebra, and Computation. Edited by F. L. Bauer. VII, 485 pages. 1991.
Vol. 80: Intelligent Tutoring Systems for Foreign Language Learning. Edited by M. L. Swartz and
M. Yazdani. IX, 347 pages. 1992. (AET)
Vol. 81: Cognitive Tools for Learning. Edited by P.A. M. Kommers, D. H. Jonassen and J. T. Mayes.
X, 278 pages. 1992. (AET)
Vol. 82: Combinatorial Optimization. New Frontiers in Theory and Practice. Edited by M. AkgOI,
H. W. Hamacher and S. TOfekQi. XI, 334 pages. 1992.